{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a896193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f39c663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\AppData\\Local\\Temp\\ipykernel_10692\\982243736.py:2: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "file_path = \".\\\\Base de dados\\\\archive\\\\accepted_2007_to_2018q4.csv\\\\accepted_2007_to_2018Q4.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc423742",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['loan_amnt', 'loan_status', 'home_ownership', 'purpose', 'addr_state', 'verification_status', \\\n",
    "                    'annual_inc', 'dti', 'fico_range_low', 'fico_range_high', 'open_acc', 'revol_bal', 'revol_util', 'emp_length']\n",
    "\n",
    "df_filtrado = df[colunas].copy()\n",
    "\n",
    "df_filtrado.dropna(subset=colunas, inplace=True)\n",
    "\n",
    "nulos = df_filtrado[colunas].isnull().any(axis=1).sum()\n",
    "print(nulos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f06b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado['fico_range_avg'] = (df_filtrado['fico_range_low'] + df_filtrado['fico_range_high']) / 2\n",
    "\n",
    "df_filtrado['loan_status_bin'] = df_filtrado['loan_status'].apply(\n",
    "    lambda x: 1 if x in ['Charged Off', 'Default', 'Late (31-120 days)', 'Late (16-30 days)','Does not meet the credit policy. Status:Charged Off'] else 0\n",
    ")\n",
    "\n",
    "df_filtrado.to_csv('df_filtrado.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fef977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_status_bin\n",
      "0    0.871437\n",
      "1    0.128563\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_filtrado = pd.read_csv(\"C:\\\\Users\\\\Gabriel\\\\Documents\\\\Faculdade\\\\TCC\\\\Cod\\\\df_filtrado.csv\")\n",
    "\n",
    "var_x = ['loan_amnt', 'home_ownership', 'purpose', 'addr_state', 'verification_status', \\\n",
    "                    'annual_inc', 'dti', 'open_acc', 'revol_bal', 'revol_util', 'emp_length', 'fico_range_avg']\n",
    "\n",
    "x = df_filtrado.loc[:, var_x]\n",
    "y = df_filtrado['loan_status_bin']\n",
    "\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aca5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
    "\n",
    "categoricas = ['home_ownership', 'purpose', 'addr_state', 'verification_status', 'emp_length']\n",
    "numericas = ['loan_amnt', 'annual_inc', 'dti', 'fico_range_avg', 'open_acc', 'revol_bal', 'revol_util']\n",
    "\n",
    "#Label Encoder = Ordinal Encoder\n",
    "#Normalização = MinMaxScaler\n",
    "#Padronização = Standart Scaler\n",
    "\n",
    "preprocessador1 = ColumnTransformer(transformers=[\n",
    "    ('num', MinMaxScaler(), numericas),\n",
    "    ('cat', OrdinalEncoder(), categoricas)\n",
    "], sparse_threshold=0)\n",
    "\n",
    "preprocessador2 = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numericas),\n",
    "    ('cat', OrdinalEncoder(), categoricas)\n",
    "], sparse_threshold=0)\n",
    "\n",
    "preprocessador3 = ColumnTransformer(transformers=[\n",
    "    ('num', MinMaxScaler(), numericas),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categoricas)\n",
    "], sparse_threshold=0)\n",
    "\n",
    "preprocessador4 = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numericas),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categoricas)\n",
    "], sparse_threshold=0)\n",
    "\n",
    "preprocessadores = {\n",
    "    'preprocessador 1': preprocessador1,\n",
    "    'preprocessador 2': preprocessador2,\n",
    "    'preprocessador 3': preprocessador3,\n",
    "    'preprocessador 4': preprocessador4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93864751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Resultados por pre-processador =======\n",
      "  Preprocessador  Accuracy   Recall  Fit Time  Score Time\n",
      "preprocessador 1  0.866221 0.505498  3.241317    0.658211\n",
      "preprocessador 2  0.859682 0.510967  2.884070    0.596061\n",
      "preprocessador 3  0.520724 0.561078  5.558237    1.307618\n",
      "preprocessador 4  0.524933 0.562239  5.183405    1.312365\n",
      "\n",
      "======= Média por pre-processador =======\n",
      "Média Accuracy: 0.6929\n",
      "Média Recall: 0.5349\n",
      "Média Fit Time:     4.2168 segundos\n",
      "Média Score Time:   0.9686 segundos\n",
      "\n",
      "============================================================\n",
      " ======= Melhor resultado =======\n",
      "============================================================\n",
      "Preprocessador:           preprocessador 4\n",
      "Accuracy:                 0.5249\n",
      "Recall:                   0.5622\n",
      "Fit Time:                 5.1834 segundos\n",
      "Score Time:               1.3124 segundos\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "resultadosNB = []\n",
    "\n",
    "for nome_prep, preprocessador in preprocessadores.items():\n",
    "    pipelineNB = Pipeline([\n",
    "        (\"preprocessador\", preprocessador),\n",
    "        (\"modelo\", GaussianNB())\n",
    "    ])\n",
    "\n",
    "    resultado = cross_validate(\n",
    "        pipelineNB, x, y, cv=5,\n",
    "        scoring=['accuracy', 'recall_macro'],\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    resultadosNB.append({\n",
    "        'Preprocessador': nome_prep,\n",
    "        'Accuracy': resultado['test_accuracy'].mean(),\n",
    "        'Recall': resultado['test_recall_macro'].mean(),\n",
    "        'Fit Time': resultado['fit_time'].mean(),\n",
    "        'Score Time': resultado['score_time'].mean()\n",
    "    })\n",
    "\n",
    "df_resultadosNB = pd.DataFrame(resultadosNB)\n",
    "df_resultadosNB = df_resultadosNB.sort_values(by='Preprocessador', ascending=True)\n",
    "\n",
    "print(\"\\n======= Resultados por pre-processador =======\")\n",
    "print(df_resultadosNB.to_string(index=False))\n",
    "\n",
    "media_acc = df_resultadosNB['Accuracy'].mean()\n",
    "media_recall = df_resultadosNB['Recall'].mean()\n",
    "media_fit_time = df_resultadosNB['Fit Time'].mean()\n",
    "media_score_time = df_resultadosNB['Score Time'].mean()\n",
    "\n",
    "print(\"\\n======= Média por pre-processador =======\")\n",
    "print(f\"Média Accuracy: {media_acc:.4f}\")\n",
    "print(f\"Média Recall: {media_recall:.4f}\")\n",
    "print(f\"Média Fit Time:     {media_fit_time:.4f} segundos\")\n",
    "print(f\"Média Score Time:   {media_score_time:.4f} segundos\")\n",
    "\n",
    "melhor = df_resultadosNB.loc[df_resultadosNB['Recall'].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" ======= Melhor resultado =======\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Preprocessador:':25} {melhor['Preprocessador']}\")\n",
    "print(f\"{'Accuracy:':25} {melhor['Accuracy']:.4f}\")\n",
    "print(f\"{'Recall:':25} {melhor['Recall']:.4f}\")\n",
    "print(f\"{'Fit Time:':25} {melhor['Fit Time']:.4f} segundos\")\n",
    "print(f\"{'Score Time:':25} {melhor['Score Time']:.4f} segundos\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_resultadosNB.to_csv('df_resultadosNB.csv', index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d26c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "resultadosDT = []\n",
    "\n",
    "for nome_prep, preprocessador in preprocessadores.items():\n",
    "    pipelineDT = Pipeline([\n",
    "        (\"preprocessador\", preprocessador),\n",
    "        (\"modelo\", DecisionTreeClassifier(class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    resultado = cross_validate(\n",
    "        pipelineDT, x, y, cv=5,\n",
    "        scoring=['accuracy', 'recall_macro'],\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    resultadosDT.append({\n",
    "        'Preprocessador': nome_prep,\n",
    "        'Accuracy': resultado['test_accuracy'].mean(),\n",
    "        'Recall': resultado['test_recall_macro'].mean(),\n",
    "        'Fit Time': resultado['fit_time'].mean(),\n",
    "        'Score Time': resultado['score_time'].mean()\n",
    "    })\n",
    "\n",
    "df_resultadosDT = pd.DataFrame(resultadosDT)\n",
    "df_resultadosDT = df_resultadosDT.sort_values(by='Preprocessador', ascending=True)\n",
    "\n",
    "print(\"\\n======= Resultados por pre-processador =======\")\n",
    "print(df_resultadosDT.to_string(index=False))\n",
    "\n",
    "media_acc = df_resultadosDT['Accuracy'].mean()\n",
    "media_recall = df_resultadosDT['Recall'].mean()\n",
    "media_fit_time = df_resultadosDT['Fit Time'].mean()\n",
    "media_score_time = df_resultadosDT['Score Time'].mean()\n",
    "\n",
    "print(\"\\n======= Media por pre-processador =======\")\n",
    "print(f\"Média Accuracy: {media_acc:.4f}\")\n",
    "print(f\"Média Recall: {media_recall:.4f}\")\n",
    "print(f\"Média Fit Time:     {media_fit_time:.4f} segundos\")\n",
    "print(f\"Média Score Time:   {media_score_time:.4f} segundos\")\n",
    "\n",
    "\n",
    "melhor = df_resultadosDT.loc[df_resultadosDT['Recall'].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" ======= Melhor resultado =======\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Preprocessador:':25} {melhor['Preprocessador']}\")\n",
    "print(f\"{'Accuracy:':25} {melhor['Accuracy']:.4f}\")\n",
    "print(f\"{'Recall:':25} {melhor['Recall']:.4f}\")\n",
    "print(f\"{'Fit Time:':25} {melhor['Fit Time']:.4f} segundos\")\n",
    "print(f\"{'Score Time:':25} {melhor['Score Time']:.4f} segundos\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_resultadosDT.to_csv('df_resultadosDT.csv', index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97285418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelo_parametrosRF = {\n",
    "    'Random Forest': {\n",
    "        'modelo': RandomForestClassifier(),\n",
    "        'parametros': {\n",
    "            'n_estimators': [10, 50 ,100],\n",
    "            'class_weight': ['balanced'],\n",
    "            'min_samples_leaf': [7],\n",
    "            'min_samples_split': [2]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "detalhes_RF = []\n",
    "\n",
    "for nome_prep, preprocessador in preprocessadores.items():\n",
    "    x_prep = x.copy()\n",
    "    x_prep = preprocessador.fit_transform(x_prep)\n",
    "    x_treinamento, x_teste, y_treinamento, y_teste = train_test_split(x_prep, y, test_size=0.15, random_state=0)\n",
    "\n",
    "    for nome_modelo, objeto in modelo_parametrosRF.items():\n",
    "\n",
    "        grid = GridSearchCV(objeto['modelo'], objeto['parametros'], cv=5, scoring='recall_macro')\n",
    "        grid.fit(x_treinamento, y_treinamento)\n",
    "\n",
    "        df_RF= pd.DataFrame(grid.cv_results_)\n",
    "        y_pred = grid.predict(x_teste)\n",
    "\n",
    "        df_RF['modelo'] = nome_modelo\n",
    "        df_RF['preprocessador'] = nome_prep\n",
    "        df_RF['n_estimators'] = df_RF['params'].apply(lambda x: x['n_estimators'])\n",
    "        \n",
    "        recall_scores = []\n",
    "        for params in df_RF['params']:\n",
    "            modelo = RandomForestClassifier(**params)\n",
    "            modelo.fit(x_treinamento, y_treinamento)\n",
    "            y_pred = modelo.predict(x_teste)\n",
    "            recall = recall_score(y_teste, y_pred, average='macro')\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "        df_RF['Recall'] = recall_scores\n",
    "\n",
    "        detalhes_RF.append(df_RF[[\n",
    "            'modelo', 'preprocessador', 'n_estimators',\n",
    "            'mean_test_score', 'Recall', 'mean_fit_time', 'mean_score_time',\n",
    "        ]])\n",
    "\n",
    "        \n",
    "\n",
    "df_detalhadoRF= pd.concat(detalhes_RF, ignore_index=True)\n",
    "df_detalhadoRF = df_detalhadoRF.sort_values(by=['preprocessador', 'n_estimators'], ascending=[True, True])\n",
    "\n",
    "for n_estimators, df_sub in df_detalhadoRF.groupby('n_estimators'):\n",
    "    print(f\"\\n=== Numero de arvores: {n_estimators}\")\n",
    "    print(df_sub.to_string(index=False))\n",
    "\n",
    "    for preproc, df_sub_prep in df_sub.groupby('preprocessador'):\n",
    "        print(f\"\\n--- Resultados para Preprocessador: {preproc} ---\")\n",
    "        print(df_sub_prep.to_string(index=False))\n",
    "\n",
    "    media_acc = df_sub['mean_test_score'].mean()\n",
    "    media_recall = df_sub['Recall'].mean()\n",
    "    print(f\"Média Accuracy: {media_acc:.4f} | Média Recall: {media_recall:.4f}\")\n",
    "\n",
    "print(\"\\n======= Media por pre-processador =======\")\n",
    "medias_por_preprocessador = df_detalhadoRF.groupby('preprocessador')[['mean_test_score', 'Recall']].mean().reset_index()\n",
    "\n",
    "for _, linha in medias_por_preprocessador.iterrows():\n",
    "    print(f\"Preprocessador: {linha['preprocessador']} | Média Accuracy: {linha['mean_test_score']:.4f} | Média Recall: {linha['Recall']:.4f}\")\n",
    "\n",
    "\n",
    "media_recall_total = df_detalhadoRF['Recall'].mean()\n",
    "media_acc_total = df_detalhadoRF['mean_test_score'].mean()\n",
    "print(f\"\\n =======  Media total ======= \\n Accuracy: {media_acc_total:.4f} | Recall: {media_recall_total:.4f}\")\n",
    "\n",
    "melhor_linhaRF = df_detalhadoRF.loc[df_detalhadoRF['Recall'].idxmax()]\n",
    "\n",
    "print(\"\\n======= Melhor Resultado Recall =======\")\n",
    "print(f\"Preprocessador: {melhor_linhaRF['preprocessador']}\")\n",
    "print(f\"Modelo: {melhor_linhaRF['modelo']}\")\n",
    "print(f\"n_estimators: {melhor_linhaRF['n_estimators']}\")\n",
    "print(f\"Accuracy (mean_test_score): {melhor_linhaRF['mean_test_score']:.4f}\")\n",
    "print(f\"Recall-score: {melhor_linhaRF['Recall']:.4f}\")\n",
    "print(f\"Tempo médio de treino: {melhor_linhaRF['mean_fit_time']:.4f} s\")\n",
    "print(f\"Tempo médio de score: {melhor_linhaRF['mean_score_time']:.4f} s\")\n",
    "\n",
    "\n",
    "df_detalhadoRF.to_csv('df_resultadosRF.csv', index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53abd072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessador: preprocessador 1 | Kernel: linear ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 1 linear 1.5         0.598959 0.534800     205.640691         7.019500\n",
      "   SVM preprocessador 1 linear 1.0         0.597831 0.532933     187.211527         7.441445\n",
      "   SVM preprocessador 1 linear 0.5         0.595283 0.528267     174.513356         7.605440\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 1 | Kernel: linear <<<\n",
      "Média Accuracy: 0.5974 | Média Recall: 0.5320\n",
      "\n",
      "=== Preprocessador: preprocessador 1 | Kernel: poly ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 1   poly 1.5         0.571818 0.616133     153.828377         6.345562\n",
      "   SVM preprocessador 1   poly 1.0         0.569858 0.605867     173.442150         8.560989\n",
      "   SVM preprocessador 1   poly 0.5         0.562779 0.578533     171.469141         9.135525\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 1 | Kernel: poly <<<\n",
      "Média Accuracy: 0.5682 | Média Recall: 0.6002\n",
      "\n",
      "=== Preprocessador: preprocessador 1 | Kernel: rbf ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 1    rbf 1.5         0.583104 0.554000     167.640420        43.224291\n",
      "   SVM preprocessador 1    rbf 1.0         0.579414 0.557867     168.103183        38.232778\n",
      "   SVM preprocessador 1    rbf 0.5         0.574246 0.558533     175.820661        44.873450\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 1 | Kernel: rbf <<<\n",
      "Média Accuracy: 0.5789 | Média Recall: 0.5568\n",
      "\n",
      "=== Preprocessador: preprocessador 1 | Kernel: sigmoid ===\n",
      "modelo   preprocessador  kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 1 sigmoid 1.5         0.509307 0.511200      84.486235         4.906549\n",
      "   SVM preprocessador 1 sigmoid 1.0         0.509474 0.512800     105.583077         8.081142\n",
      "   SVM preprocessador 1 sigmoid 0.5         0.508911 0.510133     103.420147         8.150630\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 1 | Kernel: sigmoid <<<\n",
      "Média Accuracy: 0.5092 | Média Recall: 0.5114\n",
      "\n",
      "=== Preprocessador: preprocessador 2 | Kernel: linear ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 2 linear 1.5         0.606279 0.542133     247.444393         5.913905\n",
      "   SVM preprocessador 2 linear 1.0         0.606319 0.542000     211.981931         7.043289\n",
      "   SVM preprocessador 2 linear 0.5         0.606229 0.541733     176.417805         7.756466\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 2 | Kernel: linear <<<\n",
      "Média Accuracy: 0.6063 | Média Recall: 0.5420\n",
      "\n",
      "=== Preprocessador: preprocessador 2 | Kernel: poly ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 2   poly 1.5         0.586887 0.645333     138.319722         5.927097\n",
      "   SVM preprocessador 2   poly 1.0         0.589580 0.610800     158.540360         8.772900\n",
      "   SVM preprocessador 2   poly 0.5         0.578107 0.506000     166.590661         9.079753\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 2 | Kernel: poly <<<\n",
      "Média Accuracy: 0.5849 | Média Recall: 0.5874\n",
      "\n",
      "=== Preprocessador: preprocessador 2 | Kernel: rbf ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 2    rbf 1.5         0.606729 0.543733     159.720948        39.155102\n",
      "   SVM preprocessador 2    rbf 1.0         0.607748 0.542533     158.112789        39.100082\n",
      "   SVM preprocessador 2    rbf 0.5         0.606860 0.544933     167.721188        41.125804\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 2 | Kernel: rbf <<<\n",
      "Média Accuracy: 0.6071 | Média Recall: 0.5437\n",
      "\n",
      "=== Preprocessador: preprocessador 2 | Kernel: sigmoid ===\n",
      "modelo   preprocessador  kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 2 sigmoid 1.5         0.512311 0.512933      76.229893         6.332450\n",
      "   SVM preprocessador 2 sigmoid 1.0         0.512591 0.513200      91.598728         7.744676\n",
      "   SVM preprocessador 2 sigmoid 0.5         0.512473 0.513333      89.060181         7.693079\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 2 | Kernel: sigmoid <<<\n",
      "Média Accuracy: 0.5125 | Média Recall: 0.5132\n",
      "\n",
      "=== Preprocessador: preprocessador 3 | Kernel: linear ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 3 linear 1.5         0.605061 0.543867     396.200173        37.372284\n",
      "   SVM preprocessador 3 linear 1.0         0.603807 0.543733     422.001298        32.595205\n",
      "   SVM preprocessador 3 linear 0.5         0.603289 0.540000     415.097853        34.492066\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 3 | Kernel: linear <<<\n",
      "Média Accuracy: 0.6041 | Média Recall: 0.5425\n",
      "\n",
      "=== Preprocessador: preprocessador 3 | Kernel: poly ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 3   poly 1.5         0.567093 0.601600     368.953100        23.377882\n",
      "   SVM preprocessador 3   poly 1.0         0.571617 0.595733     421.607934        33.502928\n",
      "   SVM preprocessador 3   poly 0.5         0.577333 0.584533     426.818007        35.966360\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 3 | Kernel: poly <<<\n",
      "Média Accuracy: 0.5720 | Média Recall: 0.5940\n",
      "\n",
      "=== Preprocessador: preprocessador 3 | Kernel: rbf ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 3    rbf 1.5         0.577273 0.597733     415.270218        65.192458\n",
      "   SVM preprocessador 3    rbf 1.0         0.581234 0.588933     438.973900        71.340504\n",
      "   SVM preprocessador 3    rbf 0.5         0.585687 0.576533     491.783193        69.936236\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 3 | Kernel: rbf <<<\n",
      "Média Accuracy: 0.5814 | Média Recall: 0.5877\n",
      "\n",
      "=== Preprocessador: preprocessador 3 | Kernel: sigmoid ===\n",
      "modelo   preprocessador  kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 3 sigmoid 1.5         0.519969 0.521867     290.065066        10.066592\n",
      "   SVM preprocessador 3 sigmoid 1.0         0.519806 0.521333     412.085022        20.480429\n",
      "   SVM preprocessador 3 sigmoid 0.5         0.519306 0.519333     382.932714        22.826410\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 3 | Kernel: sigmoid <<<\n",
      "Média Accuracy: 0.5197 | Média Recall: 0.5208\n",
      "\n",
      "=== Preprocessador: preprocessador 4 | Kernel: linear ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 4 linear 1.5         0.610463 0.552400     585.531847        32.250023\n",
      "   SVM preprocessador 4 linear 1.0         0.610260 0.552533     480.124444        29.709588\n",
      "   SVM preprocessador 4 linear 0.5         0.609981 0.552267     447.559974        33.515914\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 4 | Kernel: linear <<<\n",
      "Média Accuracy: 0.6102 | Média Recall: 0.5524\n",
      "\n",
      "=== Preprocessador: preprocessador 4 | Kernel: poly ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 4   poly 1.5         0.597123 0.622000     391.523928        21.817854\n",
      "   SVM preprocessador 4   poly 1.0         0.601183 0.615333     457.427478        38.822431\n",
      "   SVM preprocessador 4   poly 0.5         0.607178 0.604400     477.224058        36.597742\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 4 | Kernel: poly <<<\n",
      "Média Accuracy: 0.6018 | Média Recall: 0.6139\n",
      "\n",
      "=== Preprocessador: preprocessador 4 | Kernel: rbf ===\n",
      "modelo   preprocessador kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 4    rbf 1.5         0.604288 0.633733     445.053407        59.662174\n",
      "   SVM preprocessador 4    rbf 1.0         0.608894 0.627467     437.216991        65.460289\n",
      "   SVM preprocessador 4    rbf 0.5         0.614716 0.616533     482.874909        70.916667\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 4 | Kernel: rbf <<<\n",
      "Média Accuracy: 0.6093 | Média Recall: 0.6259\n",
      "\n",
      "=== Preprocessador: preprocessador 4 | Kernel: sigmoid ===\n",
      "modelo   preprocessador  kernel   C  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   SVM preprocessador 4 sigmoid 1.5         0.537433 0.538400     222.797549        10.590234\n",
      "   SVM preprocessador 4 sigmoid 1.0         0.537510 0.539200     361.590915        21.864199\n",
      "   SVM preprocessador 4 sigmoid 0.5         0.537483 0.540533     371.907382        21.293560\n",
      "\n",
      ">>> Medias para o Preprocessador: preprocessador 4 | Kernel: sigmoid <<<\n",
      "Média Accuracy: 0.5375 | Média Recall: 0.5394\n",
      "\n",
      "======= Media por pre-processador =======\n",
      "Preprocessador: preprocessador 1 | Média Accuracy: 0.5634 | Média Recall: 0.5501\n",
      "Preprocessador: preprocessador 2 | Média Accuracy: 0.5777 | Média Recall: 0.5466\n",
      "Preprocessador: preprocessador 3 | Média Accuracy: 0.5693 | Média Recall: 0.5613\n",
      "Preprocessador: preprocessador 4 | Média Accuracy: 0.5897 | Média Recall: 0.5829\n",
      "\n",
      " =======  Media total ======= \n",
      " Accuracy: 0.5750 | Recall: 0.5602\n",
      "\n",
      "======= Melhor resultado Recall =======\n",
      "Preprocessador: preprocessador 2\n",
      "Modelo: SVM\n",
      "Kernel: poly\n",
      "C: 1.5\n",
      "Accuracy (mean_test_score): 0.5869\n",
      "Recall-score: 0.6453\n",
      "Tempo médio de treino: 138.3197 s\n",
      "Tempo médio de score: 5.9271 s\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------SVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "amostra = x.copy()\n",
    "amostra['target'] = y\n",
    "amostra = amostra.sample(50000, random_state=0)\n",
    "\n",
    "x_sample = amostra.drop('target', axis=1)\n",
    "y_sample = amostra['target']\n",
    "\n",
    "modelo_parametrosSVM = {\n",
    "    'SVM': {\n",
    "        'modelo': SVC(),\n",
    "        'parametros': {\n",
    "            'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "            'C': [0.5, 1.0, 1.5],\n",
    "            'class_weight': ['balanced'],\n",
    "            'cache_size': [500]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "detalhes_SVM = []\n",
    "\n",
    "for nome_prep, preprocessador in preprocessadores.items():\n",
    "    x_sample_prep = x_sample.copy()\n",
    "    x_sample_prep = preprocessador.fit_transform(x_sample_prep)\n",
    "    x_treinamento, x_teste, y_treinamento, y_teste = train_test_split(x_sample_prep, y_sample, test_size=0.15, random_state=0)\n",
    "\n",
    "    for nome_modelo, objeto in modelo_parametrosSVM.items():\n",
    "        \n",
    "        grid = GridSearchCV(objeto['modelo'], objeto['parametros'], cv=5, scoring='recall_macro', n_jobs=-1)\n",
    "        grid.fit(x_treinamento, y_treinamento)\n",
    "\n",
    "        df_SVM = pd.DataFrame(grid.cv_results_)\n",
    "        y_pred = grid.predict(x_teste)\n",
    "\n",
    "        df_SVM['modelo'] = nome_modelo\n",
    "        df_SVM['preprocessador'] = nome_prep\n",
    "        df_SVM['kernel'] = df_SVM['params'].apply(lambda x: x['kernel'])\n",
    "        df_SVM['C'] = df_SVM['params'].apply(lambda x: x['C'])\n",
    "        \n",
    "        recall_scores = []\n",
    "        for params in df_SVM['params']:\n",
    "            modelo = SVC(**params)\n",
    "            modelo.fit(x_treinamento, y_treinamento)\n",
    "            y_pred = modelo.predict(x_teste)\n",
    "            recall = recall_score(y_teste, y_pred, average='weighted')\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "        df_SVM['Recall'] = recall_scores\n",
    "\n",
    "        detalhes_SVM.append(df_SVM[[\n",
    "            'modelo', 'preprocessador', 'kernel', 'C',\n",
    "            'mean_test_score', 'Recall', 'mean_fit_time', 'mean_score_time',\n",
    "        ]])\n",
    "\n",
    "        \n",
    "\n",
    "df_detalhadoSVM = pd.concat(detalhes_SVM, ignore_index=True)\n",
    "df_detalhadoSVM = df_detalhadoSVM.sort_values(by=['preprocessador', 'kernel', 'C'], ascending=[True, True, False])\n",
    "\n",
    "\n",
    "for (preproc, kernel), df_sub in df_detalhadoSVM.groupby(['preprocessador', 'kernel']):\n",
    "    print(f\"\\n=== Preprocessador: {preproc} | Kernel: {kernel} ===\")\n",
    "    print(df_sub.to_string(index=False))\n",
    "\n",
    "    media_acc = df_sub['mean_test_score'].mean()\n",
    "    media_recall = df_sub['Recall'].mean()\n",
    "    print(f\"\\n>>> Medias para o Preprocessador: {preproc} | Kernel: {kernel} <<<\")\n",
    "    print(f\"Média Accuracy: {media_acc:.4f} | Média Recall: {media_recall:.4f}\")\n",
    "\n",
    "media_recall_total = df_detalhadoSVM['Recall'].mean()\n",
    "media_acc_total = df_detalhadoSVM['mean_test_score'].mean()\n",
    "\n",
    "\n",
    "print(\"\\n======= Media por pre-processador =======\")\n",
    "medias_por_preprocessador = df_detalhadoSVM.groupby('preprocessador')[['mean_test_score', 'Recall']].mean().reset_index()\n",
    "\n",
    "for _, linha in medias_por_preprocessador.iterrows():\n",
    "    print(f\"Preprocessador: {linha['preprocessador']} | Média Accuracy: {linha['mean_test_score']:.4f} | Média Recall: {linha['Recall']:.4f}\")\n",
    "\n",
    "\n",
    "print(f\"\\n =======  Media total ======= \\n Accuracy: {media_acc_total:.4f} | Recall: {media_recall_total:.4f}\")\n",
    "\n",
    "melhor_linhaSVM = df_detalhadoSVM.loc[df_detalhadoSVM['Recall'].idxmax()]\n",
    "\n",
    "print(\"\\n======= Melhor resultado Recall =======\")\n",
    "print(f\"Preprocessador: {melhor_linhaSVM['preprocessador']}\")\n",
    "print(f\"Modelo: {melhor_linhaSVM['modelo']}\")\n",
    "print(f\"Kernel: {melhor_linhaSVM['kernel']}\")\n",
    "print(f\"C: {melhor_linhaSVM['C']}\")\n",
    "print(f\"Accuracy (mean_test_score): {melhor_linhaSVM['mean_test_score']:.4f}\")\n",
    "print(f\"Recall-score: {melhor_linhaSVM['Recall']:.4f}\")\n",
    "print(f\"Tempo médio de treino: {melhor_linhaSVM['mean_fit_time']:.4f} s\")\n",
    "print(f\"Tempo médio de score: {melhor_linhaSVM['mean_score_time']:.4f} s\")\n",
    "\n",
    "df_detalhadoSVM.to_csv('df_resultadosSVM.csv', index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad6b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68042127\n",
      "Iteration 2, loss = 0.66569182\n",
      "Iteration 3, loss = 0.66211988\n",
      "Iteration 4, loss = 0.66062704\n",
      "Iteration 5, loss = 0.65819796\n",
      "Iteration 6, loss = 0.65763216\n",
      "Iteration 7, loss = 0.65783524\n",
      "Iteration 8, loss = 0.65639479\n",
      "Iteration 9, loss = 0.65532064\n",
      "Iteration 10, loss = 0.65456339\n",
      "Iteration 11, loss = 0.65507152\n",
      "Iteration 12, loss = 0.65360571\n",
      "Iteration 13, loss = 0.65333565\n",
      "Iteration 14, loss = 0.65255790\n",
      "Iteration 15, loss = 0.65242005\n",
      "Iteration 16, loss = 0.65179182\n",
      "Iteration 17, loss = 0.65097283\n",
      "Iteration 18, loss = 0.65068764\n",
      "Iteration 19, loss = 0.64973398\n",
      "Iteration 20, loss = 0.64976171\n",
      "Iteration 21, loss = 0.64998992\n",
      "Iteration 22, loss = 0.64948257\n",
      "Iteration 23, loss = 0.64894991\n",
      "Iteration 24, loss = 0.64805508\n",
      "Iteration 25, loss = 0.64814760\n",
      "Iteration 26, loss = 0.64735861\n",
      "Iteration 27, loss = 0.64619569\n",
      "Iteration 28, loss = 0.64751129\n",
      "Iteration 29, loss = 0.64591002\n",
      "Iteration 30, loss = 0.64508129\n",
      "Iteration 31, loss = 0.64550636\n",
      "Iteration 32, loss = 0.64523328\n",
      "Iteration 33, loss = 0.64470595\n",
      "Iteration 34, loss = 0.64465298\n",
      "Iteration 35, loss = 0.64383834\n",
      "Iteration 36, loss = 0.64366692\n",
      "Iteration 37, loss = 0.64258709\n",
      "Iteration 38, loss = 0.64248129\n",
      "Iteration 39, loss = 0.64261312\n",
      "Iteration 40, loss = 0.64213795\n",
      "Iteration 41, loss = 0.64137694\n",
      "Iteration 42, loss = 0.64097842\n",
      "Iteration 43, loss = 0.64039625\n",
      "Iteration 44, loss = 0.64045831\n",
      "Iteration 45, loss = 0.63955098\n",
      "Iteration 46, loss = 0.63889586\n",
      "Iteration 47, loss = 0.63873125\n",
      "Iteration 48, loss = 0.63826374\n",
      "Iteration 49, loss = 0.63787794\n",
      "Iteration 50, loss = 0.63725964\n",
      "Iteration 51, loss = 0.63721546\n",
      "Iteration 52, loss = 0.63620610\n",
      "Iteration 53, loss = 0.63615907\n",
      "Iteration 54, loss = 0.63563110\n",
      "Iteration 55, loss = 0.63525270\n",
      "Iteration 56, loss = 0.63465875\n",
      "Iteration 57, loss = 0.63423023\n",
      "Iteration 58, loss = 0.63380818\n",
      "Iteration 59, loss = 0.63403905\n",
      "Iteration 60, loss = 0.63279763\n",
      "Iteration 61, loss = 0.63316113\n",
      "Iteration 62, loss = 0.63215556\n",
      "Iteration 63, loss = 0.63176968\n",
      "Iteration 64, loss = 0.63132645\n",
      "Iteration 65, loss = 0.63073771\n",
      "Iteration 66, loss = 0.63001574\n",
      "Iteration 67, loss = 0.63028668\n",
      "Iteration 68, loss = 0.62928445\n",
      "Iteration 69, loss = 0.62828329\n",
      "Iteration 70, loss = 0.62822370\n",
      "Iteration 71, loss = 0.62815778\n",
      "Iteration 72, loss = 0.62728767\n",
      "Iteration 73, loss = 0.62663517\n",
      "Iteration 74, loss = 0.62608211\n",
      "Iteration 75, loss = 0.62607196\n",
      "Iteration 76, loss = 0.62522540\n",
      "Iteration 77, loss = 0.62515300\n",
      "Iteration 78, loss = 0.62419265\n",
      "Iteration 79, loss = 0.62433404\n",
      "Iteration 80, loss = 0.62329633\n",
      "Iteration 81, loss = 0.62342409\n",
      "Iteration 82, loss = 0.62241965\n",
      "Iteration 83, loss = 0.62228442\n",
      "Iteration 84, loss = 0.62130232\n",
      "Iteration 85, loss = 0.62076649\n",
      "Iteration 86, loss = 0.62032627\n",
      "Iteration 87, loss = 0.61998740\n",
      "Iteration 88, loss = 0.61928314\n",
      "Iteration 89, loss = 0.61914254\n",
      "Iteration 90, loss = 0.61842606\n",
      "Iteration 91, loss = 0.61744247\n",
      "Iteration 92, loss = 0.61688371\n",
      "Iteration 93, loss = 0.61702555\n",
      "Iteration 94, loss = 0.61612550\n",
      "Iteration 95, loss = 0.61565901\n",
      "Iteration 96, loss = 0.61519983\n",
      "Iteration 97, loss = 0.61379102\n",
      "Iteration 98, loss = 0.61398907\n",
      "Iteration 99, loss = 0.61347337\n",
      "Iteration 100, loss = 0.61276762\n",
      "Iteration 101, loss = 0.61175267\n",
      "Iteration 102, loss = 0.61195419\n",
      "Iteration 103, loss = 0.61122647\n",
      "Iteration 104, loss = 0.61038315\n",
      "Iteration 105, loss = 0.61055807\n",
      "Iteration 106, loss = 0.60927542\n",
      "Iteration 107, loss = 0.60850381\n",
      "Iteration 108, loss = 0.60897541\n",
      "Iteration 109, loss = 0.60678309\n",
      "Iteration 110, loss = 0.60655904\n",
      "Iteration 111, loss = 0.60624842\n",
      "Iteration 112, loss = 0.60628788\n",
      "Iteration 113, loss = 0.60468148\n",
      "Iteration 114, loss = 0.60485310\n",
      "Iteration 115, loss = 0.60410369\n",
      "Iteration 116, loss = 0.60354361\n",
      "Iteration 117, loss = 0.60371953\n",
      "Iteration 118, loss = 0.60209938\n",
      "Iteration 119, loss = 0.60229131\n",
      "Iteration 120, loss = 0.60108576\n",
      "Iteration 121, loss = 0.60026933\n",
      "Iteration 122, loss = 0.60006426\n",
      "Iteration 123, loss = 0.59969417\n",
      "Iteration 124, loss = 0.59905298\n",
      "Iteration 125, loss = 0.59816348\n",
      "Iteration 126, loss = 0.59824750\n",
      "Iteration 127, loss = 0.59800577\n",
      "Iteration 128, loss = 0.59737891\n",
      "Iteration 129, loss = 0.59590246\n",
      "Iteration 130, loss = 0.59499122\n",
      "Iteration 131, loss = 0.59466933\n",
      "Iteration 132, loss = 0.59416896\n",
      "Iteration 133, loss = 0.59377671\n",
      "Iteration 134, loss = 0.59303695\n",
      "Iteration 135, loss = 0.59243147\n",
      "Iteration 136, loss = 0.59184028\n",
      "Iteration 137, loss = 0.59093189\n",
      "Iteration 138, loss = 0.59130747\n",
      "Iteration 139, loss = 0.59002091\n",
      "Iteration 140, loss = 0.58984241\n",
      "Iteration 141, loss = 0.58931104\n",
      "Iteration 142, loss = 0.58889825\n",
      "Iteration 143, loss = 0.58751472\n",
      "Iteration 144, loss = 0.58752932\n",
      "Iteration 145, loss = 0.58714212\n",
      "Iteration 146, loss = 0.58674250\n",
      "Iteration 147, loss = 0.58508875\n",
      "Iteration 148, loss = 0.58568363\n",
      "Iteration 149, loss = 0.58479206\n",
      "Iteration 150, loss = 0.58401112\n",
      "Iteration 151, loss = 0.58340764\n",
      "Iteration 152, loss = 0.58367071\n",
      "Iteration 153, loss = 0.58242585\n",
      "Iteration 154, loss = 0.58082534\n",
      "Iteration 155, loss = 0.58226680\n",
      "Iteration 156, loss = 0.58037521\n",
      "Iteration 157, loss = 0.58023172\n",
      "Iteration 158, loss = 0.58006854\n",
      "Iteration 159, loss = 0.57929115\n",
      "Iteration 160, loss = 0.57823566\n",
      "Iteration 161, loss = 0.57880156\n",
      "Iteration 162, loss = 0.57715826\n",
      "Iteration 163, loss = 0.57735256\n",
      "Iteration 164, loss = 0.57654099\n",
      "Iteration 165, loss = 0.57578429\n",
      "Iteration 166, loss = 0.57647330\n",
      "Iteration 167, loss = 0.57467340\n",
      "Iteration 168, loss = 0.57455471\n",
      "Iteration 169, loss = 0.57386206\n",
      "Iteration 170, loss = 0.57369128\n",
      "Iteration 171, loss = 0.57367352\n",
      "Iteration 172, loss = 0.57225891\n",
      "Iteration 173, loss = 0.57125556\n",
      "Iteration 174, loss = 0.57160445\n",
      "Iteration 175, loss = 0.57008350\n",
      "Iteration 176, loss = 0.57055359\n",
      "Iteration 177, loss = 0.56953531\n",
      "Iteration 178, loss = 0.56900407\n",
      "Iteration 179, loss = 0.56883809\n",
      "Iteration 180, loss = 0.56794815\n",
      "Iteration 181, loss = 0.56736351\n",
      "Iteration 182, loss = 0.56696430\n",
      "Iteration 183, loss = 0.56665861\n",
      "Iteration 184, loss = 0.56626504\n",
      "Iteration 185, loss = 0.56557636\n",
      "Iteration 186, loss = 0.56549596\n",
      "Iteration 187, loss = 0.56489796\n",
      "Iteration 188, loss = 0.56456243\n",
      "Iteration 189, loss = 0.56416359\n",
      "Iteration 190, loss = 0.56303603\n",
      "Iteration 191, loss = 0.56316064\n",
      "Iteration 192, loss = 0.56164986\n",
      "Iteration 193, loss = 0.56130347\n",
      "Iteration 194, loss = 0.56148233\n",
      "Iteration 195, loss = 0.56105859\n",
      "Iteration 196, loss = 0.56038330\n",
      "Iteration 197, loss = 0.55976714\n",
      "Iteration 198, loss = 0.55976986\n",
      "Iteration 199, loss = 0.55854554\n",
      "Iteration 200, loss = 0.55810526\n",
      "Iteration 201, loss = 0.55791294\n",
      "Iteration 202, loss = 0.55700515\n",
      "Iteration 203, loss = 0.55730225\n",
      "Iteration 204, loss = 0.55635214\n",
      "Iteration 205, loss = 0.55601605\n",
      "Iteration 206, loss = 0.55590036\n",
      "Iteration 207, loss = 0.55479148\n",
      "Iteration 208, loss = 0.55477494\n",
      "Iteration 209, loss = 0.55507106\n",
      "Iteration 210, loss = 0.55365389\n",
      "Iteration 211, loss = 0.55301269\n",
      "Iteration 212, loss = 0.55312676\n",
      "Iteration 213, loss = 0.55193381\n",
      "Iteration 214, loss = 0.55218008\n",
      "Iteration 215, loss = 0.55155100\n",
      "Iteration 216, loss = 0.55033476\n",
      "Iteration 217, loss = 0.55069796\n",
      "Iteration 218, loss = 0.54985101\n",
      "Iteration 219, loss = 0.54988983\n",
      "Iteration 220, loss = 0.54923351\n",
      "Iteration 221, loss = 0.54943915\n",
      "Iteration 222, loss = 0.54825620\n",
      "Iteration 223, loss = 0.54849369\n",
      "Iteration 224, loss = 0.54747586\n",
      "Iteration 225, loss = 0.54714432\n",
      "Iteration 226, loss = 0.54718834\n",
      "Iteration 227, loss = 0.54692190\n",
      "Iteration 228, loss = 0.54558999\n",
      "Iteration 229, loss = 0.54552812\n",
      "Iteration 230, loss = 0.54648304\n",
      "Iteration 231, loss = 0.54458642\n",
      "Iteration 232, loss = 0.54468485\n",
      "Iteration 233, loss = 0.54450238\n",
      "Iteration 234, loss = 0.54310331\n",
      "Iteration 235, loss = 0.54402845\n",
      "Iteration 236, loss = 0.54305160\n",
      "Iteration 237, loss = 0.54229823\n",
      "Iteration 238, loss = 0.54235059\n",
      "Iteration 239, loss = 0.54213291\n",
      "Iteration 240, loss = 0.54208688\n",
      "Iteration 241, loss = 0.54222679\n",
      "Iteration 242, loss = 0.54039272\n",
      "Iteration 243, loss = 0.54016459\n",
      "Iteration 244, loss = 0.54024248\n",
      "Iteration 245, loss = 0.53985974\n",
      "Iteration 246, loss = 0.53932046\n",
      "Iteration 247, loss = 0.53889621\n",
      "Iteration 248, loss = 0.53816638\n",
      "Iteration 249, loss = 0.53855559\n",
      "Iteration 250, loss = 0.53825028\n",
      "Iteration 251, loss = 0.53824471\n",
      "Iteration 252, loss = 0.53798898\n",
      "Iteration 253, loss = 0.53740335\n",
      "Iteration 254, loss = 0.53762897\n",
      "Iteration 255, loss = 0.53646856\n",
      "Iteration 256, loss = 0.53631917\n",
      "Iteration 257, loss = 0.53628435\n",
      "Iteration 258, loss = 0.53636022\n",
      "Iteration 259, loss = 0.53529429\n",
      "Iteration 260, loss = 0.53480870\n",
      "Iteration 261, loss = 0.53504931\n",
      "Iteration 262, loss = 0.53457957\n",
      "Iteration 263, loss = 0.53340503\n",
      "Iteration 264, loss = 0.53387743\n",
      "Iteration 265, loss = 0.53329855\n",
      "Iteration 266, loss = 0.53306841\n",
      "Iteration 267, loss = 0.53347463\n",
      "Iteration 268, loss = 0.53268907\n",
      "Iteration 269, loss = 0.53290432\n",
      "Iteration 270, loss = 0.53156118\n",
      "Iteration 271, loss = 0.53198644\n",
      "Iteration 272, loss = 0.53164794\n",
      "Iteration 273, loss = 0.53163707\n",
      "Iteration 274, loss = 0.53094597\n",
      "Iteration 275, loss = 0.53031574\n",
      "Iteration 276, loss = 0.53038049\n",
      "Iteration 277, loss = 0.53083825\n",
      "Iteration 278, loss = 0.53072677\n",
      "Iteration 279, loss = 0.52973155\n",
      "Iteration 280, loss = 0.52943658\n",
      "Iteration 281, loss = 0.52986120\n",
      "Iteration 282, loss = 0.52908537\n",
      "Iteration 283, loss = 0.52862474\n",
      "Iteration 284, loss = 0.52793960\n",
      "Iteration 285, loss = 0.52835489\n",
      "Iteration 286, loss = 0.52777769\n",
      "Iteration 287, loss = 0.52825515\n",
      "Iteration 288, loss = 0.52704932\n",
      "Iteration 289, loss = 0.52800576\n",
      "Iteration 290, loss = 0.52717012\n",
      "Iteration 291, loss = 0.52648567\n",
      "Iteration 292, loss = 0.52623105\n",
      "Iteration 293, loss = 0.52643527\n",
      "Iteration 294, loss = 0.52593006\n",
      "Iteration 295, loss = 0.52549066\n",
      "Iteration 296, loss = 0.52557327\n",
      "Iteration 297, loss = 0.52498607\n",
      "Iteration 298, loss = 0.52480353\n",
      "Iteration 299, loss = 0.52500764\n",
      "Iteration 300, loss = 0.52467751\n",
      "Iteration 301, loss = 0.52426806\n",
      "Iteration 302, loss = 0.52434728\n",
      "Iteration 303, loss = 0.52499212\n",
      "Iteration 304, loss = 0.52375723\n",
      "Iteration 305, loss = 0.52338615\n",
      "Iteration 306, loss = 0.52301910\n",
      "Iteration 307, loss = 0.52324201\n",
      "Iteration 308, loss = 0.52296507\n",
      "Iteration 309, loss = 0.52193484\n",
      "Iteration 310, loss = 0.52208390\n",
      "Iteration 311, loss = 0.52235845\n",
      "Iteration 312, loss = 0.52250645\n",
      "Iteration 313, loss = 0.52124856\n",
      "Iteration 314, loss = 0.52243562\n",
      "Iteration 315, loss = 0.52131137\n",
      "Iteration 316, loss = 0.52135975\n",
      "Iteration 317, loss = 0.52165072\n",
      "Iteration 318, loss = 0.52106081\n",
      "Iteration 319, loss = 0.52004449\n",
      "Iteration 320, loss = 0.52003097\n",
      "Iteration 321, loss = 0.51996899\n",
      "Iteration 322, loss = 0.51991068\n",
      "Iteration 323, loss = 0.51969787\n",
      "Iteration 324, loss = 0.51900769\n",
      "Iteration 325, loss = 0.51908446\n",
      "Iteration 326, loss = 0.51914585\n",
      "Iteration 327, loss = 0.51869487\n",
      "Iteration 328, loss = 0.51876668\n",
      "Iteration 329, loss = 0.51841775\n",
      "Iteration 330, loss = 0.51845849\n",
      "Iteration 331, loss = 0.51850069\n",
      "Iteration 332, loss = 0.51851792\n",
      "Iteration 333, loss = 0.51888100\n",
      "Iteration 334, loss = 0.51747319\n",
      "Iteration 335, loss = 0.51752316\n",
      "Iteration 336, loss = 0.51718410\n",
      "Iteration 337, loss = 0.51694896\n",
      "Iteration 338, loss = 0.51660294\n",
      "Iteration 339, loss = 0.51674627\n",
      "Iteration 340, loss = 0.51580895\n",
      "Iteration 341, loss = 0.51559309\n",
      "Iteration 342, loss = 0.51642410\n",
      "Iteration 343, loss = 0.51572793\n",
      "Iteration 344, loss = 0.51608554\n",
      "Iteration 345, loss = 0.51505194\n",
      "Iteration 346, loss = 0.51556557\n",
      "Iteration 347, loss = 0.51518834\n",
      "Iteration 348, loss = 0.51528902\n",
      "Iteration 349, loss = 0.51429331\n",
      "Iteration 350, loss = 0.51509927\n",
      "Iteration 351, loss = 0.51478090\n",
      "Iteration 352, loss = 0.51538146\n",
      "Iteration 353, loss = 0.51363365\n",
      "Iteration 354, loss = 0.51417289\n",
      "Iteration 355, loss = 0.51369123\n",
      "Iteration 356, loss = 0.51338368\n",
      "Iteration 357, loss = 0.51330457\n",
      "Iteration 358, loss = 0.51374513\n",
      "Iteration 359, loss = 0.51341219\n",
      "Iteration 360, loss = 0.51394892\n",
      "Iteration 361, loss = 0.51300963\n",
      "Iteration 362, loss = 0.51344977\n",
      "Iteration 363, loss = 0.51242856\n",
      "Iteration 364, loss = 0.51215650\n",
      "Iteration 365, loss = 0.51214534\n",
      "Iteration 366, loss = 0.51235666\n",
      "Iteration 367, loss = 0.51151739\n",
      "Iteration 368, loss = 0.51158586\n",
      "Iteration 369, loss = 0.51240356\n",
      "Iteration 370, loss = 0.51180305\n",
      "Iteration 371, loss = 0.51105976\n",
      "Iteration 372, loss = 0.51066241\n",
      "Iteration 373, loss = 0.51107192\n",
      "Iteration 374, loss = 0.51065547\n",
      "Iteration 375, loss = 0.51055290\n",
      "Iteration 376, loss = 0.51022244\n",
      "Iteration 377, loss = 0.50991460\n",
      "Iteration 378, loss = 0.51053400\n",
      "Iteration 379, loss = 0.50997753\n",
      "Iteration 380, loss = 0.51044670\n",
      "Iteration 381, loss = 0.50981156\n",
      "Iteration 382, loss = 0.50977014\n",
      "Iteration 383, loss = 0.50914995\n",
      "Iteration 384, loss = 0.50899630\n",
      "Iteration 385, loss = 0.50864904\n",
      "Iteration 386, loss = 0.50917085\n",
      "Iteration 387, loss = 0.50892984\n",
      "Iteration 388, loss = 0.50897306\n",
      "Iteration 389, loss = 0.50830429\n",
      "Iteration 390, loss = 0.50880200\n",
      "Iteration 391, loss = 0.50840600\n",
      "Iteration 392, loss = 0.50852069\n",
      "Iteration 393, loss = 0.50810455\n",
      "Iteration 394, loss = 0.50727569\n",
      "Iteration 395, loss = 0.50708346\n",
      "Iteration 396, loss = 0.50745912\n",
      "Iteration 397, loss = 0.50686464\n",
      "Iteration 398, loss = 0.50673837\n",
      "Iteration 399, loss = 0.50749847\n",
      "Iteration 400, loss = 0.50679971\n",
      "Iteration 401, loss = 0.50671828\n",
      "Iteration 402, loss = 0.50603789\n",
      "Iteration 403, loss = 0.50669954\n",
      "Iteration 404, loss = 0.50546551\n",
      "Iteration 405, loss = 0.50586470\n",
      "Iteration 406, loss = 0.50585973\n",
      "Iteration 407, loss = 0.50600032\n",
      "Iteration 408, loss = 0.50515023\n",
      "Iteration 409, loss = 0.50565264\n",
      "Iteration 410, loss = 0.50558059\n",
      "Iteration 411, loss = 0.50457006\n",
      "Iteration 412, loss = 0.50485793\n",
      "Iteration 413, loss = 0.50379621\n",
      "Iteration 414, loss = 0.50447286\n",
      "Iteration 415, loss = 0.50409806\n",
      "Iteration 416, loss = 0.50374510\n",
      "Iteration 417, loss = 0.50385331\n",
      "Iteration 418, loss = 0.50469957\n",
      "Iteration 419, loss = 0.50292490\n",
      "Iteration 420, loss = 0.50352523\n",
      "Iteration 421, loss = 0.50335665\n",
      "Iteration 422, loss = 0.50294338\n",
      "Iteration 423, loss = 0.50337514\n",
      "Iteration 424, loss = 0.50312772\n",
      "Iteration 425, loss = 0.50285027\n",
      "Iteration 426, loss = 0.50230930\n",
      "Iteration 427, loss = 0.50230171\n",
      "Iteration 428, loss = 0.50230765\n",
      "Iteration 429, loss = 0.50174602\n",
      "Iteration 430, loss = 0.50218341\n",
      "Iteration 431, loss = 0.50235386\n",
      "Iteration 432, loss = 0.50147489\n",
      "Iteration 433, loss = 0.50183166\n",
      "Iteration 434, loss = 0.50207102\n",
      "Iteration 435, loss = 0.50094943\n",
      "Iteration 436, loss = 0.50142942\n",
      "Iteration 437, loss = 0.50099240\n",
      "Iteration 438, loss = 0.50162257\n",
      "Iteration 439, loss = 0.50036574\n",
      "Iteration 440, loss = 0.50113531\n",
      "Iteration 441, loss = 0.50070784\n",
      "Iteration 442, loss = 0.50044796\n",
      "Iteration 443, loss = 0.50065094\n",
      "Iteration 444, loss = 0.49968441\n",
      "Iteration 445, loss = 0.49961264\n",
      "Iteration 446, loss = 0.49995009\n",
      "Iteration 447, loss = 0.49988839\n",
      "Iteration 448, loss = 0.49917916\n",
      "Iteration 449, loss = 0.49935722\n",
      "Iteration 450, loss = 0.49926219\n",
      "Iteration 451, loss = 0.49921053\n",
      "Iteration 452, loss = 0.49857771\n",
      "Iteration 453, loss = 0.49886019\n",
      "Iteration 454, loss = 0.49894573\n",
      "Iteration 455, loss = 0.49809082\n",
      "Iteration 456, loss = 0.49913238\n",
      "Iteration 457, loss = 0.49804966\n",
      "Iteration 458, loss = 0.49882418\n",
      "Iteration 459, loss = 0.49760083\n",
      "Iteration 460, loss = 0.49846610\n",
      "Iteration 461, loss = 0.49784861\n",
      "Iteration 462, loss = 0.49819921\n",
      "Iteration 463, loss = 0.49748547\n",
      "Iteration 464, loss = 0.49722772\n",
      "Iteration 465, loss = 0.49766168\n",
      "Iteration 466, loss = 0.49708017\n",
      "Iteration 467, loss = 0.49724183\n",
      "Iteration 468, loss = 0.49709580\n",
      "Iteration 469, loss = 0.49697652\n",
      "Iteration 470, loss = 0.49652336\n",
      "Iteration 471, loss = 0.49668549\n",
      "Iteration 472, loss = 0.49662179\n",
      "Iteration 473, loss = 0.49634161\n",
      "Iteration 474, loss = 0.49588739\n",
      "Iteration 475, loss = 0.49690290\n",
      "Iteration 476, loss = 0.49614732\n",
      "Iteration 477, loss = 0.49579542\n",
      "Iteration 478, loss = 0.49556845\n",
      "Iteration 479, loss = 0.49558145\n",
      "Iteration 480, loss = 0.49498970\n",
      "Iteration 481, loss = 0.49472019\n",
      "Iteration 482, loss = 0.49496161\n",
      "Iteration 483, loss = 0.49513639\n",
      "Iteration 484, loss = 0.49472770\n",
      "Iteration 485, loss = 0.49489302\n",
      "Iteration 486, loss = 0.49515700\n",
      "Iteration 487, loss = 0.49385847\n",
      "Iteration 488, loss = 0.49451892\n",
      "Iteration 489, loss = 0.49360899\n",
      "Iteration 490, loss = 0.49380305\n",
      "Iteration 491, loss = 0.49422210\n",
      "Iteration 492, loss = 0.49374766\n",
      "Iteration 493, loss = 0.49355093\n",
      "Iteration 494, loss = 0.49323812\n",
      "Iteration 495, loss = 0.49293063\n",
      "Iteration 496, loss = 0.49352072\n",
      "Iteration 497, loss = 0.49342097\n",
      "Iteration 498, loss = 0.49253403\n",
      "Iteration 499, loss = 0.49259082\n",
      "Iteration 500, loss = 0.49285181\n",
      "Iteration 501, loss = 0.49177249\n",
      "Iteration 502, loss = 0.49285994\n",
      "Iteration 503, loss = 0.49248385\n",
      "Iteration 504, loss = 0.49216900\n",
      "Iteration 505, loss = 0.49220620\n",
      "Iteration 506, loss = 0.49221228\n",
      "Iteration 507, loss = 0.49134322\n",
      "Iteration 508, loss = 0.49187777\n",
      "Iteration 509, loss = 0.49127395\n",
      "Iteration 510, loss = 0.49093463\n",
      "Iteration 511, loss = 0.49135351\n",
      "Iteration 512, loss = 0.49027410\n",
      "Iteration 513, loss = 0.49067022\n",
      "Iteration 514, loss = 0.49121064\n",
      "Iteration 515, loss = 0.49090490\n",
      "Iteration 516, loss = 0.49078618\n",
      "Iteration 517, loss = 0.48966906\n",
      "Iteration 518, loss = 0.49005060\n",
      "Iteration 519, loss = 0.49053593\n",
      "Iteration 520, loss = 0.49019031\n",
      "Iteration 521, loss = 0.48940552\n",
      "Iteration 522, loss = 0.48984878\n",
      "Iteration 523, loss = 0.49061773\n",
      "Iteration 524, loss = 0.48910413\n",
      "Iteration 525, loss = 0.48909127\n",
      "Iteration 526, loss = 0.48933283\n",
      "Iteration 527, loss = 0.48903442\n",
      "Iteration 528, loss = 0.48928920\n",
      "Iteration 529, loss = 0.48914057\n",
      "Iteration 530, loss = 0.48967501\n",
      "Iteration 531, loss = 0.48830713\n",
      "Iteration 532, loss = 0.48882453\n",
      "Iteration 533, loss = 0.48824543\n",
      "Iteration 534, loss = 0.48754258\n",
      "Iteration 535, loss = 0.48876339\n",
      "Iteration 536, loss = 0.48853830\n",
      "Iteration 537, loss = 0.48736416\n",
      "Iteration 538, loss = 0.48798071\n",
      "Iteration 539, loss = 0.48710429\n",
      "Iteration 540, loss = 0.48809068\n",
      "Iteration 541, loss = 0.48706108\n",
      "Iteration 542, loss = 0.48718629\n",
      "Iteration 543, loss = 0.48730011\n",
      "Iteration 544, loss = 0.48679416\n",
      "Iteration 545, loss = 0.48697102\n",
      "Iteration 546, loss = 0.48702146\n",
      "Iteration 547, loss = 0.48697466\n",
      "Iteration 548, loss = 0.48575190\n",
      "Iteration 549, loss = 0.48614038\n",
      "Iteration 550, loss = 0.48701159\n",
      "Iteration 551, loss = 0.48594714\n",
      "Iteration 552, loss = 0.48635195\n",
      "Iteration 553, loss = 0.48671226\n",
      "Iteration 554, loss = 0.48603879\n",
      "Iteration 555, loss = 0.48515372\n",
      "Iteration 556, loss = 0.48504783\n",
      "Iteration 557, loss = 0.48613122\n",
      "Iteration 558, loss = 0.48589304\n",
      "Iteration 559, loss = 0.48506059\n",
      "Iteration 560, loss = 0.48469680\n",
      "Iteration 561, loss = 0.48548968\n",
      "Iteration 562, loss = 0.48554241\n",
      "Iteration 563, loss = 0.48477050\n",
      "Iteration 564, loss = 0.48476997\n",
      "Iteration 565, loss = 0.48420431\n",
      "Iteration 566, loss = 0.48461001\n",
      "Iteration 567, loss = 0.48460313\n",
      "Iteration 568, loss = 0.48485619\n",
      "Iteration 569, loss = 0.48435055\n",
      "Iteration 570, loss = 0.48471772\n",
      "Iteration 571, loss = 0.48402737\n",
      "Iteration 572, loss = 0.48326807\n",
      "Iteration 573, loss = 0.48384423\n",
      "Iteration 574, loss = 0.48359261\n",
      "Iteration 575, loss = 0.48412732\n",
      "Iteration 576, loss = 0.48330996\n",
      "Iteration 577, loss = 0.48387773\n",
      "Iteration 578, loss = 0.48389987\n",
      "Iteration 579, loss = 0.48387860\n",
      "Iteration 580, loss = 0.48269490\n",
      "Iteration 581, loss = 0.48305307\n",
      "Iteration 582, loss = 0.48289747\n",
      "Iteration 583, loss = 0.48339341\n",
      "Iteration 584, loss = 0.48255931\n",
      "Iteration 585, loss = 0.48261500\n",
      "Iteration 586, loss = 0.48289961\n",
      "Iteration 587, loss = 0.48186131\n",
      "Iteration 588, loss = 0.48257351\n",
      "Iteration 589, loss = 0.48092335\n",
      "Iteration 590, loss = 0.48145945\n",
      "Iteration 591, loss = 0.48201532\n",
      "Iteration 592, loss = 0.48221941\n",
      "Iteration 593, loss = 0.48127892\n",
      "Iteration 594, loss = 0.48056069\n",
      "Iteration 595, loss = 0.48258721\n",
      "Iteration 596, loss = 0.48208602\n",
      "Iteration 597, loss = 0.48177280\n",
      "Iteration 598, loss = 0.48073744\n",
      "Iteration 599, loss = 0.48089310\n",
      "Iteration 600, loss = 0.48136531\n",
      "Iteration 601, loss = 0.47977490\n",
      "Iteration 602, loss = 0.48118825\n",
      "Iteration 603, loss = 0.48091886\n",
      "Iteration 604, loss = 0.48009798\n",
      "Iteration 605, loss = 0.48028695\n",
      "Iteration 606, loss = 0.48125633\n",
      "Iteration 607, loss = 0.47946583\n",
      "Iteration 608, loss = 0.48114138\n",
      "Iteration 609, loss = 0.47985113\n",
      "Iteration 610, loss = 0.47969111\n",
      "Iteration 611, loss = 0.47999701\n",
      "Iteration 612, loss = 0.47938198\n",
      "Iteration 613, loss = 0.47976135\n",
      "Iteration 614, loss = 0.47992922\n",
      "Iteration 615, loss = 0.47955697\n",
      "Iteration 616, loss = 0.47925408\n",
      "Iteration 617, loss = 0.47892869\n",
      "Iteration 618, loss = 0.47883743\n",
      "Iteration 619, loss = 0.47936743\n",
      "Iteration 620, loss = 0.47880813\n",
      "Iteration 621, loss = 0.47841206\n",
      "Iteration 622, loss = 0.47846800\n",
      "Iteration 623, loss = 0.47888628\n",
      "Iteration 624, loss = 0.47854789\n",
      "Iteration 625, loss = 0.47892110\n",
      "Iteration 626, loss = 0.47879133\n",
      "Iteration 627, loss = 0.47908319\n",
      "Iteration 628, loss = 0.47908881\n",
      "Iteration 629, loss = 0.47783189\n",
      "Iteration 630, loss = 0.47849767\n",
      "Iteration 631, loss = 0.47849796\n",
      "Iteration 632, loss = 0.47719894\n",
      "Iteration 633, loss = 0.47814890\n",
      "Iteration 634, loss = 0.47753460\n",
      "Iteration 635, loss = 0.47727087\n",
      "Iteration 636, loss = 0.47828457\n",
      "Iteration 637, loss = 0.47604070\n",
      "Iteration 638, loss = 0.47742534\n",
      "Iteration 639, loss = 0.47764214\n",
      "Iteration 640, loss = 0.47700468\n",
      "Iteration 641, loss = 0.47620532\n",
      "Iteration 642, loss = 0.47717216\n",
      "Iteration 643, loss = 0.47712629\n",
      "Iteration 644, loss = 0.47721500\n",
      "Iteration 645, loss = 0.47710719\n",
      "Iteration 646, loss = 0.47742605\n",
      "Iteration 647, loss = 0.47602207\n",
      "Iteration 648, loss = 0.47676942\n",
      "Iteration 649, loss = 0.47682407\n",
      "Iteration 650, loss = 0.47580633\n",
      "Iteration 651, loss = 0.47656073\n",
      "Iteration 652, loss = 0.47595902\n",
      "Iteration 653, loss = 0.47637672\n",
      "Iteration 654, loss = 0.47630045\n",
      "Iteration 655, loss = 0.47589999\n",
      "Iteration 656, loss = 0.47610430\n",
      "Iteration 657, loss = 0.47509566\n",
      "Iteration 658, loss = 0.47552809\n",
      "Iteration 659, loss = 0.47517088\n",
      "Iteration 660, loss = 0.47492268\n",
      "Iteration 661, loss = 0.47525815\n",
      "Iteration 662, loss = 0.47552869\n",
      "Iteration 663, loss = 0.47571004\n",
      "Iteration 664, loss = 0.47504988\n",
      "Iteration 665, loss = 0.47502270\n",
      "Iteration 666, loss = 0.47502132\n",
      "Iteration 667, loss = 0.47519104\n",
      "Iteration 668, loss = 0.47512225\n",
      "Iteration 669, loss = 0.47468865\n",
      "Iteration 670, loss = 0.47512472\n",
      "Iteration 671, loss = 0.47469705\n",
      "Iteration 672, loss = 0.47406575\n",
      "Iteration 673, loss = 0.47449883\n",
      "Iteration 674, loss = 0.47567843\n",
      "Iteration 675, loss = 0.47421542\n",
      "Iteration 676, loss = 0.47298915\n",
      "Iteration 677, loss = 0.47391969\n",
      "Iteration 678, loss = 0.47351354\n",
      "Iteration 679, loss = 0.47482409\n",
      "Iteration 680, loss = 0.47339292\n",
      "Iteration 681, loss = 0.47306001\n",
      "Iteration 682, loss = 0.47398385\n",
      "Iteration 683, loss = 0.47321076\n",
      "Iteration 684, loss = 0.47313388\n",
      "Iteration 685, loss = 0.47348162\n",
      "Iteration 686, loss = 0.47279463\n",
      "Iteration 687, loss = 0.47323550\n",
      "Iteration 688, loss = 0.47288574\n",
      "Iteration 689, loss = 0.47243924\n",
      "Iteration 690, loss = 0.47303626\n",
      "Iteration 691, loss = 0.47307415\n",
      "Iteration 692, loss = 0.47320870\n",
      "Iteration 693, loss = 0.47244763\n",
      "Iteration 694, loss = 0.47266093\n",
      "Iteration 695, loss = 0.47239952\n",
      "Iteration 696, loss = 0.47292849\n",
      "Iteration 697, loss = 0.47224052\n",
      "Iteration 698, loss = 0.47211915\n",
      "Iteration 699, loss = 0.47232001\n",
      "Iteration 700, loss = 0.47238518\n",
      "Iteration 701, loss = 0.47145857\n",
      "Iteration 702, loss = 0.47203154\n",
      "Iteration 703, loss = 0.47188611\n",
      "Iteration 704, loss = 0.47244811\n",
      "Iteration 705, loss = 0.47319512\n",
      "Iteration 706, loss = 0.47243992\n",
      "Iteration 707, loss = 0.47278863\n",
      "Iteration 708, loss = 0.47103225\n",
      "Iteration 709, loss = 0.47231193\n",
      "Iteration 710, loss = 0.47119828\n",
      "Iteration 711, loss = 0.47135802\n",
      "Iteration 712, loss = 0.47132388\n",
      "Iteration 713, loss = 0.47097780\n",
      "Iteration 714, loss = 0.47138563\n",
      "Iteration 715, loss = 0.47138991\n",
      "Iteration 716, loss = 0.47158186\n",
      "Iteration 717, loss = 0.47094779\n",
      "Iteration 718, loss = 0.47041981\n",
      "Iteration 719, loss = 0.47076048\n",
      "Iteration 720, loss = 0.47129497\n",
      "Iteration 721, loss = 0.47047089\n",
      "Iteration 722, loss = 0.47016397\n",
      "Iteration 723, loss = 0.47089955\n",
      "Iteration 724, loss = 0.47071899\n",
      "Iteration 725, loss = 0.47064991\n",
      "Iteration 726, loss = 0.47062079\n",
      "Iteration 727, loss = 0.47022288\n",
      "Iteration 728, loss = 0.47031196\n",
      "Iteration 729, loss = 0.47035139\n",
      "Iteration 730, loss = 0.47027682\n",
      "Iteration 731, loss = 0.46931581\n",
      "Iteration 732, loss = 0.47014421\n",
      "Iteration 733, loss = 0.46955333\n",
      "Iteration 734, loss = 0.47034647\n",
      "Iteration 735, loss = 0.46893588\n",
      "Iteration 736, loss = 0.46983149\n",
      "Iteration 737, loss = 0.47044725\n",
      "Iteration 738, loss = 0.47094038\n",
      "Iteration 739, loss = 0.47047566\n",
      "Iteration 740, loss = 0.46997376\n",
      "Iteration 741, loss = 0.46910880\n",
      "Iteration 742, loss = 0.46972035\n",
      "Iteration 743, loss = 0.46960554\n",
      "Iteration 744, loss = 0.46969127\n",
      "Iteration 745, loss = 0.46879300\n",
      "Iteration 746, loss = 0.46861394\n",
      "Iteration 747, loss = 0.46962669\n",
      "Iteration 748, loss = 0.46902123\n",
      "Iteration 749, loss = 0.46787242\n",
      "Iteration 750, loss = 0.46852830\n",
      "Iteration 751, loss = 0.46778868\n",
      "Iteration 752, loss = 0.46891776\n",
      "Iteration 753, loss = 0.46958367\n",
      "Iteration 754, loss = 0.46755768\n",
      "Iteration 755, loss = 0.46764007\n",
      "Iteration 756, loss = 0.46877427\n",
      "Iteration 757, loss = 0.46801224\n",
      "Iteration 758, loss = 0.46824299\n",
      "Iteration 759, loss = 0.46778662\n",
      "Iteration 760, loss = 0.46881434\n",
      "Iteration 761, loss = 0.46835578\n",
      "Iteration 762, loss = 0.46718299\n",
      "Iteration 763, loss = 0.46777960\n",
      "Iteration 764, loss = 0.46732425\n",
      "Iteration 765, loss = 0.46728695\n",
      "Iteration 766, loss = 0.46848460\n",
      "Iteration 767, loss = 0.46776881\n",
      "Iteration 768, loss = 0.46733962\n",
      "Iteration 769, loss = 0.46715273\n",
      "Iteration 770, loss = 0.46818714\n",
      "Iteration 771, loss = 0.46819323\n",
      "Iteration 772, loss = 0.46713345\n",
      "Iteration 773, loss = 0.46786095\n",
      "Iteration 774, loss = 0.46759837\n",
      "Iteration 775, loss = 0.46723339\n",
      "Iteration 776, loss = 0.46717129\n",
      "Iteration 777, loss = 0.46738382\n",
      "Iteration 778, loss = 0.46658869\n",
      "Iteration 779, loss = 0.46629773\n",
      "Iteration 780, loss = 0.46680229\n",
      "Iteration 781, loss = 0.46731840\n",
      "Iteration 782, loss = 0.46772317\n",
      "Iteration 783, loss = 0.46681420\n",
      "Iteration 784, loss = 0.46653883\n",
      "Iteration 785, loss = 0.46750210\n",
      "Iteration 786, loss = 0.46579149\n",
      "Iteration 787, loss = 0.46602952\n",
      "Iteration 788, loss = 0.46654430\n",
      "Iteration 789, loss = 0.46607060\n",
      "Iteration 790, loss = 0.46616598\n",
      "Iteration 791, loss = 0.46548865\n",
      "Iteration 792, loss = 0.46486829\n",
      "Iteration 793, loss = 0.46697995\n",
      "Iteration 794, loss = 0.46513174\n",
      "Iteration 795, loss = 0.46587715\n",
      "Iteration 796, loss = 0.46526528\n",
      "Iteration 797, loss = 0.46479012\n",
      "Iteration 798, loss = 0.46491669\n",
      "Iteration 799, loss = 0.46507449\n",
      "Iteration 800, loss = 0.46527662\n",
      "Iteration 801, loss = 0.46485367\n",
      "Iteration 802, loss = 0.46564721\n",
      "Iteration 803, loss = 0.46485997\n",
      "Iteration 804, loss = 0.46468598\n",
      "Iteration 805, loss = 0.46459685\n",
      "Iteration 806, loss = 0.46551791\n",
      "Iteration 807, loss = 0.46528337\n",
      "Iteration 808, loss = 0.46375856\n",
      "Iteration 809, loss = 0.46378870\n",
      "Iteration 810, loss = 0.46492488\n",
      "Iteration 811, loss = 0.46404728\n",
      "Iteration 812, loss = 0.46471618\n",
      "Iteration 813, loss = 0.46310811\n",
      "Iteration 814, loss = 0.46315352\n",
      "Iteration 815, loss = 0.46411776\n",
      "Iteration 816, loss = 0.46385543\n",
      "Iteration 817, loss = 0.46331221\n",
      "Iteration 818, loss = 0.46292170\n",
      "Iteration 819, loss = 0.46342918\n",
      "Iteration 820, loss = 0.46301486\n",
      "Iteration 821, loss = 0.46320132\n",
      "Iteration 822, loss = 0.46296003\n",
      "Iteration 823, loss = 0.46253329\n",
      "Iteration 824, loss = 0.46214204\n",
      "Iteration 825, loss = 0.46316993\n",
      "Iteration 826, loss = 0.46332086\n",
      "Iteration 827, loss = 0.46303486\n",
      "Iteration 828, loss = 0.46327814\n",
      "Iteration 829, loss = 0.46249788\n",
      "Iteration 830, loss = 0.46278032\n",
      "Iteration 831, loss = 0.46219940\n",
      "Iteration 832, loss = 0.46209719\n",
      "Iteration 833, loss = 0.46295769\n",
      "Iteration 834, loss = 0.46206624\n",
      "Iteration 835, loss = 0.46230144\n",
      "Iteration 836, loss = 0.46201473\n",
      "Iteration 837, loss = 0.46200221\n",
      "Iteration 838, loss = 0.46173751\n",
      "Iteration 839, loss = 0.46118027\n",
      "Iteration 840, loss = 0.46135660\n",
      "Iteration 841, loss = 0.46240557\n",
      "Iteration 842, loss = 0.46147983\n",
      "Iteration 843, loss = 0.46091668\n",
      "Iteration 844, loss = 0.46097437\n",
      "Iteration 845, loss = 0.46123147\n",
      "Iteration 846, loss = 0.46094293\n",
      "Iteration 847, loss = 0.46114735\n",
      "Iteration 848, loss = 0.46066432\n",
      "Iteration 849, loss = 0.46044806\n",
      "Iteration 850, loss = 0.46074495\n",
      "Iteration 851, loss = 0.46089082\n",
      "Iteration 852, loss = 0.46001858\n",
      "Iteration 853, loss = 0.46081719\n",
      "Iteration 854, loss = 0.46125964\n",
      "Iteration 855, loss = 0.46091909\n",
      "Iteration 856, loss = 0.46092544\n",
      "Iteration 857, loss = 0.46007390\n",
      "Iteration 858, loss = 0.46054688\n",
      "Iteration 859, loss = 0.46017164\n",
      "Iteration 860, loss = 0.45981475\n",
      "Iteration 861, loss = 0.45999422\n",
      "Iteration 862, loss = 0.45943165\n",
      "Iteration 863, loss = 0.45951852\n",
      "Iteration 864, loss = 0.45952657\n",
      "Iteration 865, loss = 0.45917104\n",
      "Iteration 866, loss = 0.45977322\n",
      "Iteration 867, loss = 0.45892141\n",
      "Iteration 868, loss = 0.45953626\n",
      "Iteration 869, loss = 0.45949378\n",
      "Iteration 870, loss = 0.45870138\n",
      "Iteration 871, loss = 0.45888233\n",
      "Iteration 872, loss = 0.45975138\n",
      "Iteration 873, loss = 0.45861574\n",
      "Iteration 874, loss = 0.45763544\n",
      "Iteration 875, loss = 0.45930005\n",
      "Iteration 876, loss = 0.45921367\n",
      "Iteration 877, loss = 0.45868402\n",
      "Iteration 878, loss = 0.45748960\n",
      "Iteration 879, loss = 0.45847968\n",
      "Iteration 880, loss = 0.45883056\n",
      "Iteration 881, loss = 0.45852618\n",
      "Iteration 882, loss = 0.45781269\n",
      "Iteration 883, loss = 0.45772622\n",
      "Iteration 884, loss = 0.45864724\n",
      "Iteration 885, loss = 0.45777224\n",
      "Iteration 886, loss = 0.45879665\n",
      "Iteration 887, loss = 0.45816777\n",
      "Iteration 888, loss = 0.45716260\n",
      "Iteration 889, loss = 0.45743677\n",
      "Iteration 890, loss = 0.45726546\n",
      "Iteration 891, loss = 0.45777960\n",
      "Iteration 892, loss = 0.45767263\n",
      "Iteration 893, loss = 0.45786150\n",
      "Iteration 894, loss = 0.45787998\n",
      "Iteration 895, loss = 0.45694197\n",
      "Iteration 896, loss = 0.45694757\n",
      "Iteration 897, loss = 0.45669425\n",
      "Iteration 898, loss = 0.45731462\n",
      "Iteration 899, loss = 0.45688606\n",
      "Iteration 900, loss = 0.45716623\n",
      "Iteration 901, loss = 0.45777914\n",
      "Iteration 902, loss = 0.45691134\n",
      "Iteration 903, loss = 0.45608088\n",
      "Iteration 904, loss = 0.45779650\n",
      "Iteration 905, loss = 0.45633194\n",
      "Iteration 906, loss = 0.45541878\n",
      "Iteration 907, loss = 0.45696184\n",
      "Iteration 908, loss = 0.45749758\n",
      "Iteration 909, loss = 0.45592641\n",
      "Iteration 910, loss = 0.45700847\n",
      "Iteration 911, loss = 0.45617328\n",
      "Iteration 912, loss = 0.45698896\n",
      "Iteration 913, loss = 0.45678595\n",
      "Iteration 914, loss = 0.45643837\n",
      "Iteration 915, loss = 0.45554258\n",
      "Iteration 916, loss = 0.45572259\n",
      "Iteration 917, loss = 0.45591689\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68915387\n",
      "Iteration 2, loss = 0.67224668\n",
      "Iteration 3, loss = 0.66691648\n",
      "Iteration 4, loss = 0.66409177\n",
      "Iteration 5, loss = 0.66260627\n",
      "Iteration 6, loss = 0.66191485\n",
      "Iteration 7, loss = 0.66109187\n",
      "Iteration 8, loss = 0.66020237\n",
      "Iteration 9, loss = 0.65960192\n",
      "Iteration 10, loss = 0.65926766\n",
      "Iteration 11, loss = 0.65937725\n",
      "Iteration 12, loss = 0.65867066\n",
      "Iteration 13, loss = 0.65837751\n",
      "Iteration 14, loss = 0.65781214\n",
      "Iteration 15, loss = 0.65768758\n",
      "Iteration 16, loss = 0.65716494\n",
      "Iteration 17, loss = 0.65686579\n",
      "Iteration 18, loss = 0.65652463\n",
      "Iteration 19, loss = 0.65611694\n",
      "Iteration 20, loss = 0.65640340\n",
      "Iteration 21, loss = 0.65636003\n",
      "Iteration 22, loss = 0.65610018\n",
      "Iteration 23, loss = 0.65576473\n",
      "Iteration 24, loss = 0.65536822\n",
      "Iteration 25, loss = 0.65564689\n",
      "Iteration 26, loss = 0.65542109\n",
      "Iteration 27, loss = 0.65463870\n",
      "Iteration 28, loss = 0.65569775\n",
      "Iteration 29, loss = 0.65476444\n",
      "Iteration 30, loss = 0.65427907\n",
      "Iteration 31, loss = 0.65498732\n",
      "Iteration 32, loss = 0.65476583\n",
      "Iteration 33, loss = 0.65484084\n",
      "Iteration 34, loss = 0.65489482\n",
      "Iteration 35, loss = 0.65460398\n",
      "Iteration 36, loss = 0.65456194\n",
      "Iteration 37, loss = 0.65446170\n",
      "Iteration 38, loss = 0.65418275\n",
      "Iteration 39, loss = 0.65449565\n",
      "Iteration 40, loss = 0.65453905\n",
      "Iteration 41, loss = 0.65418190\n",
      "Iteration 42, loss = 0.65429864\n",
      "Iteration 43, loss = 0.65404096\n",
      "Iteration 44, loss = 0.65450249\n",
      "Iteration 45, loss = 0.65425952\n",
      "Iteration 46, loss = 0.65387471\n",
      "Iteration 47, loss = 0.65385025\n",
      "Iteration 48, loss = 0.65367199\n",
      "Iteration 49, loss = 0.65381528\n",
      "Iteration 50, loss = 0.65374998\n",
      "Iteration 51, loss = 0.65388730\n",
      "Iteration 52, loss = 0.65335842\n",
      "Iteration 53, loss = 0.65398379\n",
      "Iteration 54, loss = 0.65370787\n",
      "Iteration 55, loss = 0.65353620\n",
      "Iteration 56, loss = 0.65346128\n",
      "Iteration 57, loss = 0.65343377\n",
      "Iteration 58, loss = 0.65357379\n",
      "Iteration 59, loss = 0.65386504\n",
      "Iteration 60, loss = 0.65331506\n",
      "Iteration 61, loss = 0.65405254\n",
      "Iteration 62, loss = 0.65316986\n",
      "Iteration 63, loss = 0.65352406\n",
      "Iteration 64, loss = 0.65338109\n",
      "Iteration 65, loss = 0.65338127\n",
      "Iteration 66, loss = 0.65308072\n",
      "Iteration 67, loss = 0.65357867\n",
      "Iteration 68, loss = 0.65287522\n",
      "Iteration 69, loss = 0.65247590\n",
      "Iteration 70, loss = 0.65298536\n",
      "Iteration 71, loss = 0.65308770\n",
      "Iteration 72, loss = 0.65271171\n",
      "Iteration 73, loss = 0.65266246\n",
      "Iteration 74, loss = 0.65276273\n",
      "Iteration 75, loss = 0.65255539\n",
      "Iteration 76, loss = 0.65287404\n",
      "Iteration 77, loss = 0.65267459\n",
      "Iteration 78, loss = 0.65249267\n",
      "Iteration 79, loss = 0.65300163\n",
      "Iteration 80, loss = 0.65256284\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68042127\n",
      "Iteration 2, loss = 0.66569182\n",
      "Iteration 3, loss = 0.66211988\n",
      "Iteration 4, loss = 0.66062704\n",
      "Iteration 5, loss = 0.65819796\n",
      "Iteration 6, loss = 0.65763216\n",
      "Iteration 7, loss = 0.65783524\n",
      "Iteration 8, loss = 0.65639479\n",
      "Iteration 9, loss = 0.65532064\n",
      "Iteration 10, loss = 0.65456339\n",
      "Iteration 11, loss = 0.65507152\n",
      "Iteration 12, loss = 0.65360571\n",
      "Iteration 13, loss = 0.65333565\n",
      "Iteration 14, loss = 0.65255790\n",
      "Iteration 15, loss = 0.65242005\n",
      "Iteration 16, loss = 0.65179182\n",
      "Iteration 17, loss = 0.65097283\n",
      "Iteration 18, loss = 0.65068764\n",
      "Iteration 19, loss = 0.64973398\n",
      "Iteration 20, loss = 0.64976171\n",
      "Iteration 21, loss = 0.64998992\n",
      "Iteration 22, loss = 0.64948257\n",
      "Iteration 23, loss = 0.64894991\n",
      "Iteration 24, loss = 0.64805508\n",
      "Iteration 25, loss = 0.64814760\n",
      "Iteration 26, loss = 0.64735861\n",
      "Iteration 27, loss = 0.64619569\n",
      "Iteration 28, loss = 0.64751129\n",
      "Iteration 29, loss = 0.64591002\n",
      "Iteration 30, loss = 0.64508129\n",
      "Iteration 31, loss = 0.64550636\n",
      "Iteration 32, loss = 0.64523328\n",
      "Iteration 33, loss = 0.64470595\n",
      "Iteration 34, loss = 0.64465298\n",
      "Iteration 35, loss = 0.64383834\n",
      "Iteration 36, loss = 0.64366692\n",
      "Iteration 37, loss = 0.64258709\n",
      "Iteration 38, loss = 0.64248129\n",
      "Iteration 39, loss = 0.64261312\n",
      "Iteration 40, loss = 0.64213795\n",
      "Iteration 41, loss = 0.64137694\n",
      "Iteration 42, loss = 0.64097842\n",
      "Iteration 43, loss = 0.64039625\n",
      "Iteration 44, loss = 0.64045831\n",
      "Iteration 45, loss = 0.63955098\n",
      "Iteration 46, loss = 0.63889586\n",
      "Iteration 47, loss = 0.63873125\n",
      "Iteration 48, loss = 0.63826374\n",
      "Iteration 49, loss = 0.63787794\n",
      "Iteration 50, loss = 0.63725964\n",
      "Iteration 51, loss = 0.63721546\n",
      "Iteration 52, loss = 0.63620610\n",
      "Iteration 53, loss = 0.63615907\n",
      "Iteration 54, loss = 0.63563110\n",
      "Iteration 55, loss = 0.63525270\n",
      "Iteration 56, loss = 0.63465875\n",
      "Iteration 57, loss = 0.63423023\n",
      "Iteration 58, loss = 0.63380818\n",
      "Iteration 59, loss = 0.63403905\n",
      "Iteration 60, loss = 0.63279763\n",
      "Iteration 61, loss = 0.63316113\n",
      "Iteration 62, loss = 0.63215556\n",
      "Iteration 63, loss = 0.63176968\n",
      "Iteration 64, loss = 0.63132645\n",
      "Iteration 65, loss = 0.63073771\n",
      "Iteration 66, loss = 0.63001574\n",
      "Iteration 67, loss = 0.63028668\n",
      "Iteration 68, loss = 0.62928445\n",
      "Iteration 69, loss = 0.62828329\n",
      "Iteration 70, loss = 0.62822370\n",
      "Iteration 71, loss = 0.62815778\n",
      "Iteration 72, loss = 0.62728767\n",
      "Iteration 73, loss = 0.62663517\n",
      "Iteration 74, loss = 0.62608211\n",
      "Iteration 75, loss = 0.62607196\n",
      "Iteration 76, loss = 0.62522540\n",
      "Iteration 77, loss = 0.62515300\n",
      "Iteration 78, loss = 0.62419265\n",
      "Iteration 79, loss = 0.62433404\n",
      "Iteration 80, loss = 0.62329633\n",
      "Iteration 81, loss = 0.62342409\n",
      "Iteration 82, loss = 0.62241965\n",
      "Iteration 83, loss = 0.62228442\n",
      "Iteration 84, loss = 0.62130232\n",
      "Iteration 85, loss = 0.62076649\n",
      "Iteration 86, loss = 0.62032627\n",
      "Iteration 87, loss = 0.61998740\n",
      "Iteration 88, loss = 0.61928314\n",
      "Iteration 89, loss = 0.61914254\n",
      "Iteration 90, loss = 0.61842606\n",
      "Iteration 91, loss = 0.61744247\n",
      "Iteration 92, loss = 0.61688371\n",
      "Iteration 93, loss = 0.61702555\n",
      "Iteration 94, loss = 0.61612550\n",
      "Iteration 95, loss = 0.61565901\n",
      "Iteration 96, loss = 0.61519983\n",
      "Iteration 97, loss = 0.61379102\n",
      "Iteration 98, loss = 0.61398907\n",
      "Iteration 99, loss = 0.61347337\n",
      "Iteration 100, loss = 0.61276762\n",
      "Iteration 101, loss = 0.61175267\n",
      "Iteration 102, loss = 0.61195419\n",
      "Iteration 103, loss = 0.61122647\n",
      "Iteration 104, loss = 0.61038315\n",
      "Iteration 105, loss = 0.61055807\n",
      "Iteration 106, loss = 0.60927542\n",
      "Iteration 107, loss = 0.60850381\n",
      "Iteration 108, loss = 0.60897541\n",
      "Iteration 109, loss = 0.60678309\n",
      "Iteration 110, loss = 0.60655904\n",
      "Iteration 111, loss = 0.60624842\n",
      "Iteration 112, loss = 0.60628788\n",
      "Iteration 113, loss = 0.60468148\n",
      "Iteration 114, loss = 0.60485310\n",
      "Iteration 115, loss = 0.60410369\n",
      "Iteration 116, loss = 0.60354361\n",
      "Iteration 117, loss = 0.60371953\n",
      "Iteration 118, loss = 0.60209938\n",
      "Iteration 119, loss = 0.60229131\n",
      "Iteration 120, loss = 0.60108576\n",
      "Iteration 121, loss = 0.60026933\n",
      "Iteration 122, loss = 0.60006426\n",
      "Iteration 123, loss = 0.59969417\n",
      "Iteration 124, loss = 0.59905298\n",
      "Iteration 125, loss = 0.59816348\n",
      "Iteration 126, loss = 0.59824750\n",
      "Iteration 127, loss = 0.59800577\n",
      "Iteration 128, loss = 0.59737891\n",
      "Iteration 129, loss = 0.59590246\n",
      "Iteration 130, loss = 0.59499122\n",
      "Iteration 131, loss = 0.59466933\n",
      "Iteration 132, loss = 0.59416896\n",
      "Iteration 133, loss = 0.59377671\n",
      "Iteration 134, loss = 0.59303695\n",
      "Iteration 135, loss = 0.59243147\n",
      "Iteration 136, loss = 0.59184028\n",
      "Iteration 137, loss = 0.59093189\n",
      "Iteration 138, loss = 0.59130747\n",
      "Iteration 139, loss = 0.59002091\n",
      "Iteration 140, loss = 0.58984241\n",
      "Iteration 141, loss = 0.58931104\n",
      "Iteration 142, loss = 0.58889825\n",
      "Iteration 143, loss = 0.58751472\n",
      "Iteration 144, loss = 0.58752932\n",
      "Iteration 145, loss = 0.58714212\n",
      "Iteration 146, loss = 0.58674250\n",
      "Iteration 147, loss = 0.58508875\n",
      "Iteration 148, loss = 0.58568363\n",
      "Iteration 149, loss = 0.58479206\n",
      "Iteration 150, loss = 0.58401112\n",
      "Iteration 151, loss = 0.58340764\n",
      "Iteration 152, loss = 0.58367071\n",
      "Iteration 153, loss = 0.58242585\n",
      "Iteration 154, loss = 0.58082534\n",
      "Iteration 155, loss = 0.58226680\n",
      "Iteration 156, loss = 0.58037521\n",
      "Iteration 157, loss = 0.58023172\n",
      "Iteration 158, loss = 0.58006854\n",
      "Iteration 159, loss = 0.57929115\n",
      "Iteration 160, loss = 0.57823566\n",
      "Iteration 161, loss = 0.57880156\n",
      "Iteration 162, loss = 0.57715826\n",
      "Iteration 163, loss = 0.57735256\n",
      "Iteration 164, loss = 0.57654099\n",
      "Iteration 165, loss = 0.57578429\n",
      "Iteration 166, loss = 0.57647330\n",
      "Iteration 167, loss = 0.57467340\n",
      "Iteration 168, loss = 0.57455471\n",
      "Iteration 169, loss = 0.57386206\n",
      "Iteration 170, loss = 0.57369128\n",
      "Iteration 171, loss = 0.57367352\n",
      "Iteration 172, loss = 0.57225891\n",
      "Iteration 173, loss = 0.57125556\n",
      "Iteration 174, loss = 0.57160445\n",
      "Iteration 175, loss = 0.57008350\n",
      "Iteration 176, loss = 0.57055359\n",
      "Iteration 177, loss = 0.56953531\n",
      "Iteration 178, loss = 0.56900407\n",
      "Iteration 179, loss = 0.56883809\n",
      "Iteration 180, loss = 0.56794815\n",
      "Iteration 181, loss = 0.56736351\n",
      "Iteration 182, loss = 0.56696430\n",
      "Iteration 183, loss = 0.56665861\n",
      "Iteration 184, loss = 0.56626504\n",
      "Iteration 185, loss = 0.56557636\n",
      "Iteration 186, loss = 0.56549596\n",
      "Iteration 187, loss = 0.56489796\n",
      "Iteration 188, loss = 0.56456243\n",
      "Iteration 189, loss = 0.56416359\n",
      "Iteration 190, loss = 0.56303603\n",
      "Iteration 191, loss = 0.56316064\n",
      "Iteration 192, loss = 0.56164986\n",
      "Iteration 193, loss = 0.56130347\n",
      "Iteration 194, loss = 0.56148233\n",
      "Iteration 195, loss = 0.56105859\n",
      "Iteration 196, loss = 0.56038330\n",
      "Iteration 197, loss = 0.55976714\n",
      "Iteration 198, loss = 0.55976986\n",
      "Iteration 199, loss = 0.55854554\n",
      "Iteration 200, loss = 0.55810526\n",
      "Iteration 201, loss = 0.55791294\n",
      "Iteration 202, loss = 0.55700515\n",
      "Iteration 203, loss = 0.55730225\n",
      "Iteration 204, loss = 0.55635214\n",
      "Iteration 205, loss = 0.55601605\n",
      "Iteration 206, loss = 0.55590036\n",
      "Iteration 207, loss = 0.55479148\n",
      "Iteration 208, loss = 0.55477494\n",
      "Iteration 209, loss = 0.55507106\n",
      "Iteration 210, loss = 0.55365389\n",
      "Iteration 211, loss = 0.55301269\n",
      "Iteration 212, loss = 0.55312676\n",
      "Iteration 213, loss = 0.55193381\n",
      "Iteration 214, loss = 0.55218008\n",
      "Iteration 215, loss = 0.55155100\n",
      "Iteration 216, loss = 0.55033476\n",
      "Iteration 217, loss = 0.55069796\n",
      "Iteration 218, loss = 0.54985101\n",
      "Iteration 219, loss = 0.54988983\n",
      "Iteration 220, loss = 0.54923351\n",
      "Iteration 221, loss = 0.54943915\n",
      "Iteration 222, loss = 0.54825620\n",
      "Iteration 223, loss = 0.54849369\n",
      "Iteration 224, loss = 0.54747586\n",
      "Iteration 225, loss = 0.54714432\n",
      "Iteration 226, loss = 0.54718834\n",
      "Iteration 227, loss = 0.54692190\n",
      "Iteration 228, loss = 0.54558999\n",
      "Iteration 229, loss = 0.54552812\n",
      "Iteration 230, loss = 0.54648304\n",
      "Iteration 231, loss = 0.54458642\n",
      "Iteration 232, loss = 0.54468485\n",
      "Iteration 233, loss = 0.54450238\n",
      "Iteration 234, loss = 0.54310331\n",
      "Iteration 235, loss = 0.54402845\n",
      "Iteration 236, loss = 0.54305160\n",
      "Iteration 237, loss = 0.54229823\n",
      "Iteration 238, loss = 0.54235059\n",
      "Iteration 239, loss = 0.54213291\n",
      "Iteration 240, loss = 0.54208688\n",
      "Iteration 241, loss = 0.54222679\n",
      "Iteration 242, loss = 0.54039272\n",
      "Iteration 243, loss = 0.54016459\n",
      "Iteration 244, loss = 0.54024248\n",
      "Iteration 245, loss = 0.53985974\n",
      "Iteration 246, loss = 0.53932046\n",
      "Iteration 247, loss = 0.53889621\n",
      "Iteration 248, loss = 0.53816638\n",
      "Iteration 249, loss = 0.53855559\n",
      "Iteration 250, loss = 0.53825028\n",
      "Iteration 251, loss = 0.53824471\n",
      "Iteration 252, loss = 0.53798898\n",
      "Iteration 253, loss = 0.53740335\n",
      "Iteration 254, loss = 0.53762897\n",
      "Iteration 255, loss = 0.53646856\n",
      "Iteration 256, loss = 0.53631917\n",
      "Iteration 257, loss = 0.53628435\n",
      "Iteration 258, loss = 0.53636022\n",
      "Iteration 259, loss = 0.53529429\n",
      "Iteration 260, loss = 0.53480870\n",
      "Iteration 261, loss = 0.53504931\n",
      "Iteration 262, loss = 0.53457957\n",
      "Iteration 263, loss = 0.53340503\n",
      "Iteration 264, loss = 0.53387743\n",
      "Iteration 265, loss = 0.53329855\n",
      "Iteration 266, loss = 0.53306841\n",
      "Iteration 267, loss = 0.53347463\n",
      "Iteration 268, loss = 0.53268907\n",
      "Iteration 269, loss = 0.53290432\n",
      "Iteration 270, loss = 0.53156118\n",
      "Iteration 271, loss = 0.53198644\n",
      "Iteration 272, loss = 0.53164794\n",
      "Iteration 273, loss = 0.53163707\n",
      "Iteration 274, loss = 0.53094597\n",
      "Iteration 275, loss = 0.53031574\n",
      "Iteration 276, loss = 0.53038049\n",
      "Iteration 277, loss = 0.53083825\n",
      "Iteration 278, loss = 0.53072677\n",
      "Iteration 279, loss = 0.52973155\n",
      "Iteration 280, loss = 0.52943658\n",
      "Iteration 281, loss = 0.52986120\n",
      "Iteration 282, loss = 0.52908537\n",
      "Iteration 283, loss = 0.52862474\n",
      "Iteration 284, loss = 0.52793960\n",
      "Iteration 285, loss = 0.52835489\n",
      "Iteration 286, loss = 0.52777769\n",
      "Iteration 287, loss = 0.52825515\n",
      "Iteration 288, loss = 0.52704932\n",
      "Iteration 289, loss = 0.52800576\n",
      "Iteration 290, loss = 0.52717012\n",
      "Iteration 291, loss = 0.52648567\n",
      "Iteration 292, loss = 0.52623105\n",
      "Iteration 293, loss = 0.52643527\n",
      "Iteration 294, loss = 0.52593006\n",
      "Iteration 295, loss = 0.52549066\n",
      "Iteration 296, loss = 0.52557327\n",
      "Iteration 297, loss = 0.52498607\n",
      "Iteration 298, loss = 0.52480353\n",
      "Iteration 299, loss = 0.52500764\n",
      "Iteration 300, loss = 0.52467751\n",
      "Iteration 301, loss = 0.52426806\n",
      "Iteration 302, loss = 0.52434728\n",
      "Iteration 303, loss = 0.52499212\n",
      "Iteration 304, loss = 0.52375723\n",
      "Iteration 305, loss = 0.52338615\n",
      "Iteration 306, loss = 0.52301910\n",
      "Iteration 307, loss = 0.52324201\n",
      "Iteration 308, loss = 0.52296507\n",
      "Iteration 309, loss = 0.52193484\n",
      "Iteration 310, loss = 0.52208390\n",
      "Iteration 311, loss = 0.52235845\n",
      "Iteration 312, loss = 0.52250645\n",
      "Iteration 313, loss = 0.52124856\n",
      "Iteration 314, loss = 0.52243562\n",
      "Iteration 315, loss = 0.52131137\n",
      "Iteration 316, loss = 0.52135975\n",
      "Iteration 317, loss = 0.52165072\n",
      "Iteration 318, loss = 0.52106081\n",
      "Iteration 319, loss = 0.52004449\n",
      "Iteration 320, loss = 0.52003097\n",
      "Iteration 321, loss = 0.51996899\n",
      "Iteration 322, loss = 0.51991068\n",
      "Iteration 323, loss = 0.51969787\n",
      "Iteration 324, loss = 0.51900769\n",
      "Iteration 325, loss = 0.51908446\n",
      "Iteration 326, loss = 0.51914585\n",
      "Iteration 327, loss = 0.51869487\n",
      "Iteration 328, loss = 0.51876668\n",
      "Iteration 329, loss = 0.51841775\n",
      "Iteration 330, loss = 0.51845849\n",
      "Iteration 331, loss = 0.51850069\n",
      "Iteration 332, loss = 0.51851792\n",
      "Iteration 333, loss = 0.51888100\n",
      "Iteration 334, loss = 0.51747319\n",
      "Iteration 335, loss = 0.51752316\n",
      "Iteration 336, loss = 0.51718410\n",
      "Iteration 337, loss = 0.51694896\n",
      "Iteration 338, loss = 0.51660294\n",
      "Iteration 339, loss = 0.51674627\n",
      "Iteration 340, loss = 0.51580895\n",
      "Iteration 341, loss = 0.51559309\n",
      "Iteration 342, loss = 0.51642410\n",
      "Iteration 343, loss = 0.51572793\n",
      "Iteration 344, loss = 0.51608554\n",
      "Iteration 345, loss = 0.51505194\n",
      "Iteration 346, loss = 0.51556557\n",
      "Iteration 347, loss = 0.51518834\n",
      "Iteration 348, loss = 0.51528902\n",
      "Iteration 349, loss = 0.51429331\n",
      "Iteration 350, loss = 0.51509927\n",
      "Iteration 351, loss = 0.51478090\n",
      "Iteration 352, loss = 0.51538146\n",
      "Iteration 353, loss = 0.51363365\n",
      "Iteration 354, loss = 0.51417289\n",
      "Iteration 355, loss = 0.51369123\n",
      "Iteration 356, loss = 0.51338368\n",
      "Iteration 357, loss = 0.51330457\n",
      "Iteration 358, loss = 0.51374513\n",
      "Iteration 359, loss = 0.51341219\n",
      "Iteration 360, loss = 0.51394892\n",
      "Iteration 361, loss = 0.51300963\n",
      "Iteration 362, loss = 0.51344977\n",
      "Iteration 363, loss = 0.51242856\n",
      "Iteration 364, loss = 0.51215650\n",
      "Iteration 365, loss = 0.51214534\n",
      "Iteration 366, loss = 0.51235666\n",
      "Iteration 367, loss = 0.51151739\n",
      "Iteration 368, loss = 0.51158586\n",
      "Iteration 369, loss = 0.51240356\n",
      "Iteration 370, loss = 0.51180305\n",
      "Iteration 371, loss = 0.51105976\n",
      "Iteration 372, loss = 0.51066241\n",
      "Iteration 373, loss = 0.51107192\n",
      "Iteration 374, loss = 0.51065547\n",
      "Iteration 375, loss = 0.51055290\n",
      "Iteration 376, loss = 0.51022244\n",
      "Iteration 377, loss = 0.50991460\n",
      "Iteration 378, loss = 0.51053400\n",
      "Iteration 379, loss = 0.50997753\n",
      "Iteration 380, loss = 0.51044670\n",
      "Iteration 381, loss = 0.50981156\n",
      "Iteration 382, loss = 0.50977014\n",
      "Iteration 383, loss = 0.50914995\n",
      "Iteration 384, loss = 0.50899630\n",
      "Iteration 385, loss = 0.50864904\n",
      "Iteration 386, loss = 0.50917085\n",
      "Iteration 387, loss = 0.50892984\n",
      "Iteration 388, loss = 0.50897306\n",
      "Iteration 389, loss = 0.50830429\n",
      "Iteration 390, loss = 0.50880200\n",
      "Iteration 391, loss = 0.50840600\n",
      "Iteration 392, loss = 0.50852069\n",
      "Iteration 393, loss = 0.50810455\n",
      "Iteration 394, loss = 0.50727569\n",
      "Iteration 395, loss = 0.50708346\n",
      "Iteration 396, loss = 0.50745912\n",
      "Iteration 397, loss = 0.50686464\n",
      "Iteration 398, loss = 0.50673837\n",
      "Iteration 399, loss = 0.50749847\n",
      "Iteration 400, loss = 0.50679971\n",
      "Iteration 401, loss = 0.50671828\n",
      "Iteration 402, loss = 0.50603789\n",
      "Iteration 403, loss = 0.50669954\n",
      "Iteration 404, loss = 0.50546551\n",
      "Iteration 405, loss = 0.50586470\n",
      "Iteration 406, loss = 0.50585973\n",
      "Iteration 407, loss = 0.50600032\n",
      "Iteration 408, loss = 0.50515023\n",
      "Iteration 409, loss = 0.50565264\n",
      "Iteration 410, loss = 0.50558059\n",
      "Iteration 411, loss = 0.50457006\n",
      "Iteration 412, loss = 0.50485793\n",
      "Iteration 413, loss = 0.50379621\n",
      "Iteration 414, loss = 0.50447286\n",
      "Iteration 415, loss = 0.50409806\n",
      "Iteration 416, loss = 0.50374510\n",
      "Iteration 417, loss = 0.50385331\n",
      "Iteration 418, loss = 0.50469957\n",
      "Iteration 419, loss = 0.50292490\n",
      "Iteration 420, loss = 0.50352523\n",
      "Iteration 421, loss = 0.50335665\n",
      "Iteration 422, loss = 0.50294338\n",
      "Iteration 423, loss = 0.50337514\n",
      "Iteration 424, loss = 0.50312772\n",
      "Iteration 425, loss = 0.50285027\n",
      "Iteration 426, loss = 0.50230930\n",
      "Iteration 427, loss = 0.50230171\n",
      "Iteration 428, loss = 0.50230765\n",
      "Iteration 429, loss = 0.50174602\n",
      "Iteration 430, loss = 0.50218341\n",
      "Iteration 431, loss = 0.50235386\n",
      "Iteration 432, loss = 0.50147489\n",
      "Iteration 433, loss = 0.50183166\n",
      "Iteration 434, loss = 0.50207102\n",
      "Iteration 435, loss = 0.50094943\n",
      "Iteration 436, loss = 0.50142942\n",
      "Iteration 437, loss = 0.50099240\n",
      "Iteration 438, loss = 0.50162257\n",
      "Iteration 439, loss = 0.50036574\n",
      "Iteration 440, loss = 0.50113531\n",
      "Iteration 441, loss = 0.50070784\n",
      "Iteration 442, loss = 0.50044796\n",
      "Iteration 443, loss = 0.50065094\n",
      "Iteration 444, loss = 0.49968441\n",
      "Iteration 445, loss = 0.49961264\n",
      "Iteration 446, loss = 0.49995009\n",
      "Iteration 447, loss = 0.49988839\n",
      "Iteration 448, loss = 0.49917916\n",
      "Iteration 449, loss = 0.49935722\n",
      "Iteration 450, loss = 0.49926219\n",
      "Iteration 451, loss = 0.49921053\n",
      "Iteration 452, loss = 0.49857771\n",
      "Iteration 453, loss = 0.49886019\n",
      "Iteration 454, loss = 0.49894573\n",
      "Iteration 455, loss = 0.49809082\n",
      "Iteration 456, loss = 0.49913238\n",
      "Iteration 457, loss = 0.49804966\n",
      "Iteration 458, loss = 0.49882418\n",
      "Iteration 459, loss = 0.49760083\n",
      "Iteration 460, loss = 0.49846610\n",
      "Iteration 461, loss = 0.49784861\n",
      "Iteration 462, loss = 0.49819921\n",
      "Iteration 463, loss = 0.49748547\n",
      "Iteration 464, loss = 0.49722772\n",
      "Iteration 465, loss = 0.49766168\n",
      "Iteration 466, loss = 0.49708017\n",
      "Iteration 467, loss = 0.49724183\n",
      "Iteration 468, loss = 0.49709580\n",
      "Iteration 469, loss = 0.49697652\n",
      "Iteration 470, loss = 0.49652336\n",
      "Iteration 471, loss = 0.49668549\n",
      "Iteration 472, loss = 0.49662179\n",
      "Iteration 473, loss = 0.49634161\n",
      "Iteration 474, loss = 0.49588739\n",
      "Iteration 475, loss = 0.49690290\n",
      "Iteration 476, loss = 0.49614732\n",
      "Iteration 477, loss = 0.49579542\n",
      "Iteration 478, loss = 0.49556845\n",
      "Iteration 479, loss = 0.49558145\n",
      "Iteration 480, loss = 0.49498970\n",
      "Iteration 481, loss = 0.49472019\n",
      "Iteration 482, loss = 0.49496161\n",
      "Iteration 483, loss = 0.49513639\n",
      "Iteration 484, loss = 0.49472770\n",
      "Iteration 485, loss = 0.49489302\n",
      "Iteration 486, loss = 0.49515700\n",
      "Iteration 487, loss = 0.49385847\n",
      "Iteration 488, loss = 0.49451892\n",
      "Iteration 489, loss = 0.49360899\n",
      "Iteration 490, loss = 0.49380305\n",
      "Iteration 491, loss = 0.49422210\n",
      "Iteration 492, loss = 0.49374766\n",
      "Iteration 493, loss = 0.49355093\n",
      "Iteration 494, loss = 0.49323812\n",
      "Iteration 495, loss = 0.49293063\n",
      "Iteration 496, loss = 0.49352072\n",
      "Iteration 497, loss = 0.49342097\n",
      "Iteration 498, loss = 0.49253403\n",
      "Iteration 499, loss = 0.49259082\n",
      "Iteration 500, loss = 0.49285181\n",
      "Iteration 501, loss = 0.49177249\n",
      "Iteration 502, loss = 0.49285994\n",
      "Iteration 503, loss = 0.49248385\n",
      "Iteration 504, loss = 0.49216900\n",
      "Iteration 505, loss = 0.49220620\n",
      "Iteration 506, loss = 0.49221228\n",
      "Iteration 507, loss = 0.49134322\n",
      "Iteration 508, loss = 0.49187777\n",
      "Iteration 509, loss = 0.49127395\n",
      "Iteration 510, loss = 0.49093463\n",
      "Iteration 511, loss = 0.49135351\n",
      "Iteration 512, loss = 0.49027410\n",
      "Iteration 513, loss = 0.49067022\n",
      "Iteration 514, loss = 0.49121064\n",
      "Iteration 515, loss = 0.49090490\n",
      "Iteration 516, loss = 0.49078618\n",
      "Iteration 517, loss = 0.48966906\n",
      "Iteration 518, loss = 0.49005060\n",
      "Iteration 519, loss = 0.49053593\n",
      "Iteration 520, loss = 0.49019031\n",
      "Iteration 521, loss = 0.48940552\n",
      "Iteration 522, loss = 0.48984878\n",
      "Iteration 523, loss = 0.49061773\n",
      "Iteration 524, loss = 0.48910413\n",
      "Iteration 525, loss = 0.48909127\n",
      "Iteration 526, loss = 0.48933283\n",
      "Iteration 527, loss = 0.48903442\n",
      "Iteration 528, loss = 0.48928920\n",
      "Iteration 529, loss = 0.48914057\n",
      "Iteration 530, loss = 0.48967501\n",
      "Iteration 531, loss = 0.48830713\n",
      "Iteration 532, loss = 0.48882453\n",
      "Iteration 533, loss = 0.48824543\n",
      "Iteration 534, loss = 0.48754258\n",
      "Iteration 535, loss = 0.48876339\n",
      "Iteration 536, loss = 0.48853830\n",
      "Iteration 537, loss = 0.48736416\n",
      "Iteration 538, loss = 0.48798071\n",
      "Iteration 539, loss = 0.48710429\n",
      "Iteration 540, loss = 0.48809068\n",
      "Iteration 541, loss = 0.48706108\n",
      "Iteration 542, loss = 0.48718629\n",
      "Iteration 543, loss = 0.48730011\n",
      "Iteration 544, loss = 0.48679416\n",
      "Iteration 545, loss = 0.48697102\n",
      "Iteration 546, loss = 0.48702146\n",
      "Iteration 547, loss = 0.48697466\n",
      "Iteration 548, loss = 0.48575190\n",
      "Iteration 549, loss = 0.48614038\n",
      "Iteration 550, loss = 0.48701159\n",
      "Iteration 551, loss = 0.48594714\n",
      "Iteration 552, loss = 0.48635195\n",
      "Iteration 553, loss = 0.48671226\n",
      "Iteration 554, loss = 0.48603879\n",
      "Iteration 555, loss = 0.48515372\n",
      "Iteration 556, loss = 0.48504783\n",
      "Iteration 557, loss = 0.48613122\n",
      "Iteration 558, loss = 0.48589304\n",
      "Iteration 559, loss = 0.48506059\n",
      "Iteration 560, loss = 0.48469680\n",
      "Iteration 561, loss = 0.48548968\n",
      "Iteration 562, loss = 0.48554241\n",
      "Iteration 563, loss = 0.48477050\n",
      "Iteration 564, loss = 0.48476997\n",
      "Iteration 565, loss = 0.48420431\n",
      "Iteration 566, loss = 0.48461001\n",
      "Iteration 567, loss = 0.48460313\n",
      "Iteration 568, loss = 0.48485619\n",
      "Iteration 569, loss = 0.48435055\n",
      "Iteration 570, loss = 0.48471772\n",
      "Iteration 571, loss = 0.48402737\n",
      "Iteration 572, loss = 0.48326807\n",
      "Iteration 573, loss = 0.48384423\n",
      "Iteration 574, loss = 0.48359261\n",
      "Iteration 575, loss = 0.48412732\n",
      "Iteration 576, loss = 0.48330996\n",
      "Iteration 577, loss = 0.48387773\n",
      "Iteration 578, loss = 0.48389987\n",
      "Iteration 579, loss = 0.48387860\n",
      "Iteration 580, loss = 0.48269490\n",
      "Iteration 581, loss = 0.48305307\n",
      "Iteration 582, loss = 0.48289747\n",
      "Iteration 583, loss = 0.48339341\n",
      "Iteration 584, loss = 0.48255931\n",
      "Iteration 585, loss = 0.48261500\n",
      "Iteration 586, loss = 0.48289961\n",
      "Iteration 587, loss = 0.48186131\n",
      "Iteration 588, loss = 0.48257351\n",
      "Iteration 589, loss = 0.48092335\n",
      "Iteration 590, loss = 0.48145945\n",
      "Iteration 591, loss = 0.48201532\n",
      "Iteration 592, loss = 0.48221941\n",
      "Iteration 593, loss = 0.48127892\n",
      "Iteration 594, loss = 0.48056069\n",
      "Iteration 595, loss = 0.48258721\n",
      "Iteration 596, loss = 0.48208602\n",
      "Iteration 597, loss = 0.48177280\n",
      "Iteration 598, loss = 0.48073744\n",
      "Iteration 599, loss = 0.48089310\n",
      "Iteration 600, loss = 0.48136531\n",
      "Iteration 601, loss = 0.47977490\n",
      "Iteration 602, loss = 0.48118825\n",
      "Iteration 603, loss = 0.48091886\n",
      "Iteration 604, loss = 0.48009798\n",
      "Iteration 605, loss = 0.48028695\n",
      "Iteration 606, loss = 0.48125633\n",
      "Iteration 607, loss = 0.47946583\n",
      "Iteration 608, loss = 0.48114138\n",
      "Iteration 609, loss = 0.47985113\n",
      "Iteration 610, loss = 0.47969111\n",
      "Iteration 611, loss = 0.47999701\n",
      "Iteration 612, loss = 0.47938198\n",
      "Iteration 613, loss = 0.47976135\n",
      "Iteration 614, loss = 0.47992922\n",
      "Iteration 615, loss = 0.47955697\n",
      "Iteration 616, loss = 0.47925408\n",
      "Iteration 617, loss = 0.47892869\n",
      "Iteration 618, loss = 0.47883743\n",
      "Iteration 619, loss = 0.47936743\n",
      "Iteration 620, loss = 0.47880813\n",
      "Iteration 621, loss = 0.47841206\n",
      "Iteration 622, loss = 0.47846800\n",
      "Iteration 623, loss = 0.47888628\n",
      "Iteration 624, loss = 0.47854789\n",
      "Iteration 625, loss = 0.47892110\n",
      "Iteration 626, loss = 0.47879133\n",
      "Iteration 627, loss = 0.47908319\n",
      "Iteration 628, loss = 0.47908881\n",
      "Iteration 629, loss = 0.47783189\n",
      "Iteration 630, loss = 0.47849767\n",
      "Iteration 631, loss = 0.47849796\n",
      "Iteration 632, loss = 0.47719894\n",
      "Iteration 633, loss = 0.47814890\n",
      "Iteration 634, loss = 0.47753460\n",
      "Iteration 635, loss = 0.47727087\n",
      "Iteration 636, loss = 0.47828457\n",
      "Iteration 637, loss = 0.47604070\n",
      "Iteration 638, loss = 0.47742534\n",
      "Iteration 639, loss = 0.47764214\n",
      "Iteration 640, loss = 0.47700468\n",
      "Iteration 641, loss = 0.47620532\n",
      "Iteration 642, loss = 0.47717216\n",
      "Iteration 643, loss = 0.47712629\n",
      "Iteration 644, loss = 0.47721500\n",
      "Iteration 645, loss = 0.47710719\n",
      "Iteration 646, loss = 0.47742605\n",
      "Iteration 647, loss = 0.47602207\n",
      "Iteration 648, loss = 0.47676942\n",
      "Iteration 649, loss = 0.47682407\n",
      "Iteration 650, loss = 0.47580633\n",
      "Iteration 651, loss = 0.47656073\n",
      "Iteration 652, loss = 0.47595902\n",
      "Iteration 653, loss = 0.47637672\n",
      "Iteration 654, loss = 0.47630045\n",
      "Iteration 655, loss = 0.47589999\n",
      "Iteration 656, loss = 0.47610430\n",
      "Iteration 657, loss = 0.47509566\n",
      "Iteration 658, loss = 0.47552809\n",
      "Iteration 659, loss = 0.47517088\n",
      "Iteration 660, loss = 0.47492268\n",
      "Iteration 661, loss = 0.47525815\n",
      "Iteration 662, loss = 0.47552869\n",
      "Iteration 663, loss = 0.47571004\n",
      "Iteration 664, loss = 0.47504988\n",
      "Iteration 665, loss = 0.47502270\n",
      "Iteration 666, loss = 0.47502132\n",
      "Iteration 667, loss = 0.47519104\n",
      "Iteration 668, loss = 0.47512225\n",
      "Iteration 669, loss = 0.47468865\n",
      "Iteration 670, loss = 0.47512472\n",
      "Iteration 671, loss = 0.47469705\n",
      "Iteration 672, loss = 0.47406575\n",
      "Iteration 673, loss = 0.47449883\n",
      "Iteration 674, loss = 0.47567843\n",
      "Iteration 675, loss = 0.47421542\n",
      "Iteration 676, loss = 0.47298915\n",
      "Iteration 677, loss = 0.47391969\n",
      "Iteration 678, loss = 0.47351354\n",
      "Iteration 679, loss = 0.47482409\n",
      "Iteration 680, loss = 0.47339292\n",
      "Iteration 681, loss = 0.47306001\n",
      "Iteration 682, loss = 0.47398385\n",
      "Iteration 683, loss = 0.47321076\n",
      "Iteration 684, loss = 0.47313388\n",
      "Iteration 685, loss = 0.47348162\n",
      "Iteration 686, loss = 0.47279463\n",
      "Iteration 687, loss = 0.47323550\n",
      "Iteration 688, loss = 0.47288574\n",
      "Iteration 689, loss = 0.47243924\n",
      "Iteration 690, loss = 0.47303626\n",
      "Iteration 691, loss = 0.47307415\n",
      "Iteration 692, loss = 0.47320870\n",
      "Iteration 693, loss = 0.47244763\n",
      "Iteration 694, loss = 0.47266093\n",
      "Iteration 695, loss = 0.47239952\n",
      "Iteration 696, loss = 0.47292849\n",
      "Iteration 697, loss = 0.47224052\n",
      "Iteration 698, loss = 0.47211915\n",
      "Iteration 699, loss = 0.47232001\n",
      "Iteration 700, loss = 0.47238518\n",
      "Iteration 701, loss = 0.47145857\n",
      "Iteration 702, loss = 0.47203154\n",
      "Iteration 703, loss = 0.47188611\n",
      "Iteration 704, loss = 0.47244811\n",
      "Iteration 705, loss = 0.47319512\n",
      "Iteration 706, loss = 0.47243992\n",
      "Iteration 707, loss = 0.47278863\n",
      "Iteration 708, loss = 0.47103225\n",
      "Iteration 709, loss = 0.47231193\n",
      "Iteration 710, loss = 0.47119828\n",
      "Iteration 711, loss = 0.47135802\n",
      "Iteration 712, loss = 0.47132388\n",
      "Iteration 713, loss = 0.47097780\n",
      "Iteration 714, loss = 0.47138563\n",
      "Iteration 715, loss = 0.47138991\n",
      "Iteration 716, loss = 0.47158186\n",
      "Iteration 717, loss = 0.47094779\n",
      "Iteration 718, loss = 0.47041981\n",
      "Iteration 719, loss = 0.47076048\n",
      "Iteration 720, loss = 0.47129497\n",
      "Iteration 721, loss = 0.47047089\n",
      "Iteration 722, loss = 0.47016397\n",
      "Iteration 723, loss = 0.47089955\n",
      "Iteration 724, loss = 0.47071899\n",
      "Iteration 725, loss = 0.47064991\n",
      "Iteration 726, loss = 0.47062079\n",
      "Iteration 727, loss = 0.47022288\n",
      "Iteration 728, loss = 0.47031196\n",
      "Iteration 729, loss = 0.47035139\n",
      "Iteration 730, loss = 0.47027682\n",
      "Iteration 731, loss = 0.46931581\n",
      "Iteration 732, loss = 0.47014421\n",
      "Iteration 733, loss = 0.46955333\n",
      "Iteration 734, loss = 0.47034647\n",
      "Iteration 735, loss = 0.46893588\n",
      "Iteration 736, loss = 0.46983149\n",
      "Iteration 737, loss = 0.47044725\n",
      "Iteration 738, loss = 0.47094038\n",
      "Iteration 739, loss = 0.47047566\n",
      "Iteration 740, loss = 0.46997376\n",
      "Iteration 741, loss = 0.46910880\n",
      "Iteration 742, loss = 0.46972035\n",
      "Iteration 743, loss = 0.46960554\n",
      "Iteration 744, loss = 0.46969127\n",
      "Iteration 745, loss = 0.46879300\n",
      "Iteration 746, loss = 0.46861394\n",
      "Iteration 747, loss = 0.46962669\n",
      "Iteration 748, loss = 0.46902123\n",
      "Iteration 749, loss = 0.46787242\n",
      "Iteration 750, loss = 0.46852830\n",
      "Iteration 751, loss = 0.46778868\n",
      "Iteration 752, loss = 0.46891776\n",
      "Iteration 753, loss = 0.46958367\n",
      "Iteration 754, loss = 0.46755768\n",
      "Iteration 755, loss = 0.46764007\n",
      "Iteration 756, loss = 0.46877427\n",
      "Iteration 757, loss = 0.46801224\n",
      "Iteration 758, loss = 0.46824299\n",
      "Iteration 759, loss = 0.46778662\n",
      "Iteration 760, loss = 0.46881434\n",
      "Iteration 761, loss = 0.46835578\n",
      "Iteration 762, loss = 0.46718299\n",
      "Iteration 763, loss = 0.46777960\n",
      "Iteration 764, loss = 0.46732425\n",
      "Iteration 765, loss = 0.46728695\n",
      "Iteration 766, loss = 0.46848460\n",
      "Iteration 767, loss = 0.46776881\n",
      "Iteration 768, loss = 0.46733962\n",
      "Iteration 769, loss = 0.46715273\n",
      "Iteration 770, loss = 0.46818714\n",
      "Iteration 771, loss = 0.46819323\n",
      "Iteration 772, loss = 0.46713345\n",
      "Iteration 773, loss = 0.46786095\n",
      "Iteration 774, loss = 0.46759837\n",
      "Iteration 775, loss = 0.46723339\n",
      "Iteration 776, loss = 0.46717129\n",
      "Iteration 777, loss = 0.46738382\n",
      "Iteration 778, loss = 0.46658869\n",
      "Iteration 779, loss = 0.46629773\n",
      "Iteration 780, loss = 0.46680229\n",
      "Iteration 781, loss = 0.46731840\n",
      "Iteration 782, loss = 0.46772317\n",
      "Iteration 783, loss = 0.46681420\n",
      "Iteration 784, loss = 0.46653883\n",
      "Iteration 785, loss = 0.46750210\n",
      "Iteration 786, loss = 0.46579149\n",
      "Iteration 787, loss = 0.46602952\n",
      "Iteration 788, loss = 0.46654430\n",
      "Iteration 789, loss = 0.46607060\n",
      "Iteration 790, loss = 0.46616598\n",
      "Iteration 791, loss = 0.46548865\n",
      "Iteration 792, loss = 0.46486829\n",
      "Iteration 793, loss = 0.46697995\n",
      "Iteration 794, loss = 0.46513174\n",
      "Iteration 795, loss = 0.46587715\n",
      "Iteration 796, loss = 0.46526528\n",
      "Iteration 797, loss = 0.46479012\n",
      "Iteration 798, loss = 0.46491669\n",
      "Iteration 799, loss = 0.46507449\n",
      "Iteration 800, loss = 0.46527662\n",
      "Iteration 801, loss = 0.46485367\n",
      "Iteration 802, loss = 0.46564721\n",
      "Iteration 803, loss = 0.46485997\n",
      "Iteration 804, loss = 0.46468598\n",
      "Iteration 805, loss = 0.46459685\n",
      "Iteration 806, loss = 0.46551791\n",
      "Iteration 807, loss = 0.46528337\n",
      "Iteration 808, loss = 0.46375856\n",
      "Iteration 809, loss = 0.46378870\n",
      "Iteration 810, loss = 0.46492488\n",
      "Iteration 811, loss = 0.46404728\n",
      "Iteration 812, loss = 0.46471618\n",
      "Iteration 813, loss = 0.46310811\n",
      "Iteration 814, loss = 0.46315352\n",
      "Iteration 815, loss = 0.46411776\n",
      "Iteration 816, loss = 0.46385543\n",
      "Iteration 817, loss = 0.46331221\n",
      "Iteration 818, loss = 0.46292170\n",
      "Iteration 819, loss = 0.46342918\n",
      "Iteration 820, loss = 0.46301486\n",
      "Iteration 821, loss = 0.46320132\n",
      "Iteration 822, loss = 0.46296003\n",
      "Iteration 823, loss = 0.46253329\n",
      "Iteration 824, loss = 0.46214204\n",
      "Iteration 825, loss = 0.46316993\n",
      "Iteration 826, loss = 0.46332086\n",
      "Iteration 827, loss = 0.46303486\n",
      "Iteration 828, loss = 0.46327814\n",
      "Iteration 829, loss = 0.46249788\n",
      "Iteration 830, loss = 0.46278032\n",
      "Iteration 831, loss = 0.46219940\n",
      "Iteration 832, loss = 0.46209719\n",
      "Iteration 833, loss = 0.46295769\n",
      "Iteration 834, loss = 0.46206624\n",
      "Iteration 835, loss = 0.46230144\n",
      "Iteration 836, loss = 0.46201473\n",
      "Iteration 837, loss = 0.46200221\n",
      "Iteration 838, loss = 0.46173751\n",
      "Iteration 839, loss = 0.46118027\n",
      "Iteration 840, loss = 0.46135660\n",
      "Iteration 841, loss = 0.46240557\n",
      "Iteration 842, loss = 0.46147983\n",
      "Iteration 843, loss = 0.46091668\n",
      "Iteration 844, loss = 0.46097437\n",
      "Iteration 845, loss = 0.46123147\n",
      "Iteration 846, loss = 0.46094293\n",
      "Iteration 847, loss = 0.46114735\n",
      "Iteration 848, loss = 0.46066432\n",
      "Iteration 849, loss = 0.46044806\n",
      "Iteration 850, loss = 0.46074495\n",
      "Iteration 851, loss = 0.46089082\n",
      "Iteration 852, loss = 0.46001858\n",
      "Iteration 853, loss = 0.46081719\n",
      "Iteration 854, loss = 0.46125964\n",
      "Iteration 855, loss = 0.46091909\n",
      "Iteration 856, loss = 0.46092544\n",
      "Iteration 857, loss = 0.46007390\n",
      "Iteration 858, loss = 0.46054688\n",
      "Iteration 859, loss = 0.46017164\n",
      "Iteration 860, loss = 0.45981475\n",
      "Iteration 861, loss = 0.45999422\n",
      "Iteration 862, loss = 0.45943165\n",
      "Iteration 863, loss = 0.45951852\n",
      "Iteration 864, loss = 0.45952657\n",
      "Iteration 865, loss = 0.45917104\n",
      "Iteration 866, loss = 0.45977322\n",
      "Iteration 867, loss = 0.45892141\n",
      "Iteration 868, loss = 0.45953626\n",
      "Iteration 869, loss = 0.45949378\n",
      "Iteration 870, loss = 0.45870138\n",
      "Iteration 871, loss = 0.45888233\n",
      "Iteration 872, loss = 0.45975138\n",
      "Iteration 873, loss = 0.45861574\n",
      "Iteration 874, loss = 0.45763544\n",
      "Iteration 875, loss = 0.45930005\n",
      "Iteration 876, loss = 0.45921367\n",
      "Iteration 877, loss = 0.45868402\n",
      "Iteration 878, loss = 0.45748960\n",
      "Iteration 879, loss = 0.45847968\n",
      "Iteration 880, loss = 0.45883056\n",
      "Iteration 881, loss = 0.45852618\n",
      "Iteration 882, loss = 0.45781269\n",
      "Iteration 883, loss = 0.45772622\n",
      "Iteration 884, loss = 0.45864724\n",
      "Iteration 885, loss = 0.45777224\n",
      "Iteration 886, loss = 0.45879665\n",
      "Iteration 887, loss = 0.45816777\n",
      "Iteration 888, loss = 0.45716260\n",
      "Iteration 889, loss = 0.45743677\n",
      "Iteration 890, loss = 0.45726546\n",
      "Iteration 891, loss = 0.45777960\n",
      "Iteration 892, loss = 0.45767263\n",
      "Iteration 893, loss = 0.45786150\n",
      "Iteration 894, loss = 0.45787998\n",
      "Iteration 895, loss = 0.45694197\n",
      "Iteration 896, loss = 0.45694757\n",
      "Iteration 897, loss = 0.45669425\n",
      "Iteration 898, loss = 0.45731462\n",
      "Iteration 899, loss = 0.45688606\n",
      "Iteration 900, loss = 0.45716623\n",
      "Iteration 901, loss = 0.45777914\n",
      "Iteration 902, loss = 0.45691134\n",
      "Iteration 903, loss = 0.45608088\n",
      "Iteration 904, loss = 0.45779650\n",
      "Iteration 905, loss = 0.45633194\n",
      "Iteration 906, loss = 0.45541878\n",
      "Iteration 907, loss = 0.45696184\n",
      "Iteration 908, loss = 0.45749758\n",
      "Iteration 909, loss = 0.45592641\n",
      "Iteration 910, loss = 0.45700847\n",
      "Iteration 911, loss = 0.45617328\n",
      "Iteration 912, loss = 0.45698896\n",
      "Iteration 913, loss = 0.45678595\n",
      "Iteration 914, loss = 0.45643837\n",
      "Iteration 915, loss = 0.45554258\n",
      "Iteration 916, loss = 0.45572259\n",
      "Iteration 917, loss = 0.45591689\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68702666\n",
      "Iteration 2, loss = 0.66894899\n",
      "Iteration 3, loss = 0.66381062\n",
      "Iteration 4, loss = 0.66338301\n",
      "Iteration 5, loss = 0.65932518\n",
      "Iteration 6, loss = 0.65948302\n",
      "Iteration 7, loss = 0.65870584\n",
      "Iteration 8, loss = 0.65683856\n",
      "Iteration 9, loss = 0.65654013\n",
      "Iteration 10, loss = 0.65466598\n",
      "Iteration 11, loss = 0.65433640\n",
      "Iteration 12, loss = 0.65315350\n",
      "Iteration 13, loss = 0.65294716\n",
      "Iteration 14, loss = 0.65241540\n",
      "Iteration 15, loss = 0.65104584\n",
      "Iteration 16, loss = 0.65076316\n",
      "Iteration 17, loss = 0.64942572\n",
      "Iteration 18, loss = 0.64939621\n",
      "Iteration 19, loss = 0.64776108\n",
      "Iteration 20, loss = 0.64757611\n",
      "Iteration 21, loss = 0.64705679\n",
      "Iteration 22, loss = 0.64680951\n",
      "Iteration 23, loss = 0.64614699\n",
      "Iteration 24, loss = 0.64455053\n",
      "Iteration 25, loss = 0.64431456\n",
      "Iteration 26, loss = 0.64296928\n",
      "Iteration 27, loss = 0.64177445\n",
      "Iteration 28, loss = 0.64315173\n",
      "Iteration 29, loss = 0.64064474\n",
      "Iteration 30, loss = 0.63934842\n",
      "Iteration 31, loss = 0.63973485\n",
      "Iteration 32, loss = 0.63935779\n",
      "Iteration 33, loss = 0.63844616\n",
      "Iteration 34, loss = 0.63793378\n",
      "Iteration 35, loss = 0.63650485\n",
      "Iteration 36, loss = 0.63610283\n",
      "Iteration 37, loss = 0.63512255\n",
      "Iteration 38, loss = 0.63481008\n",
      "Iteration 39, loss = 0.63474247\n",
      "Iteration 40, loss = 0.63373446\n",
      "Iteration 41, loss = 0.63294497\n",
      "Iteration 42, loss = 0.63224140\n",
      "Iteration 43, loss = 0.63120634\n",
      "Iteration 44, loss = 0.63151437\n",
      "Iteration 45, loss = 0.62980746\n",
      "Iteration 46, loss = 0.62925261\n",
      "Iteration 47, loss = 0.62877824\n",
      "Iteration 48, loss = 0.62794491\n",
      "Iteration 49, loss = 0.62712879\n",
      "Iteration 50, loss = 0.62661775\n",
      "Iteration 51, loss = 0.62662211\n",
      "Iteration 52, loss = 0.62511416\n",
      "Iteration 53, loss = 0.62468433\n",
      "Iteration 54, loss = 0.62446302\n",
      "Iteration 55, loss = 0.62336652\n",
      "Iteration 56, loss = 0.62288188\n",
      "Iteration 57, loss = 0.62216013\n",
      "Iteration 58, loss = 0.62154069\n",
      "Iteration 59, loss = 0.62166944\n",
      "Iteration 60, loss = 0.62065617\n",
      "Iteration 61, loss = 0.62059472\n",
      "Iteration 62, loss = 0.61936478\n",
      "Iteration 63, loss = 0.61943480\n",
      "Iteration 64, loss = 0.61827186\n",
      "Iteration 65, loss = 0.61812435\n",
      "Iteration 66, loss = 0.61686022\n",
      "Iteration 67, loss = 0.61685856\n",
      "Iteration 68, loss = 0.61520540\n",
      "Iteration 69, loss = 0.61444617\n",
      "Iteration 70, loss = 0.61434689\n",
      "Iteration 71, loss = 0.61410749\n",
      "Iteration 72, loss = 0.61351248\n",
      "Iteration 73, loss = 0.61242676\n",
      "Iteration 74, loss = 0.61193931\n",
      "Iteration 75, loss = 0.61168553\n",
      "Iteration 76, loss = 0.61101776\n",
      "Iteration 77, loss = 0.61046360\n",
      "Iteration 78, loss = 0.60881111\n",
      "Iteration 79, loss = 0.60950638\n",
      "Iteration 80, loss = 0.60860245\n",
      "Iteration 81, loss = 0.60870024\n",
      "Iteration 82, loss = 0.60704912\n",
      "Iteration 83, loss = 0.60689281\n",
      "Iteration 84, loss = 0.60632230\n",
      "Iteration 85, loss = 0.60577935\n",
      "Iteration 86, loss = 0.60542409\n",
      "Iteration 87, loss = 0.60472166\n",
      "Iteration 88, loss = 0.60426309\n",
      "Iteration 89, loss = 0.60385672\n",
      "Iteration 90, loss = 0.60295096\n",
      "Iteration 91, loss = 0.60234595\n",
      "Iteration 92, loss = 0.60207810\n",
      "Iteration 93, loss = 0.60121693\n",
      "Iteration 94, loss = 0.60135113\n",
      "Iteration 95, loss = 0.60010082\n",
      "Iteration 96, loss = 0.60010148\n",
      "Iteration 97, loss = 0.59934314\n",
      "Iteration 98, loss = 0.59872172\n",
      "Iteration 99, loss = 0.59786269\n",
      "Iteration 100, loss = 0.59819566\n",
      "Iteration 101, loss = 0.59724272\n",
      "Iteration 102, loss = 0.59684503\n",
      "Iteration 103, loss = 0.59628495\n",
      "Iteration 104, loss = 0.59599854\n",
      "Iteration 105, loss = 0.59562692\n",
      "Iteration 106, loss = 0.59459359\n",
      "Iteration 107, loss = 0.59480230\n",
      "Iteration 108, loss = 0.59505195\n",
      "Iteration 109, loss = 0.59297856\n",
      "Iteration 110, loss = 0.59339259\n",
      "Iteration 111, loss = 0.59321975\n",
      "Iteration 112, loss = 0.59229800\n",
      "Iteration 113, loss = 0.59108253\n",
      "Iteration 114, loss = 0.59118864\n",
      "Iteration 115, loss = 0.59076363\n",
      "Iteration 116, loss = 0.59044877\n",
      "Iteration 117, loss = 0.59075903\n",
      "Iteration 118, loss = 0.58882862\n",
      "Iteration 119, loss = 0.58896842\n",
      "Iteration 120, loss = 0.58858549\n",
      "Iteration 121, loss = 0.58796181\n",
      "Iteration 122, loss = 0.58751345\n",
      "Iteration 123, loss = 0.58741429\n",
      "Iteration 124, loss = 0.58699055\n",
      "Iteration 125, loss = 0.58591170\n",
      "Iteration 126, loss = 0.58611598\n",
      "Iteration 127, loss = 0.58527003\n",
      "Iteration 128, loss = 0.58517429\n",
      "Iteration 129, loss = 0.58412683\n",
      "Iteration 130, loss = 0.58411231\n",
      "Iteration 131, loss = 0.58377732\n",
      "Iteration 132, loss = 0.58278555\n",
      "Iteration 133, loss = 0.58289819\n",
      "Iteration 134, loss = 0.58293643\n",
      "Iteration 135, loss = 0.58173834\n",
      "Iteration 136, loss = 0.58213378\n",
      "Iteration 137, loss = 0.58121081\n",
      "Iteration 138, loss = 0.58114386\n",
      "Iteration 139, loss = 0.58031142\n",
      "Iteration 140, loss = 0.58092391\n",
      "Iteration 141, loss = 0.57925933\n",
      "Iteration 142, loss = 0.57913599\n",
      "Iteration 143, loss = 0.57936699\n",
      "Iteration 144, loss = 0.57840948\n",
      "Iteration 145, loss = 0.57797308\n",
      "Iteration 146, loss = 0.57796048\n",
      "Iteration 147, loss = 0.57772796\n",
      "Iteration 148, loss = 0.57750027\n",
      "Iteration 149, loss = 0.57721271\n",
      "Iteration 150, loss = 0.57632294\n",
      "Iteration 151, loss = 0.57563076\n",
      "Iteration 152, loss = 0.57512760\n",
      "Iteration 153, loss = 0.57530888\n",
      "Iteration 154, loss = 0.57521131\n",
      "Iteration 155, loss = 0.57557565\n",
      "Iteration 156, loss = 0.57354580\n",
      "Iteration 157, loss = 0.57435009\n",
      "Iteration 158, loss = 0.57335837\n",
      "Iteration 159, loss = 0.57383252\n",
      "Iteration 160, loss = 0.57271228\n",
      "Iteration 161, loss = 0.57316592\n",
      "Iteration 162, loss = 0.57191037\n",
      "Iteration 163, loss = 0.57271928\n",
      "Iteration 164, loss = 0.57289734\n",
      "Iteration 165, loss = 0.57176613\n",
      "Iteration 166, loss = 0.57169996\n",
      "Iteration 167, loss = 0.57028364\n",
      "Iteration 168, loss = 0.57104785\n",
      "Iteration 169, loss = 0.57106335\n",
      "Iteration 170, loss = 0.57067508\n",
      "Iteration 171, loss = 0.56989037\n",
      "Iteration 172, loss = 0.56957713\n",
      "Iteration 173, loss = 0.56922294\n",
      "Iteration 174, loss = 0.56985241\n",
      "Iteration 175, loss = 0.56850966\n",
      "Iteration 176, loss = 0.56877592\n",
      "Iteration 177, loss = 0.56820741\n",
      "Iteration 178, loss = 0.56819542\n",
      "Iteration 179, loss = 0.56801285\n",
      "Iteration 180, loss = 0.56691291\n",
      "Iteration 181, loss = 0.56654910\n",
      "Iteration 182, loss = 0.56646727\n",
      "Iteration 183, loss = 0.56633036\n",
      "Iteration 184, loss = 0.56747414\n",
      "Iteration 185, loss = 0.56536607\n",
      "Iteration 186, loss = 0.56611133\n",
      "Iteration 187, loss = 0.56624416\n",
      "Iteration 188, loss = 0.56551578\n",
      "Iteration 189, loss = 0.56563877\n",
      "Iteration 190, loss = 0.56494240\n",
      "Iteration 191, loss = 0.56513284\n",
      "Iteration 192, loss = 0.56500212\n",
      "Iteration 193, loss = 0.56419119\n",
      "Iteration 194, loss = 0.56436808\n",
      "Iteration 195, loss = 0.56361922\n",
      "Iteration 196, loss = 0.56314184\n",
      "Iteration 197, loss = 0.56326574\n",
      "Iteration 198, loss = 0.56369365\n",
      "Iteration 199, loss = 0.56250510\n",
      "Iteration 200, loss = 0.56171681\n",
      "Iteration 201, loss = 0.56121600\n",
      "Iteration 202, loss = 0.56217022\n",
      "Iteration 203, loss = 0.56226893\n",
      "Iteration 204, loss = 0.56184240\n",
      "Iteration 205, loss = 0.56058386\n",
      "Iteration 206, loss = 0.56128321\n",
      "Iteration 207, loss = 0.55978238\n",
      "Iteration 208, loss = 0.56077478\n",
      "Iteration 209, loss = 0.55988154\n",
      "Iteration 210, loss = 0.56003754\n",
      "Iteration 211, loss = 0.55885737\n",
      "Iteration 212, loss = 0.56008453\n",
      "Iteration 213, loss = 0.55812500\n",
      "Iteration 214, loss = 0.56011565\n",
      "Iteration 215, loss = 0.55958461\n",
      "Iteration 216, loss = 0.55849676\n",
      "Iteration 217, loss = 0.55857025\n",
      "Iteration 218, loss = 0.55795278\n",
      "Iteration 219, loss = 0.55788083\n",
      "Iteration 220, loss = 0.55727661\n",
      "Iteration 221, loss = 0.55817012\n",
      "Iteration 222, loss = 0.55733989\n",
      "Iteration 223, loss = 0.55612604\n",
      "Iteration 224, loss = 0.55761614\n",
      "Iteration 225, loss = 0.55621968\n",
      "Iteration 226, loss = 0.55652568\n",
      "Iteration 227, loss = 0.55712840\n",
      "Iteration 228, loss = 0.55537299\n",
      "Iteration 229, loss = 0.55573995\n",
      "Iteration 230, loss = 0.55589691\n",
      "Iteration 231, loss = 0.55509267\n",
      "Iteration 232, loss = 0.55509282\n",
      "Iteration 233, loss = 0.55492970\n",
      "Iteration 234, loss = 0.55488035\n",
      "Iteration 235, loss = 0.55434775\n",
      "Iteration 236, loss = 0.55423972\n",
      "Iteration 237, loss = 0.55350786\n",
      "Iteration 238, loss = 0.55363230\n",
      "Iteration 239, loss = 0.55373987\n",
      "Iteration 240, loss = 0.55515997\n",
      "Iteration 241, loss = 0.55392199\n",
      "Iteration 242, loss = 0.55343213\n",
      "Iteration 243, loss = 0.55234339\n",
      "Iteration 244, loss = 0.55113946\n",
      "Iteration 245, loss = 0.55272416\n",
      "Iteration 246, loss = 0.55202337\n",
      "Iteration 247, loss = 0.55285131\n",
      "Iteration 248, loss = 0.55258065\n",
      "Iteration 249, loss = 0.55162899\n",
      "Iteration 250, loss = 0.55164844\n",
      "Iteration 251, loss = 0.55106445\n",
      "Iteration 252, loss = 0.55165866\n",
      "Iteration 253, loss = 0.55084606\n",
      "Iteration 254, loss = 0.55038291\n",
      "Iteration 255, loss = 0.55045234\n",
      "Iteration 256, loss = 0.54972140\n",
      "Iteration 257, loss = 0.55047126\n",
      "Iteration 258, loss = 0.55121132\n",
      "Iteration 259, loss = 0.55046316\n",
      "Iteration 260, loss = 0.54935138\n",
      "Iteration 261, loss = 0.54951638\n",
      "Iteration 262, loss = 0.54993650\n",
      "Iteration 263, loss = 0.54860251\n",
      "Iteration 264, loss = 0.54877426\n",
      "Iteration 265, loss = 0.54856981\n",
      "Iteration 266, loss = 0.54858947\n",
      "Iteration 267, loss = 0.54868829\n",
      "Iteration 268, loss = 0.54961842\n",
      "Iteration 269, loss = 0.54802137\n",
      "Iteration 270, loss = 0.54773746\n",
      "Iteration 271, loss = 0.54903176\n",
      "Iteration 272, loss = 0.54721924\n",
      "Iteration 273, loss = 0.54698034\n",
      "Iteration 274, loss = 0.54795734\n",
      "Iteration 275, loss = 0.54633997\n",
      "Iteration 276, loss = 0.54732157\n",
      "Iteration 277, loss = 0.54682331\n",
      "Iteration 278, loss = 0.54752665\n",
      "Iteration 279, loss = 0.54645391\n",
      "Iteration 280, loss = 0.54738037\n",
      "Iteration 281, loss = 0.54681903\n",
      "Iteration 282, loss = 0.54618274\n",
      "Iteration 283, loss = 0.54640839\n",
      "Iteration 284, loss = 0.54494323\n",
      "Iteration 285, loss = 0.54488668\n",
      "Iteration 286, loss = 0.54559190\n",
      "Iteration 287, loss = 0.54608030\n",
      "Iteration 288, loss = 0.54545511\n",
      "Iteration 289, loss = 0.54561520\n",
      "Iteration 290, loss = 0.54515840\n",
      "Iteration 291, loss = 0.54475810\n",
      "Iteration 292, loss = 0.54578135\n",
      "Iteration 293, loss = 0.54526047\n",
      "Iteration 294, loss = 0.54432185\n",
      "Iteration 295, loss = 0.54551483\n",
      "Iteration 296, loss = 0.54407188\n",
      "Iteration 297, loss = 0.54466300\n",
      "Iteration 298, loss = 0.54377749\n",
      "Iteration 299, loss = 0.54417234\n",
      "Iteration 300, loss = 0.54341667\n",
      "Iteration 301, loss = 0.54460722\n",
      "Iteration 302, loss = 0.54295044\n",
      "Iteration 303, loss = 0.54437070\n",
      "Iteration 304, loss = 0.54355493\n",
      "Iteration 305, loss = 0.54278497\n",
      "Iteration 306, loss = 0.54395572\n",
      "Iteration 307, loss = 0.54274533\n",
      "Iteration 308, loss = 0.54339032\n",
      "Iteration 309, loss = 0.54156487\n",
      "Iteration 310, loss = 0.54260315\n",
      "Iteration 311, loss = 0.54263769\n",
      "Iteration 312, loss = 0.54229670\n",
      "Iteration 313, loss = 0.54211812\n",
      "Iteration 314, loss = 0.54087795\n",
      "Iteration 315, loss = 0.54172935\n",
      "Iteration 316, loss = 0.54209682\n",
      "Iteration 317, loss = 0.54270286\n",
      "Iteration 318, loss = 0.54116216\n",
      "Iteration 319, loss = 0.54001185\n",
      "Iteration 320, loss = 0.54149921\n",
      "Iteration 321, loss = 0.54162618\n",
      "Iteration 322, loss = 0.54031370\n",
      "Iteration 323, loss = 0.54118834\n",
      "Iteration 324, loss = 0.54071488\n",
      "Iteration 325, loss = 0.54053290\n",
      "Iteration 326, loss = 0.54077786\n",
      "Iteration 327, loss = 0.54056175\n",
      "Iteration 328, loss = 0.54110012\n",
      "Iteration 329, loss = 0.53922524\n",
      "Iteration 330, loss = 0.53964142\n",
      "Iteration 331, loss = 0.53855555\n",
      "Iteration 332, loss = 0.53941364\n",
      "Iteration 333, loss = 0.53925545\n",
      "Iteration 334, loss = 0.53915407\n",
      "Iteration 335, loss = 0.53787471\n",
      "Iteration 336, loss = 0.53946973\n",
      "Iteration 337, loss = 0.53866698\n",
      "Iteration 338, loss = 0.53870117\n",
      "Iteration 339, loss = 0.53766132\n",
      "Iteration 340, loss = 0.53849509\n",
      "Iteration 341, loss = 0.53730859\n",
      "Iteration 342, loss = 0.53794445\n",
      "Iteration 343, loss = 0.53764800\n",
      "Iteration 344, loss = 0.53860557\n",
      "Iteration 345, loss = 0.53684898\n",
      "Iteration 346, loss = 0.53889798\n",
      "Iteration 347, loss = 0.53755555\n",
      "Iteration 348, loss = 0.53844472\n",
      "Iteration 349, loss = 0.53693684\n",
      "Iteration 350, loss = 0.53744959\n",
      "Iteration 351, loss = 0.53722336\n",
      "Iteration 352, loss = 0.53657878\n",
      "Iteration 353, loss = 0.53712008\n",
      "Iteration 354, loss = 0.53815779\n",
      "Iteration 355, loss = 0.53654199\n",
      "Iteration 356, loss = 0.53670376\n",
      "Iteration 357, loss = 0.53674120\n",
      "Iteration 358, loss = 0.53583588\n",
      "Iteration 359, loss = 0.53531384\n",
      "Iteration 360, loss = 0.53584117\n",
      "Iteration 361, loss = 0.53669270\n",
      "Iteration 362, loss = 0.53648221\n",
      "Iteration 363, loss = 0.53556670\n",
      "Iteration 364, loss = 0.53519834\n",
      "Iteration 365, loss = 0.53513379\n",
      "Iteration 366, loss = 0.53509058\n",
      "Iteration 367, loss = 0.53481682\n",
      "Iteration 368, loss = 0.53529338\n",
      "Iteration 369, loss = 0.53555271\n",
      "Iteration 370, loss = 0.53563180\n",
      "Iteration 371, loss = 0.53429766\n",
      "Iteration 372, loss = 0.53444561\n",
      "Iteration 373, loss = 0.53382660\n",
      "Iteration 374, loss = 0.53561853\n",
      "Iteration 375, loss = 0.53608885\n",
      "Iteration 376, loss = 0.53395884\n",
      "Iteration 377, loss = 0.53483800\n",
      "Iteration 378, loss = 0.53558337\n",
      "Iteration 379, loss = 0.53415751\n",
      "Iteration 380, loss = 0.53339912\n",
      "Iteration 381, loss = 0.53375605\n",
      "Iteration 382, loss = 0.53338124\n",
      "Iteration 383, loss = 0.53236712\n",
      "Iteration 384, loss = 0.53234973\n",
      "Iteration 385, loss = 0.53238312\n",
      "Iteration 386, loss = 0.53296374\n",
      "Iteration 387, loss = 0.53316793\n",
      "Iteration 388, loss = 0.53465368\n",
      "Iteration 389, loss = 0.53287968\n",
      "Iteration 390, loss = 0.53206064\n",
      "Iteration 391, loss = 0.53190953\n",
      "Iteration 392, loss = 0.53068298\n",
      "Iteration 393, loss = 0.53258207\n",
      "Iteration 394, loss = 0.53208192\n",
      "Iteration 395, loss = 0.53195873\n",
      "Iteration 396, loss = 0.53172315\n",
      "Iteration 397, loss = 0.53100979\n",
      "Iteration 398, loss = 0.53150707\n",
      "Iteration 399, loss = 0.53193097\n",
      "Iteration 400, loss = 0.53097175\n",
      "Iteration 401, loss = 0.53219214\n",
      "Iteration 402, loss = 0.53142677\n",
      "Iteration 403, loss = 0.53086590\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67360753\n",
      "Iteration 2, loss = 0.65406693\n",
      "Iteration 3, loss = 0.65376302\n",
      "Iteration 4, loss = 0.65301670\n",
      "Iteration 5, loss = 0.65242116\n",
      "Iteration 6, loss = 0.65197465\n",
      "Iteration 7, loss = 0.65168457\n",
      "Iteration 8, loss = 0.65100167\n",
      "Iteration 9, loss = 0.65052341\n",
      "Iteration 10, loss = 0.64993105\n",
      "Iteration 11, loss = 0.64956593\n",
      "Iteration 12, loss = 0.64917084\n",
      "Iteration 13, loss = 0.64856048\n",
      "Iteration 14, loss = 0.64779598\n",
      "Iteration 15, loss = 0.64770288\n",
      "Iteration 16, loss = 0.64683664\n",
      "Iteration 17, loss = 0.64637458\n",
      "Iteration 18, loss = 0.64595496\n",
      "Iteration 19, loss = 0.64551077\n",
      "Iteration 20, loss = 0.64580404\n",
      "Iteration 21, loss = 0.64498652\n",
      "Iteration 22, loss = 0.64444128\n",
      "Iteration 23, loss = 0.64405300\n",
      "Iteration 24, loss = 0.64371590\n",
      "Iteration 25, loss = 0.64354501\n",
      "Iteration 26, loss = 0.64343873\n",
      "Iteration 27, loss = 0.64237923\n",
      "Iteration 28, loss = 0.64244580\n",
      "Iteration 29, loss = 0.64202664\n",
      "Iteration 30, loss = 0.64136770\n",
      "Iteration 31, loss = 0.64135496\n",
      "Iteration 32, loss = 0.64117608\n",
      "Iteration 33, loss = 0.64093526\n",
      "Iteration 34, loss = 0.64064390\n",
      "Iteration 35, loss = 0.64034468\n",
      "Iteration 36, loss = 0.63976143\n",
      "Iteration 37, loss = 0.63954107\n",
      "Iteration 38, loss = 0.63895811\n",
      "Iteration 39, loss = 0.63880341\n",
      "Iteration 40, loss = 0.63853365\n",
      "Iteration 41, loss = 0.63808233\n",
      "Iteration 42, loss = 0.63794560\n",
      "Iteration 43, loss = 0.63727083\n",
      "Iteration 44, loss = 0.63754944\n",
      "Iteration 45, loss = 0.63682427\n",
      "Iteration 46, loss = 0.63639209\n",
      "Iteration 47, loss = 0.63603692\n",
      "Iteration 48, loss = 0.63555516\n",
      "Iteration 49, loss = 0.63519095\n",
      "Iteration 50, loss = 0.63502136\n",
      "Iteration 51, loss = 0.63466148\n",
      "Iteration 52, loss = 0.63405836\n",
      "Iteration 53, loss = 0.63425670\n",
      "Iteration 54, loss = 0.63354461\n",
      "Iteration 55, loss = 0.63307520\n",
      "Iteration 56, loss = 0.63260554\n",
      "Iteration 57, loss = 0.63231325\n",
      "Iteration 58, loss = 0.63202027\n",
      "Iteration 59, loss = 0.63188142\n",
      "Iteration 60, loss = 0.63132923\n",
      "Iteration 61, loss = 0.63156832\n",
      "Iteration 62, loss = 0.63066031\n",
      "Iteration 63, loss = 0.63055223\n",
      "Iteration 64, loss = 0.63010119\n",
      "Iteration 65, loss = 0.62985215\n",
      "Iteration 66, loss = 0.62930809\n",
      "Iteration 67, loss = 0.62933232\n",
      "Iteration 68, loss = 0.62845449\n",
      "Iteration 69, loss = 0.62795681\n",
      "Iteration 70, loss = 0.62785048\n",
      "Iteration 71, loss = 0.62781763\n",
      "Iteration 72, loss = 0.62713272\n",
      "Iteration 73, loss = 0.62688764\n",
      "Iteration 74, loss = 0.62702617\n",
      "Iteration 75, loss = 0.62621366\n",
      "Iteration 76, loss = 0.62615310\n",
      "Iteration 77, loss = 0.62560801\n",
      "Iteration 78, loss = 0.62525191\n",
      "Iteration 79, loss = 0.62541966\n",
      "Iteration 80, loss = 0.62468474\n",
      "Iteration 81, loss = 0.62472161\n",
      "Iteration 82, loss = 0.62363928\n",
      "Iteration 83, loss = 0.62427712\n",
      "Iteration 84, loss = 0.62337988\n",
      "Iteration 85, loss = 0.62312694\n",
      "Iteration 86, loss = 0.62272445\n",
      "Iteration 87, loss = 0.62243957\n",
      "Iteration 88, loss = 0.62227074\n",
      "Iteration 89, loss = 0.62208228\n",
      "Iteration 90, loss = 0.62177246\n",
      "Iteration 91, loss = 0.62146825\n",
      "Iteration 92, loss = 0.62144230\n",
      "Iteration 93, loss = 0.62122861\n",
      "Iteration 94, loss = 0.62086154\n",
      "Iteration 95, loss = 0.62039311\n",
      "Iteration 96, loss = 0.62015971\n",
      "Iteration 97, loss = 0.61983193\n",
      "Iteration 98, loss = 0.61963223\n",
      "Iteration 99, loss = 0.61938016\n",
      "Iteration 100, loss = 0.61916438\n",
      "Iteration 101, loss = 0.61862607\n",
      "Iteration 102, loss = 0.61886172\n",
      "Iteration 103, loss = 0.61846360\n",
      "Iteration 104, loss = 0.61796352\n",
      "Iteration 105, loss = 0.61851566\n",
      "Iteration 106, loss = 0.61760746\n",
      "Iteration 107, loss = 0.61741643\n",
      "Iteration 108, loss = 0.61730683\n",
      "Iteration 109, loss = 0.61639178\n",
      "Iteration 110, loss = 0.61633607\n",
      "Iteration 111, loss = 0.61674165\n",
      "Iteration 112, loss = 0.61695925\n",
      "Iteration 113, loss = 0.61568243\n",
      "Iteration 114, loss = 0.61566177\n",
      "Iteration 115, loss = 0.61568961\n",
      "Iteration 116, loss = 0.61530887\n",
      "Iteration 117, loss = 0.61557675\n",
      "Iteration 118, loss = 0.61464038\n",
      "Iteration 119, loss = 0.61529492\n",
      "Iteration 120, loss = 0.61414139\n",
      "Iteration 121, loss = 0.61401670\n",
      "Iteration 122, loss = 0.61415491\n",
      "Iteration 123, loss = 0.61395726\n",
      "Iteration 124, loss = 0.61382587\n",
      "Iteration 125, loss = 0.61381009\n",
      "Iteration 126, loss = 0.61348407\n",
      "Iteration 127, loss = 0.61375823\n",
      "Iteration 128, loss = 0.61329096\n",
      "Iteration 129, loss = 0.61254972\n",
      "Iteration 130, loss = 0.61234232\n",
      "Iteration 131, loss = 0.61209771\n",
      "Iteration 132, loss = 0.61178796\n",
      "Iteration 133, loss = 0.61150977\n",
      "Iteration 134, loss = 0.61155628\n",
      "Iteration 135, loss = 0.61156570\n",
      "Iteration 136, loss = 0.61121934\n",
      "Iteration 137, loss = 0.61055072\n",
      "Iteration 138, loss = 0.61108741\n",
      "Iteration 139, loss = 0.61052368\n",
      "Iteration 140, loss = 0.61028096\n",
      "Iteration 141, loss = 0.61051196\n",
      "Iteration 142, loss = 0.61005018\n",
      "Iteration 143, loss = 0.60962113\n",
      "Iteration 144, loss = 0.60964683\n",
      "Iteration 145, loss = 0.60963508\n",
      "Iteration 146, loss = 0.60975397\n",
      "Iteration 147, loss = 0.60874290\n",
      "Iteration 148, loss = 0.60895302\n",
      "Iteration 149, loss = 0.60871552\n",
      "Iteration 150, loss = 0.60878447\n",
      "Iteration 151, loss = 0.60838134\n",
      "Iteration 152, loss = 0.60803900\n",
      "Iteration 153, loss = 0.60801932\n",
      "Iteration 154, loss = 0.60744944\n",
      "Iteration 155, loss = 0.60838456\n",
      "Iteration 156, loss = 0.60708796\n",
      "Iteration 157, loss = 0.60721165\n",
      "Iteration 158, loss = 0.60727044\n",
      "Iteration 159, loss = 0.60708009\n",
      "Iteration 160, loss = 0.60661680\n",
      "Iteration 161, loss = 0.60742003\n",
      "Iteration 162, loss = 0.60639855\n",
      "Iteration 163, loss = 0.60633253\n",
      "Iteration 164, loss = 0.60621956\n",
      "Iteration 165, loss = 0.60614248\n",
      "Iteration 166, loss = 0.60606755\n",
      "Iteration 167, loss = 0.60541436\n",
      "Iteration 168, loss = 0.60535555\n",
      "Iteration 169, loss = 0.60537126\n",
      "Iteration 170, loss = 0.60514864\n",
      "Iteration 171, loss = 0.60539925\n",
      "Iteration 172, loss = 0.60445941\n",
      "Iteration 173, loss = 0.60394797\n",
      "Iteration 174, loss = 0.60433301\n",
      "Iteration 175, loss = 0.60360047\n",
      "Iteration 176, loss = 0.60418298\n",
      "Iteration 177, loss = 0.60330648\n",
      "Iteration 178, loss = 0.60353189\n",
      "Iteration 179, loss = 0.60280802\n",
      "Iteration 180, loss = 0.60287002\n",
      "Iteration 181, loss = 0.60256679\n",
      "Iteration 182, loss = 0.60277803\n",
      "Iteration 183, loss = 0.60212599\n",
      "Iteration 184, loss = 0.60248632\n",
      "Iteration 185, loss = 0.60214311\n",
      "Iteration 186, loss = 0.60215803\n",
      "Iteration 187, loss = 0.60188341\n",
      "Iteration 188, loss = 0.60136101\n",
      "Iteration 189, loss = 0.60141548\n",
      "Iteration 190, loss = 0.60144290\n",
      "Iteration 191, loss = 0.60178751\n",
      "Iteration 192, loss = 0.60069574\n",
      "Iteration 193, loss = 0.60012565\n",
      "Iteration 194, loss = 0.60072366\n",
      "Iteration 195, loss = 0.60036295\n",
      "Iteration 196, loss = 0.60002307\n",
      "Iteration 197, loss = 0.59975389\n",
      "Iteration 198, loss = 0.59947036\n",
      "Iteration 199, loss = 0.59894458\n",
      "Iteration 200, loss = 0.59894788\n",
      "Iteration 201, loss = 0.59881845\n",
      "Iteration 202, loss = 0.59811136\n",
      "Iteration 203, loss = 0.59825888\n",
      "Iteration 204, loss = 0.59779415\n",
      "Iteration 205, loss = 0.59837825\n",
      "Iteration 206, loss = 0.59755649\n",
      "Iteration 207, loss = 0.59706208\n",
      "Iteration 208, loss = 0.59725239\n",
      "Iteration 209, loss = 0.59735014\n",
      "Iteration 210, loss = 0.59669915\n",
      "Iteration 211, loss = 0.59608993\n",
      "Iteration 212, loss = 0.59584635\n",
      "Iteration 213, loss = 0.59553285\n",
      "Iteration 214, loss = 0.59628941\n",
      "Iteration 215, loss = 0.59562457\n",
      "Iteration 216, loss = 0.59500541\n",
      "Iteration 217, loss = 0.59526907\n",
      "Iteration 218, loss = 0.59472254\n",
      "Iteration 219, loss = 0.59427572\n",
      "Iteration 220, loss = 0.59407193\n",
      "Iteration 221, loss = 0.59412132\n",
      "Iteration 222, loss = 0.59370659\n",
      "Iteration 223, loss = 0.59305511\n",
      "Iteration 224, loss = 0.59286659\n",
      "Iteration 225, loss = 0.59286452\n",
      "Iteration 226, loss = 0.59279044\n",
      "Iteration 227, loss = 0.59284095\n",
      "Iteration 228, loss = 0.59166019\n",
      "Iteration 229, loss = 0.59246884\n",
      "Iteration 230, loss = 0.59201087\n",
      "Iteration 231, loss = 0.59127863\n",
      "Iteration 232, loss = 0.59144026\n",
      "Iteration 233, loss = 0.59093651\n",
      "Iteration 234, loss = 0.59042908\n",
      "Iteration 235, loss = 0.59087215\n",
      "Iteration 236, loss = 0.59020527\n",
      "Iteration 237, loss = 0.59003388\n",
      "Iteration 238, loss = 0.58979681\n",
      "Iteration 239, loss = 0.58904637\n",
      "Iteration 240, loss = 0.58903882\n",
      "Iteration 241, loss = 0.58901037\n",
      "Iteration 242, loss = 0.58830960\n",
      "Iteration 243, loss = 0.58793982\n",
      "Iteration 244, loss = 0.58776118\n",
      "Iteration 245, loss = 0.58769787\n",
      "Iteration 246, loss = 0.58735379\n",
      "Iteration 247, loss = 0.58673186\n",
      "Iteration 248, loss = 0.58657364\n",
      "Iteration 249, loss = 0.58629316\n",
      "Iteration 250, loss = 0.58610194\n",
      "Iteration 251, loss = 0.58602140\n",
      "Iteration 252, loss = 0.58526157\n",
      "Iteration 253, loss = 0.58522559\n",
      "Iteration 254, loss = 0.58516121\n",
      "Iteration 255, loss = 0.58476691\n",
      "Iteration 256, loss = 0.58423684\n",
      "Iteration 257, loss = 0.58409193\n",
      "Iteration 258, loss = 0.58418241\n",
      "Iteration 259, loss = 0.58320563\n",
      "Iteration 260, loss = 0.58322482\n",
      "Iteration 261, loss = 0.58234208\n",
      "Iteration 262, loss = 0.58234672\n",
      "Iteration 263, loss = 0.58196843\n",
      "Iteration 264, loss = 0.58196913\n",
      "Iteration 265, loss = 0.58132708\n",
      "Iteration 266, loss = 0.58098195\n",
      "Iteration 267, loss = 0.58089950\n",
      "Iteration 268, loss = 0.58054647\n",
      "Iteration 269, loss = 0.58029219\n",
      "Iteration 270, loss = 0.57967284\n",
      "Iteration 271, loss = 0.57945917\n",
      "Iteration 272, loss = 0.57956329\n",
      "Iteration 273, loss = 0.57835353\n",
      "Iteration 274, loss = 0.57880909\n",
      "Iteration 275, loss = 0.57810384\n",
      "Iteration 276, loss = 0.57801511\n",
      "Iteration 277, loss = 0.57809124\n",
      "Iteration 278, loss = 0.57749288\n",
      "Iteration 279, loss = 0.57666272\n",
      "Iteration 280, loss = 0.57722347\n",
      "Iteration 281, loss = 0.57616834\n",
      "Iteration 282, loss = 0.57564003\n",
      "Iteration 283, loss = 0.57599130\n",
      "Iteration 284, loss = 0.57472060\n",
      "Iteration 285, loss = 0.57506206\n",
      "Iteration 286, loss = 0.57443246\n",
      "Iteration 287, loss = 0.57437333\n",
      "Iteration 288, loss = 0.57401024\n",
      "Iteration 289, loss = 0.57344150\n",
      "Iteration 290, loss = 0.57291614\n",
      "Iteration 291, loss = 0.57267841\n",
      "Iteration 292, loss = 0.57238286\n",
      "Iteration 293, loss = 0.57201500\n",
      "Iteration 294, loss = 0.57150477\n",
      "Iteration 295, loss = 0.57111137\n",
      "Iteration 296, loss = 0.57055913\n",
      "Iteration 297, loss = 0.57038599\n",
      "Iteration 298, loss = 0.56978209\n",
      "Iteration 299, loss = 0.57004279\n",
      "Iteration 300, loss = 0.56880923\n",
      "Iteration 301, loss = 0.56899256\n",
      "Iteration 302, loss = 0.56817616\n",
      "Iteration 303, loss = 0.56854194\n",
      "Iteration 304, loss = 0.56728937\n",
      "Iteration 305, loss = 0.56689930\n",
      "Iteration 306, loss = 0.56656832\n",
      "Iteration 307, loss = 0.56618003\n",
      "Iteration 308, loss = 0.56557309\n",
      "Iteration 309, loss = 0.56541311\n",
      "Iteration 310, loss = 0.56495092\n",
      "Iteration 311, loss = 0.56468061\n",
      "Iteration 312, loss = 0.56509338\n",
      "Iteration 313, loss = 0.56356757\n",
      "Iteration 314, loss = 0.56329598\n",
      "Iteration 315, loss = 0.56274783\n",
      "Iteration 316, loss = 0.56265735\n",
      "Iteration 317, loss = 0.56197023\n",
      "Iteration 318, loss = 0.56161600\n",
      "Iteration 319, loss = 0.56048689\n",
      "Iteration 320, loss = 0.56038729\n",
      "Iteration 321, loss = 0.55972230\n",
      "Iteration 322, loss = 0.55971528\n",
      "Iteration 323, loss = 0.55907150\n",
      "Iteration 324, loss = 0.55879487\n",
      "Iteration 325, loss = 0.55882241\n",
      "Iteration 326, loss = 0.55778303\n",
      "Iteration 327, loss = 0.55752570\n",
      "Iteration 328, loss = 0.55707599\n",
      "Iteration 329, loss = 0.55644171\n",
      "Iteration 330, loss = 0.55639718\n",
      "Iteration 331, loss = 0.55556882\n",
      "Iteration 332, loss = 0.55489956\n",
      "Iteration 333, loss = 0.55477501\n",
      "Iteration 334, loss = 0.55433308\n",
      "Iteration 335, loss = 0.55358339\n",
      "Iteration 336, loss = 0.55337338\n",
      "Iteration 337, loss = 0.55260379\n",
      "Iteration 338, loss = 0.55247411\n",
      "Iteration 339, loss = 0.55193152\n",
      "Iteration 340, loss = 0.55186415\n",
      "Iteration 341, loss = 0.55115485\n",
      "Iteration 342, loss = 0.55033016\n",
      "Iteration 343, loss = 0.55002133\n",
      "Iteration 344, loss = 0.54955932\n",
      "Iteration 345, loss = 0.54944809\n",
      "Iteration 346, loss = 0.54889199\n",
      "Iteration 347, loss = 0.54845675\n",
      "Iteration 348, loss = 0.54764584\n",
      "Iteration 349, loss = 0.54762138\n",
      "Iteration 350, loss = 0.54701150\n",
      "Iteration 351, loss = 0.54642588\n",
      "Iteration 352, loss = 0.54590208\n",
      "Iteration 353, loss = 0.54555847\n",
      "Iteration 354, loss = 0.54511568\n",
      "Iteration 355, loss = 0.54436184\n",
      "Iteration 356, loss = 0.54412355\n",
      "Iteration 357, loss = 0.54347267\n",
      "Iteration 358, loss = 0.54341858\n",
      "Iteration 359, loss = 0.54310014\n",
      "Iteration 360, loss = 0.54222505\n",
      "Iteration 361, loss = 0.54163253\n",
      "Iteration 362, loss = 0.54130624\n",
      "Iteration 363, loss = 0.54073710\n",
      "Iteration 364, loss = 0.54017871\n",
      "Iteration 365, loss = 0.53994443\n",
      "Iteration 366, loss = 0.53914568\n",
      "Iteration 367, loss = 0.53935281\n",
      "Iteration 368, loss = 0.53873765\n",
      "Iteration 369, loss = 0.53824968\n",
      "Iteration 370, loss = 0.53768946\n",
      "Iteration 371, loss = 0.53678144\n",
      "Iteration 372, loss = 0.53662487\n",
      "Iteration 373, loss = 0.53611430\n",
      "Iteration 374, loss = 0.53643306\n",
      "Iteration 375, loss = 0.53601140\n",
      "Iteration 376, loss = 0.53488769\n",
      "Iteration 377, loss = 0.53422310\n",
      "Iteration 378, loss = 0.53406198\n",
      "Iteration 379, loss = 0.53352968\n",
      "Iteration 380, loss = 0.53271340\n",
      "Iteration 381, loss = 0.53302832\n",
      "Iteration 382, loss = 0.53242848\n",
      "Iteration 383, loss = 0.53086666\n",
      "Iteration 384, loss = 0.53105383\n",
      "Iteration 385, loss = 0.53089542\n",
      "Iteration 386, loss = 0.52984282\n",
      "Iteration 387, loss = 0.52965965\n",
      "Iteration 388, loss = 0.52896679\n",
      "Iteration 389, loss = 0.52891338\n",
      "Iteration 390, loss = 0.52820715\n",
      "Iteration 391, loss = 0.52772248\n",
      "Iteration 392, loss = 0.52753675\n",
      "Iteration 393, loss = 0.52673528\n",
      "Iteration 394, loss = 0.52610637\n",
      "Iteration 395, loss = 0.52590216\n",
      "Iteration 396, loss = 0.52519944\n",
      "Iteration 397, loss = 0.52449200\n",
      "Iteration 398, loss = 0.52429317\n",
      "Iteration 399, loss = 0.52417438\n",
      "Iteration 400, loss = 0.52374086\n",
      "Iteration 401, loss = 0.52355133\n",
      "Iteration 402, loss = 0.52284086\n",
      "Iteration 403, loss = 0.52198864\n",
      "Iteration 404, loss = 0.52218569\n",
      "Iteration 405, loss = 0.52121266\n",
      "Iteration 406, loss = 0.52063748\n",
      "Iteration 407, loss = 0.52049125\n",
      "Iteration 408, loss = 0.52007198\n",
      "Iteration 409, loss = 0.51909356\n",
      "Iteration 410, loss = 0.51896210\n",
      "Iteration 411, loss = 0.51823987\n",
      "Iteration 412, loss = 0.51828341\n",
      "Iteration 413, loss = 0.51734671\n",
      "Iteration 414, loss = 0.51653326\n",
      "Iteration 415, loss = 0.51621556\n",
      "Iteration 416, loss = 0.51593317\n",
      "Iteration 417, loss = 0.51560493\n",
      "Iteration 418, loss = 0.51522159\n",
      "Iteration 419, loss = 0.51440128\n",
      "Iteration 420, loss = 0.51427905\n",
      "Iteration 421, loss = 0.51315193\n",
      "Iteration 422, loss = 0.51360796\n",
      "Iteration 423, loss = 0.51255797\n",
      "Iteration 424, loss = 0.51212619\n",
      "Iteration 425, loss = 0.51160564\n",
      "Iteration 426, loss = 0.51113364\n",
      "Iteration 427, loss = 0.51075495\n",
      "Iteration 428, loss = 0.51021783\n",
      "Iteration 429, loss = 0.50970455\n",
      "Iteration 430, loss = 0.50991378\n",
      "Iteration 431, loss = 0.50909483\n",
      "Iteration 432, loss = 0.50845452\n",
      "Iteration 433, loss = 0.50770955\n",
      "Iteration 434, loss = 0.50773925\n",
      "Iteration 435, loss = 0.50715650\n",
      "Iteration 436, loss = 0.50665471\n",
      "Iteration 437, loss = 0.50640779\n",
      "Iteration 438, loss = 0.50554314\n",
      "Iteration 439, loss = 0.50483080\n",
      "Iteration 440, loss = 0.50452616\n",
      "Iteration 441, loss = 0.50471741\n",
      "Iteration 442, loss = 0.50415653\n",
      "Iteration 443, loss = 0.50309708\n",
      "Iteration 444, loss = 0.50259902\n",
      "Iteration 445, loss = 0.50228032\n",
      "Iteration 446, loss = 0.50206617\n",
      "Iteration 447, loss = 0.50113731\n",
      "Iteration 448, loss = 0.50097789\n",
      "Iteration 449, loss = 0.50032812\n",
      "Iteration 450, loss = 0.50048431\n",
      "Iteration 451, loss = 0.49951841\n",
      "Iteration 452, loss = 0.49897123\n",
      "Iteration 453, loss = 0.49838028\n",
      "Iteration 454, loss = 0.49853169\n",
      "Iteration 455, loss = 0.49751947\n",
      "Iteration 456, loss = 0.49708804\n",
      "Iteration 457, loss = 0.49624265\n",
      "Iteration 458, loss = 0.49650264\n",
      "Iteration 459, loss = 0.49541317\n",
      "Iteration 460, loss = 0.49538250\n",
      "Iteration 461, loss = 0.49488908\n",
      "Iteration 462, loss = 0.49458569\n",
      "Iteration 463, loss = 0.49390974\n",
      "Iteration 464, loss = 0.49369638\n",
      "Iteration 465, loss = 0.49284737\n",
      "Iteration 466, loss = 0.49229659\n",
      "Iteration 467, loss = 0.49208717\n",
      "Iteration 468, loss = 0.49232009\n",
      "Iteration 469, loss = 0.49097691\n",
      "Iteration 470, loss = 0.49076026\n",
      "Iteration 471, loss = 0.49032397\n",
      "Iteration 472, loss = 0.48969988\n",
      "Iteration 473, loss = 0.48923089\n",
      "Iteration 474, loss = 0.48910418\n",
      "Iteration 475, loss = 0.48835904\n",
      "Iteration 476, loss = 0.48810201\n",
      "Iteration 477, loss = 0.48744399\n",
      "Iteration 478, loss = 0.48712718\n",
      "Iteration 479, loss = 0.48691229\n",
      "Iteration 480, loss = 0.48615595\n",
      "Iteration 481, loss = 0.48600849\n",
      "Iteration 482, loss = 0.48524319\n",
      "Iteration 483, loss = 0.48477956\n",
      "Iteration 484, loss = 0.48461179\n",
      "Iteration 485, loss = 0.48401203\n",
      "Iteration 486, loss = 0.48352236\n",
      "Iteration 487, loss = 0.48322925\n",
      "Iteration 488, loss = 0.48290484\n",
      "Iteration 489, loss = 0.48214236\n",
      "Iteration 490, loss = 0.48153897\n",
      "Iteration 491, loss = 0.48180398\n",
      "Iteration 492, loss = 0.48076288\n",
      "Iteration 493, loss = 0.48090322\n",
      "Iteration 494, loss = 0.48018001\n",
      "Iteration 495, loss = 0.47967779\n",
      "Iteration 496, loss = 0.47922339\n",
      "Iteration 497, loss = 0.47900751\n",
      "Iteration 498, loss = 0.47825239\n",
      "Iteration 499, loss = 0.47795515\n",
      "Iteration 500, loss = 0.47759365\n",
      "Iteration 501, loss = 0.47701002\n",
      "Iteration 502, loss = 0.47671472\n",
      "Iteration 503, loss = 0.47661313\n",
      "Iteration 504, loss = 0.47595266\n",
      "Iteration 505, loss = 0.47554219\n",
      "Iteration 506, loss = 0.47528757\n",
      "Iteration 507, loss = 0.47444539\n",
      "Iteration 508, loss = 0.47432351\n",
      "Iteration 509, loss = 0.47390341\n",
      "Iteration 510, loss = 0.47375245\n",
      "Iteration 511, loss = 0.47294170\n",
      "Iteration 512, loss = 0.47210083\n",
      "Iteration 513, loss = 0.47202517\n",
      "Iteration 514, loss = 0.47186291\n",
      "Iteration 515, loss = 0.47161083\n",
      "Iteration 516, loss = 0.47116101\n",
      "Iteration 517, loss = 0.47032613\n",
      "Iteration 518, loss = 0.47011433\n",
      "Iteration 519, loss = 0.46971454\n",
      "Iteration 520, loss = 0.46942652\n",
      "Iteration 521, loss = 0.46893084\n",
      "Iteration 522, loss = 0.46874484\n",
      "Iteration 523, loss = 0.46848752\n",
      "Iteration 524, loss = 0.46767568\n",
      "Iteration 525, loss = 0.46724300\n",
      "Iteration 526, loss = 0.46642989\n",
      "Iteration 527, loss = 0.46695037\n",
      "Iteration 528, loss = 0.46634277\n",
      "Iteration 529, loss = 0.46601287\n",
      "Iteration 530, loss = 0.46519138\n",
      "Iteration 531, loss = 0.46488649\n",
      "Iteration 532, loss = 0.46498537\n",
      "Iteration 533, loss = 0.46402219\n",
      "Iteration 534, loss = 0.46317182\n",
      "Iteration 535, loss = 0.46329381\n",
      "Iteration 536, loss = 0.46283140\n",
      "Iteration 537, loss = 0.46275356\n",
      "Iteration 538, loss = 0.46244166\n",
      "Iteration 539, loss = 0.46194437\n",
      "Iteration 540, loss = 0.46152634\n",
      "Iteration 541, loss = 0.46113447\n",
      "Iteration 542, loss = 0.46067979\n",
      "Iteration 543, loss = 0.46019568\n",
      "Iteration 544, loss = 0.46036084\n",
      "Iteration 545, loss = 0.45992699\n",
      "Iteration 546, loss = 0.45895095\n",
      "Iteration 547, loss = 0.45886738\n",
      "Iteration 548, loss = 0.45873332\n",
      "Iteration 549, loss = 0.45778514\n",
      "Iteration 550, loss = 0.45761289\n",
      "Iteration 551, loss = 0.45746408\n",
      "Iteration 552, loss = 0.45672289\n",
      "Iteration 553, loss = 0.45668771\n",
      "Iteration 554, loss = 0.45627878\n",
      "Iteration 555, loss = 0.45578154\n",
      "Iteration 556, loss = 0.45547738\n",
      "Iteration 557, loss = 0.45525183\n",
      "Iteration 558, loss = 0.45468292\n",
      "Iteration 559, loss = 0.45388264\n",
      "Iteration 560, loss = 0.45365308\n",
      "Iteration 561, loss = 0.45406707\n",
      "Iteration 562, loss = 0.45337290\n",
      "Iteration 563, loss = 0.45359205\n",
      "Iteration 564, loss = 0.45252831\n",
      "Iteration 565, loss = 0.45241934\n",
      "Iteration 566, loss = 0.45188391\n",
      "Iteration 567, loss = 0.45128855\n",
      "Iteration 568, loss = 0.45159346\n",
      "Iteration 569, loss = 0.45080037\n",
      "Iteration 570, loss = 0.45082444\n",
      "Iteration 571, loss = 0.45008005\n",
      "Iteration 572, loss = 0.44981848\n",
      "Iteration 573, loss = 0.44927768\n",
      "Iteration 574, loss = 0.44905755\n",
      "Iteration 575, loss = 0.44925976\n",
      "Iteration 576, loss = 0.44835948\n",
      "Iteration 577, loss = 0.44804147\n",
      "Iteration 578, loss = 0.44808526\n",
      "Iteration 579, loss = 0.44766470\n",
      "Iteration 580, loss = 0.44680068\n",
      "Iteration 581, loss = 0.44720527\n",
      "Iteration 582, loss = 0.44657587\n",
      "Iteration 583, loss = 0.44617672\n",
      "Iteration 584, loss = 0.44587072\n",
      "Iteration 585, loss = 0.44608182\n",
      "Iteration 586, loss = 0.44500919\n",
      "Iteration 587, loss = 0.44470018\n",
      "Iteration 588, loss = 0.44448115\n",
      "Iteration 589, loss = 0.44430667\n",
      "Iteration 590, loss = 0.44409701\n",
      "Iteration 591, loss = 0.44357516\n",
      "Iteration 592, loss = 0.44299726\n",
      "Iteration 593, loss = 0.44294763\n",
      "Iteration 594, loss = 0.44270745\n",
      "Iteration 595, loss = 0.44268288\n",
      "Iteration 596, loss = 0.44153737\n",
      "Iteration 597, loss = 0.44182550\n",
      "Iteration 598, loss = 0.44132832\n",
      "Iteration 599, loss = 0.44105845\n",
      "Iteration 600, loss = 0.44084180\n",
      "Iteration 601, loss = 0.44022203\n",
      "Iteration 602, loss = 0.43978119\n",
      "Iteration 603, loss = 0.43947812\n",
      "Iteration 604, loss = 0.43936272\n",
      "Iteration 605, loss = 0.43950132\n",
      "Iteration 606, loss = 0.43943021\n",
      "Iteration 607, loss = 0.43835332\n",
      "Iteration 608, loss = 0.43844376\n",
      "Iteration 609, loss = 0.43779379\n",
      "Iteration 610, loss = 0.43722437\n",
      "Iteration 611, loss = 0.43739114\n",
      "Iteration 612, loss = 0.43658579\n",
      "Iteration 613, loss = 0.43672146\n",
      "Iteration 614, loss = 0.43590200\n",
      "Iteration 615, loss = 0.43629926\n",
      "Iteration 616, loss = 0.43581982\n",
      "Iteration 617, loss = 0.43569062\n",
      "Iteration 618, loss = 0.43503465\n",
      "Iteration 619, loss = 0.43507352\n",
      "Iteration 620, loss = 0.43480182\n",
      "Iteration 621, loss = 0.43424485\n",
      "Iteration 622, loss = 0.43419627\n",
      "Iteration 623, loss = 0.43358848\n",
      "Iteration 624, loss = 0.43358446\n",
      "Iteration 625, loss = 0.43317189\n",
      "Iteration 626, loss = 0.43313802\n",
      "Iteration 627, loss = 0.43272663\n",
      "Iteration 628, loss = 0.43282113\n",
      "Iteration 629, loss = 0.43198027\n",
      "Iteration 630, loss = 0.43180398\n",
      "Iteration 631, loss = 0.43197182\n",
      "Iteration 632, loss = 0.43129770\n",
      "Iteration 633, loss = 0.43109766\n",
      "Iteration 634, loss = 0.43044667\n",
      "Iteration 635, loss = 0.43015514\n",
      "Iteration 636, loss = 0.42983986\n",
      "Iteration 637, loss = 0.42988742\n",
      "Iteration 638, loss = 0.42954801\n",
      "Iteration 639, loss = 0.42952098\n",
      "Iteration 640, loss = 0.42929423\n",
      "Iteration 641, loss = 0.42859788\n",
      "Iteration 642, loss = 0.42819148\n",
      "Iteration 643, loss = 0.42836882\n",
      "Iteration 644, loss = 0.42783393\n",
      "Iteration 645, loss = 0.42770899\n",
      "Iteration 646, loss = 0.42711416\n",
      "Iteration 647, loss = 0.42702220\n",
      "Iteration 648, loss = 0.42679222\n",
      "Iteration 649, loss = 0.42674096\n",
      "Iteration 650, loss = 0.42648441\n",
      "Iteration 651, loss = 0.42592559\n",
      "Iteration 652, loss = 0.42575597\n",
      "Iteration 653, loss = 0.42585384\n",
      "Iteration 654, loss = 0.42553909\n",
      "Iteration 655, loss = 0.42499100\n",
      "Iteration 656, loss = 0.42480283\n",
      "Iteration 657, loss = 0.42437109\n",
      "Iteration 658, loss = 0.42433745\n",
      "Iteration 659, loss = 0.42389878\n",
      "Iteration 660, loss = 0.42359240\n",
      "Iteration 661, loss = 0.42343587\n",
      "Iteration 662, loss = 0.42382875\n",
      "Iteration 663, loss = 0.42341786\n",
      "Iteration 664, loss = 0.42260596\n",
      "Iteration 665, loss = 0.42265619\n",
      "Iteration 666, loss = 0.42249532\n",
      "Iteration 667, loss = 0.42192938\n",
      "Iteration 668, loss = 0.42198136\n",
      "Iteration 669, loss = 0.42176037\n",
      "Iteration 670, loss = 0.42133527\n",
      "Iteration 671, loss = 0.42089369\n",
      "Iteration 672, loss = 0.42097307\n",
      "Iteration 673, loss = 0.42055974\n",
      "Iteration 674, loss = 0.42007114\n",
      "Iteration 675, loss = 0.42017386\n",
      "Iteration 676, loss = 0.41995002\n",
      "Iteration 677, loss = 0.41983024\n",
      "Iteration 678, loss = 0.41923331\n",
      "Iteration 679, loss = 0.41931985\n",
      "Iteration 680, loss = 0.41914487\n",
      "Iteration 681, loss = 0.41891929\n",
      "Iteration 682, loss = 0.41881487\n",
      "Iteration 683, loss = 0.41821547\n",
      "Iteration 684, loss = 0.41798298\n",
      "Iteration 685, loss = 0.41828022\n",
      "Iteration 686, loss = 0.41749616\n",
      "Iteration 687, loss = 0.41729934\n",
      "Iteration 688, loss = 0.41706459\n",
      "Iteration 689, loss = 0.41685785\n",
      "Iteration 690, loss = 0.41692270\n",
      "Iteration 691, loss = 0.41657458\n",
      "Iteration 692, loss = 0.41633427\n",
      "Iteration 693, loss = 0.41597387\n",
      "Iteration 694, loss = 0.41645797\n",
      "Iteration 695, loss = 0.41564551\n",
      "Iteration 696, loss = 0.41555743\n",
      "Iteration 697, loss = 0.41504224\n",
      "Iteration 698, loss = 0.41461680\n",
      "Iteration 699, loss = 0.41471480\n",
      "Iteration 700, loss = 0.41452561\n",
      "Iteration 701, loss = 0.41455318\n",
      "Iteration 702, loss = 0.41450542\n",
      "Iteration 703, loss = 0.41347394\n",
      "Iteration 704, loss = 0.41386515\n",
      "Iteration 705, loss = 0.41399284\n",
      "Iteration 706, loss = 0.41340909\n",
      "Iteration 707, loss = 0.41316336\n",
      "Iteration 708, loss = 0.41310747\n",
      "Iteration 709, loss = 0.41279164\n",
      "Iteration 710, loss = 0.41231793\n",
      "Iteration 711, loss = 0.41221893\n",
      "Iteration 712, loss = 0.41179253\n",
      "Iteration 713, loss = 0.41220841\n",
      "Iteration 714, loss = 0.41133227\n",
      "Iteration 715, loss = 0.41185651\n",
      "Iteration 716, loss = 0.41147155\n",
      "Iteration 717, loss = 0.41095702\n",
      "Iteration 718, loss = 0.41052728\n",
      "Iteration 719, loss = 0.41037014\n",
      "Iteration 720, loss = 0.41052371\n",
      "Iteration 721, loss = 0.41032145\n",
      "Iteration 722, loss = 0.41048205\n",
      "Iteration 723, loss = 0.40997658\n",
      "Iteration 724, loss = 0.41007451\n",
      "Iteration 725, loss = 0.40949008\n",
      "Iteration 726, loss = 0.40937186\n",
      "Iteration 727, loss = 0.40870080\n",
      "Iteration 728, loss = 0.40899418\n",
      "Iteration 729, loss = 0.40871135\n",
      "Iteration 730, loss = 0.40849499\n",
      "Iteration 731, loss = 0.40825014\n",
      "Iteration 732, loss = 0.40804763\n",
      "Iteration 733, loss = 0.40783190\n",
      "Iteration 734, loss = 0.40750104\n",
      "Iteration 735, loss = 0.40730662\n",
      "Iteration 736, loss = 0.40749055\n",
      "Iteration 737, loss = 0.40726866\n",
      "Iteration 738, loss = 0.40708488\n",
      "Iteration 739, loss = 0.40660653\n",
      "Iteration 740, loss = 0.40648458\n",
      "Iteration 741, loss = 0.40627929\n",
      "Iteration 742, loss = 0.40621616\n",
      "Iteration 743, loss = 0.40593037\n",
      "Iteration 744, loss = 0.40611163\n",
      "Iteration 745, loss = 0.40524077\n",
      "Iteration 746, loss = 0.40563673\n",
      "Iteration 747, loss = 0.40544906\n",
      "Iteration 748, loss = 0.40492581\n",
      "Iteration 749, loss = 0.40448092\n",
      "Iteration 750, loss = 0.40492170\n",
      "Iteration 751, loss = 0.40474096\n",
      "Iteration 752, loss = 0.40447855\n",
      "Iteration 753, loss = 0.40408223\n",
      "Iteration 754, loss = 0.40379161\n",
      "Iteration 755, loss = 0.40369747\n",
      "Iteration 756, loss = 0.40359686\n",
      "Iteration 757, loss = 0.40316075\n",
      "Iteration 758, loss = 0.40340028\n",
      "Iteration 759, loss = 0.40264242\n",
      "Iteration 760, loss = 0.40293951\n",
      "Iteration 761, loss = 0.40269930\n",
      "Iteration 762, loss = 0.40295006\n",
      "Iteration 763, loss = 0.40229124\n",
      "Iteration 764, loss = 0.40234203\n",
      "Iteration 765, loss = 0.40160213\n",
      "Iteration 766, loss = 0.40189605\n",
      "Iteration 767, loss = 0.40207550\n",
      "Iteration 768, loss = 0.40139076\n",
      "Iteration 769, loss = 0.40113892\n",
      "Iteration 770, loss = 0.40133307\n",
      "Iteration 771, loss = 0.40068753\n",
      "Iteration 772, loss = 0.40063074\n",
      "Iteration 773, loss = 0.40090474\n",
      "Iteration 774, loss = 0.40033047\n",
      "Iteration 775, loss = 0.40022705\n",
      "Iteration 776, loss = 0.39986904\n",
      "Iteration 777, loss = 0.39999956\n",
      "Iteration 778, loss = 0.39983213\n",
      "Iteration 779, loss = 0.39937177\n",
      "Iteration 780, loss = 0.39911959\n",
      "Iteration 781, loss = 0.39951335\n",
      "Iteration 782, loss = 0.39955535\n",
      "Iteration 783, loss = 0.39891460\n",
      "Iteration 784, loss = 0.39863021\n",
      "Iteration 785, loss = 0.39910120\n",
      "Iteration 786, loss = 0.39809374\n",
      "Iteration 787, loss = 0.39807355\n",
      "Iteration 788, loss = 0.39805566\n",
      "Iteration 789, loss = 0.39777568\n",
      "Iteration 790, loss = 0.39754515\n",
      "Iteration 791, loss = 0.39768731\n",
      "Iteration 792, loss = 0.39701945\n",
      "Iteration 793, loss = 0.39733793\n",
      "Iteration 794, loss = 0.39717332\n",
      "Iteration 795, loss = 0.39688610\n",
      "Iteration 796, loss = 0.39653677\n",
      "Iteration 797, loss = 0.39667034\n",
      "Iteration 798, loss = 0.39678591\n",
      "Iteration 799, loss = 0.39631025\n",
      "Iteration 800, loss = 0.39625650\n",
      "Iteration 801, loss = 0.39550126\n",
      "Iteration 802, loss = 0.39588423\n",
      "Iteration 803, loss = 0.39589964\n",
      "Iteration 804, loss = 0.39560894\n",
      "Iteration 805, loss = 0.39574406\n",
      "Iteration 806, loss = 0.39516419\n",
      "Iteration 807, loss = 0.39501536\n",
      "Iteration 808, loss = 0.39542804\n",
      "Iteration 809, loss = 0.39480971\n",
      "Iteration 810, loss = 0.39431061\n",
      "Iteration 811, loss = 0.39480981\n",
      "Iteration 812, loss = 0.39452969\n",
      "Iteration 813, loss = 0.39405088\n",
      "Iteration 814, loss = 0.39410139\n",
      "Iteration 815, loss = 0.39385622\n",
      "Iteration 816, loss = 0.39392655\n",
      "Iteration 817, loss = 0.39381943\n",
      "Iteration 818, loss = 0.39318589\n",
      "Iteration 819, loss = 0.39310426\n",
      "Iteration 820, loss = 0.39298224\n",
      "Iteration 821, loss = 0.39287511\n",
      "Iteration 822, loss = 0.39323218\n",
      "Iteration 823, loss = 0.39286758\n",
      "Iteration 824, loss = 0.39304577\n",
      "Iteration 825, loss = 0.39270066\n",
      "Iteration 826, loss = 0.39236095\n",
      "Iteration 827, loss = 0.39153548\n",
      "Iteration 828, loss = 0.39200071\n",
      "Iteration 829, loss = 0.39175882\n",
      "Iteration 830, loss = 0.39159015\n",
      "Iteration 831, loss = 0.39193160\n",
      "Iteration 832, loss = 0.39148611\n",
      "Iteration 833, loss = 0.39130810\n",
      "Iteration 834, loss = 0.39101093\n",
      "Iteration 835, loss = 0.39108315\n",
      "Iteration 836, loss = 0.39059851\n",
      "Iteration 837, loss = 0.39048508\n",
      "Iteration 838, loss = 0.39076245\n",
      "Iteration 839, loss = 0.39039400\n",
      "Iteration 840, loss = 0.39022914\n",
      "Iteration 841, loss = 0.39020013\n",
      "Iteration 842, loss = 0.39043598\n",
      "Iteration 843, loss = 0.38918871\n",
      "Iteration 844, loss = 0.38933825\n",
      "Iteration 845, loss = 0.38953125\n",
      "Iteration 846, loss = 0.38995611\n",
      "Iteration 847, loss = 0.38921160\n",
      "Iteration 848, loss = 0.38915395\n",
      "Iteration 849, loss = 0.38917490\n",
      "Iteration 850, loss = 0.38914187\n",
      "Iteration 851, loss = 0.38859485\n",
      "Iteration 852, loss = 0.38851019\n",
      "Iteration 853, loss = 0.38857922\n",
      "Iteration 854, loss = 0.38860901\n",
      "Iteration 855, loss = 0.38831648\n",
      "Iteration 856, loss = 0.38846383\n",
      "Iteration 857, loss = 0.38812395\n",
      "Iteration 858, loss = 0.38827347\n",
      "Iteration 859, loss = 0.38746719\n",
      "Iteration 860, loss = 0.38746092\n",
      "Iteration 861, loss = 0.38744786\n",
      "Iteration 862, loss = 0.38735196\n",
      "Iteration 863, loss = 0.38717139\n",
      "Iteration 864, loss = 0.38717494\n",
      "Iteration 865, loss = 0.38681311\n",
      "Iteration 866, loss = 0.38676707\n",
      "Iteration 867, loss = 0.38687817\n",
      "Iteration 868, loss = 0.38632874\n",
      "Iteration 869, loss = 0.38628076\n",
      "Iteration 870, loss = 0.38630800\n",
      "Iteration 871, loss = 0.38637229\n",
      "Iteration 872, loss = 0.38540753\n",
      "Iteration 873, loss = 0.38598599\n",
      "Iteration 874, loss = 0.38567896\n",
      "Iteration 875, loss = 0.38564918\n",
      "Iteration 876, loss = 0.38610673\n",
      "Iteration 877, loss = 0.38543397\n",
      "Iteration 878, loss = 0.38502564\n",
      "Iteration 879, loss = 0.38556589\n",
      "Iteration 880, loss = 0.38539028\n",
      "Iteration 881, loss = 0.38470357\n",
      "Iteration 882, loss = 0.38461466\n",
      "Iteration 883, loss = 0.38507720\n",
      "Iteration 884, loss = 0.38443473\n",
      "Iteration 885, loss = 0.38432984\n",
      "Iteration 886, loss = 0.38459846\n",
      "Iteration 887, loss = 0.38433947\n",
      "Iteration 888, loss = 0.38424084\n",
      "Iteration 889, loss = 0.38356374\n",
      "Iteration 890, loss = 0.38390013\n",
      "Iteration 891, loss = 0.38379117\n",
      "Iteration 892, loss = 0.38363487\n",
      "Iteration 893, loss = 0.38338706\n",
      "Iteration 894, loss = 0.38329816\n",
      "Iteration 895, loss = 0.38332528\n",
      "Iteration 896, loss = 0.38320833\n",
      "Iteration 897, loss = 0.38304115\n",
      "Iteration 898, loss = 0.38278687\n",
      "Iteration 899, loss = 0.38258483\n",
      "Iteration 900, loss = 0.38303937\n",
      "Iteration 901, loss = 0.38221037\n",
      "Iteration 902, loss = 0.38230147\n",
      "Iteration 903, loss = 0.38177977\n",
      "Iteration 904, loss = 0.38233186\n",
      "Iteration 905, loss = 0.38213370\n",
      "Iteration 906, loss = 0.38154909\n",
      "Iteration 907, loss = 0.38229112\n",
      "Iteration 908, loss = 0.38194245\n",
      "Iteration 909, loss = 0.38114959\n",
      "Iteration 910, loss = 0.38152337\n",
      "Iteration 911, loss = 0.38140547\n",
      "Iteration 912, loss = 0.38152910\n",
      "Iteration 913, loss = 0.38087843\n",
      "Iteration 914, loss = 0.38092362\n",
      "Iteration 915, loss = 0.38055330\n",
      "Iteration 916, loss = 0.38096435\n",
      "Iteration 917, loss = 0.38073076\n",
      "Iteration 918, loss = 0.38046994\n",
      "Iteration 919, loss = 0.38064279\n",
      "Iteration 920, loss = 0.38081982\n",
      "Iteration 921, loss = 0.38041088\n",
      "Iteration 922, loss = 0.38028281\n",
      "Iteration 923, loss = 0.37997877\n",
      "Iteration 924, loss = 0.38014091\n",
      "Iteration 925, loss = 0.37997169\n",
      "Iteration 926, loss = 0.37974275\n",
      "Iteration 927, loss = 0.37978671\n",
      "Iteration 928, loss = 0.37935038\n",
      "Iteration 929, loss = 0.37932855\n",
      "Iteration 930, loss = 0.37934167\n",
      "Iteration 931, loss = 0.37890612\n",
      "Iteration 932, loss = 0.37926008\n",
      "Iteration 933, loss = 0.37946250\n",
      "Iteration 934, loss = 0.37906355\n",
      "Iteration 935, loss = 0.37883996\n",
      "Iteration 936, loss = 0.37875808\n",
      "Iteration 937, loss = 0.37876646\n",
      "Iteration 938, loss = 0.37818272\n",
      "Iteration 939, loss = 0.37844473\n",
      "Iteration 940, loss = 0.37841819\n",
      "Iteration 941, loss = 0.37811909\n",
      "Iteration 942, loss = 0.37790252\n",
      "Iteration 943, loss = 0.37818961\n",
      "Iteration 944, loss = 0.37809510\n",
      "Iteration 945, loss = 0.37803063\n",
      "Iteration 946, loss = 0.37771539\n",
      "Iteration 947, loss = 0.37733086\n",
      "Iteration 948, loss = 0.37750673\n",
      "Iteration 949, loss = 0.37767782\n",
      "Iteration 950, loss = 0.37749043\n",
      "Iteration 951, loss = 0.37709181\n",
      "Iteration 952, loss = 0.37683524\n",
      "Iteration 953, loss = 0.37699851\n",
      "Iteration 954, loss = 0.37679026\n",
      "Iteration 955, loss = 0.37664339\n",
      "Iteration 956, loss = 0.37657740\n",
      "Iteration 957, loss = 0.37639924\n",
      "Iteration 958, loss = 0.37639096\n",
      "Iteration 959, loss = 0.37632506\n",
      "Iteration 960, loss = 0.37651794\n",
      "Iteration 961, loss = 0.37629472\n",
      "Iteration 962, loss = 0.37593935\n",
      "Iteration 963, loss = 0.37631263\n",
      "Iteration 964, loss = 0.37582992\n",
      "Iteration 965, loss = 0.37556012\n",
      "Iteration 966, loss = 0.37534505\n",
      "Iteration 967, loss = 0.37557328\n",
      "Iteration 968, loss = 0.37542947\n",
      "Iteration 969, loss = 0.37564907\n",
      "Iteration 970, loss = 0.37558921\n",
      "Iteration 971, loss = 0.37536819\n",
      "Iteration 972, loss = 0.37523275\n",
      "Iteration 973, loss = 0.37499009\n",
      "Iteration 974, loss = 0.37486565\n",
      "Iteration 975, loss = 0.37505978\n",
      "Iteration 976, loss = 0.37477646\n",
      "Iteration 977, loss = 0.37471318\n",
      "Iteration 978, loss = 0.37452896\n",
      "Iteration 979, loss = 0.37502328\n",
      "Iteration 980, loss = 0.37486157\n",
      "Iteration 981, loss = 0.37440141\n",
      "Iteration 982, loss = 0.37419617\n",
      "Iteration 983, loss = 0.37391287\n",
      "Iteration 984, loss = 0.37399811\n",
      "Iteration 985, loss = 0.37383011\n",
      "Iteration 986, loss = 0.37421541\n",
      "Iteration 987, loss = 0.37372794\n",
      "Iteration 988, loss = 0.37391559\n",
      "Iteration 989, loss = 0.37340174\n",
      "Iteration 990, loss = 0.37393086\n",
      "Iteration 991, loss = 0.37348955\n",
      "Iteration 992, loss = 0.37329163\n",
      "Iteration 993, loss = 0.37322581\n",
      "Iteration 994, loss = 0.37298331\n",
      "Iteration 995, loss = 0.37278902\n",
      "Iteration 996, loss = 0.37278221\n",
      "Iteration 997, loss = 0.37278746\n",
      "Iteration 998, loss = 0.37268585\n",
      "Iteration 999, loss = 0.37252976\n",
      "Iteration 1000, loss = 0.37280376\n",
      "Iteration 1001, loss = 0.37282289\n",
      "Iteration 1002, loss = 0.37244399\n",
      "Iteration 1003, loss = 0.37233557\n",
      "Iteration 1004, loss = 0.37233510\n",
      "Iteration 1005, loss = 0.37274167\n",
      "Iteration 1006, loss = 0.37251601\n",
      "Iteration 1007, loss = 0.37223564\n",
      "Iteration 1008, loss = 0.37201325\n",
      "Iteration 1009, loss = 0.37208536\n",
      "Iteration 1010, loss = 0.37159560\n",
      "Iteration 1011, loss = 0.37138380\n",
      "Iteration 1012, loss = 0.37141025\n",
      "Iteration 1013, loss = 0.37156253\n",
      "Iteration 1014, loss = 0.37126960\n",
      "Iteration 1015, loss = 0.37165030\n",
      "Iteration 1016, loss = 0.37144390\n",
      "Iteration 1017, loss = 0.37091247\n",
      "Iteration 1018, loss = 0.37113257\n",
      "Iteration 1019, loss = 0.37080697\n",
      "Iteration 1020, loss = 0.37100865\n",
      "Iteration 1021, loss = 0.37057714\n",
      "Iteration 1022, loss = 0.37063796\n",
      "Iteration 1023, loss = 0.37087983\n",
      "Iteration 1024, loss = 0.37034914\n",
      "Iteration 1025, loss = 0.37027787\n",
      "Iteration 1026, loss = 0.37029091\n",
      "Iteration 1027, loss = 0.37086522\n",
      "Iteration 1028, loss = 0.36991738\n",
      "Iteration 1029, loss = 0.37041973\n",
      "Iteration 1030, loss = 0.37016653\n",
      "Iteration 1031, loss = 0.36943735\n",
      "Iteration 1032, loss = 0.36991629\n",
      "Iteration 1033, loss = 0.36987454\n",
      "Iteration 1034, loss = 0.37000679\n",
      "Iteration 1035, loss = 0.36981910\n",
      "Iteration 1036, loss = 0.36938088\n",
      "Iteration 1037, loss = 0.36930210\n",
      "Iteration 1038, loss = 0.36942375\n",
      "Iteration 1039, loss = 0.36951327\n",
      "Iteration 1040, loss = 0.36920943\n",
      "Iteration 1041, loss = 0.36919582\n",
      "Iteration 1042, loss = 0.36905888\n",
      "Iteration 1043, loss = 0.36946314\n",
      "Iteration 1044, loss = 0.36856594\n",
      "Iteration 1045, loss = 0.36932144\n",
      "Iteration 1046, loss = 0.36872756\n",
      "Iteration 1047, loss = 0.36893956\n",
      "Iteration 1048, loss = 0.36867014\n",
      "Iteration 1049, loss = 0.36867607\n",
      "Iteration 1050, loss = 0.36830037\n",
      "Iteration 1051, loss = 0.36838748\n",
      "Iteration 1052, loss = 0.36815055\n",
      "Iteration 1053, loss = 0.36837134\n",
      "Iteration 1054, loss = 0.36825005\n",
      "Iteration 1055, loss = 0.36838669\n",
      "Iteration 1056, loss = 0.36805173\n",
      "Iteration 1057, loss = 0.36779293\n",
      "Iteration 1058, loss = 0.36797975\n",
      "Iteration 1059, loss = 0.36805245\n",
      "Iteration 1060, loss = 0.36766001\n",
      "Iteration 1061, loss = 0.36795570\n",
      "Iteration 1062, loss = 0.36761693\n",
      "Iteration 1063, loss = 0.36789179\n",
      "Iteration 1064, loss = 0.36741833\n",
      "Iteration 1065, loss = 0.36773655\n",
      "Iteration 1066, loss = 0.36735537\n",
      "Iteration 1067, loss = 0.36717534\n",
      "Iteration 1068, loss = 0.36699407\n",
      "Iteration 1069, loss = 0.36721752\n",
      "Iteration 1070, loss = 0.36737711\n",
      "Iteration 1071, loss = 0.36694954\n",
      "Iteration 1072, loss = 0.36681076\n",
      "Iteration 1073, loss = 0.36664326\n",
      "Iteration 1074, loss = 0.36644209\n",
      "Iteration 1075, loss = 0.36639841\n",
      "Iteration 1076, loss = 0.36656566\n",
      "Iteration 1077, loss = 0.36675886\n",
      "Iteration 1078, loss = 0.36665902\n",
      "Iteration 1079, loss = 0.36682396\n",
      "Iteration 1080, loss = 0.36585573\n",
      "Iteration 1081, loss = 0.36634180\n",
      "Iteration 1082, loss = 0.36613767\n",
      "Iteration 1083, loss = 0.36642518\n",
      "Iteration 1084, loss = 0.36626882\n",
      "Iteration 1085, loss = 0.36573198\n",
      "Iteration 1086, loss = 0.36613948\n",
      "Iteration 1087, loss = 0.36565370\n",
      "Iteration 1088, loss = 0.36563502\n",
      "Iteration 1089, loss = 0.36575831\n",
      "Iteration 1090, loss = 0.36629716\n",
      "Iteration 1091, loss = 0.36522440\n",
      "Iteration 1092, loss = 0.36575559\n",
      "Iteration 1093, loss = 0.36521601\n",
      "Iteration 1094, loss = 0.36542479\n",
      "Iteration 1095, loss = 0.36484200\n",
      "Iteration 1096, loss = 0.36510315\n",
      "Iteration 1097, loss = 0.36567405\n",
      "Iteration 1098, loss = 0.36537192\n",
      "Iteration 1099, loss = 0.36523261\n",
      "Iteration 1100, loss = 0.36527431\n",
      "Iteration 1101, loss = 0.36514912\n",
      "Iteration 1102, loss = 0.36474971\n",
      "Iteration 1103, loss = 0.36500116\n",
      "Iteration 1104, loss = 0.36451578\n",
      "Iteration 1105, loss = 0.36466965\n",
      "Iteration 1106, loss = 0.36429036\n",
      "Iteration 1107, loss = 0.36426764\n",
      "Iteration 1108, loss = 0.36414519\n",
      "Iteration 1109, loss = 0.36465638\n",
      "Iteration 1110, loss = 0.36450431\n",
      "Iteration 1111, loss = 0.36432827\n",
      "Iteration 1112, loss = 0.36384813\n",
      "Iteration 1113, loss = 0.36419845\n",
      "Iteration 1114, loss = 0.36401438\n",
      "Iteration 1115, loss = 0.36404078\n",
      "Iteration 1116, loss = 0.36348557\n",
      "Iteration 1117, loss = 0.36371692\n",
      "Iteration 1118, loss = 0.36376576\n",
      "Iteration 1119, loss = 0.36428715\n",
      "Iteration 1120, loss = 0.36406888\n",
      "Iteration 1121, loss = 0.36369470\n",
      "Iteration 1122, loss = 0.36340878\n",
      "Iteration 1123, loss = 0.36334571\n",
      "Iteration 1124, loss = 0.36294819\n",
      "Iteration 1125, loss = 0.36291272\n",
      "Iteration 1126, loss = 0.36355738\n",
      "Iteration 1127, loss = 0.36343746\n",
      "Iteration 1128, loss = 0.36331343\n",
      "Iteration 1129, loss = 0.36289246\n",
      "Iteration 1130, loss = 0.36317893\n",
      "Iteration 1131, loss = 0.36291348\n",
      "Iteration 1132, loss = 0.36307813\n",
      "Iteration 1133, loss = 0.36275745\n",
      "Iteration 1134, loss = 0.36268568\n",
      "Iteration 1135, loss = 0.36261500\n",
      "Iteration 1136, loss = 0.36240449\n",
      "Iteration 1137, loss = 0.36243563\n",
      "Iteration 1138, loss = 0.36235914\n",
      "Iteration 1139, loss = 0.36270429\n",
      "Iteration 1140, loss = 0.36190492\n",
      "Iteration 1141, loss = 0.36224811\n",
      "Iteration 1142, loss = 0.36200345\n",
      "Iteration 1143, loss = 0.36235549\n",
      "Iteration 1144, loss = 0.36216492\n",
      "Iteration 1145, loss = 0.36198750\n",
      "Iteration 1146, loss = 0.36175792\n",
      "Iteration 1147, loss = 0.36191966\n",
      "Iteration 1148, loss = 0.36238435\n",
      "Iteration 1149, loss = 0.36170855\n",
      "Iteration 1150, loss = 0.36183564\n",
      "Iteration 1151, loss = 0.36163478\n",
      "Iteration 1152, loss = 0.36216799\n",
      "Iteration 1153, loss = 0.36115204\n",
      "Iteration 1154, loss = 0.36144437\n",
      "Iteration 1155, loss = 0.36130496\n",
      "Iteration 1156, loss = 0.36132270\n",
      "Iteration 1157, loss = 0.36117603\n",
      "Iteration 1158, loss = 0.36130027\n",
      "Iteration 1159, loss = 0.36102599\n",
      "Iteration 1160, loss = 0.36108633\n",
      "Iteration 1161, loss = 0.36102675\n",
      "Iteration 1162, loss = 0.36130836\n",
      "Iteration 1163, loss = 0.36051255\n",
      "Iteration 1164, loss = 0.36067278\n",
      "Iteration 1165, loss = 0.36116844\n",
      "Iteration 1166, loss = 0.36088648\n",
      "Iteration 1167, loss = 0.36050097\n",
      "Iteration 1168, loss = 0.36053632\n",
      "Iteration 1169, loss = 0.36081980\n",
      "Iteration 1170, loss = 0.36039691\n",
      "Iteration 1171, loss = 0.36090499\n",
      "Iteration 1172, loss = 0.36065493\n",
      "Iteration 1173, loss = 0.36012825\n",
      "Iteration 1174, loss = 0.36038543\n",
      "Iteration 1175, loss = 0.36038538\n",
      "Iteration 1176, loss = 0.36002546\n",
      "Iteration 1177, loss = 0.36019102\n",
      "Iteration 1178, loss = 0.36003181\n",
      "Iteration 1179, loss = 0.35973746\n",
      "Iteration 1180, loss = 0.36053837\n",
      "Iteration 1181, loss = 0.35968896\n",
      "Iteration 1182, loss = 0.35971663\n",
      "Iteration 1183, loss = 0.36005886\n",
      "Iteration 1184, loss = 0.35965340\n",
      "Iteration 1185, loss = 0.35987497\n",
      "Iteration 1186, loss = 0.35921667\n",
      "Iteration 1187, loss = 0.35921331\n",
      "Iteration 1188, loss = 0.35960460\n",
      "Iteration 1189, loss = 0.35973772\n",
      "Iteration 1190, loss = 0.35927934\n",
      "Iteration 1191, loss = 0.35952391\n",
      "Iteration 1192, loss = 0.35903743\n",
      "Iteration 1193, loss = 0.35934855\n",
      "Iteration 1194, loss = 0.35903672\n",
      "Iteration 1195, loss = 0.35950734\n",
      "Iteration 1196, loss = 0.35865663\n",
      "Iteration 1197, loss = 0.35913801\n",
      "Iteration 1198, loss = 0.35962934\n",
      "Iteration 1199, loss = 0.35904804\n",
      "Iteration 1200, loss = 0.35865336\n",
      "Iteration 1201, loss = 0.35883134\n",
      "Iteration 1202, loss = 0.35872268\n",
      "Iteration 1203, loss = 0.35849758\n",
      "Iteration 1204, loss = 0.35903729\n",
      "Iteration 1205, loss = 0.35843215\n",
      "Iteration 1206, loss = 0.35835013\n",
      "Iteration 1207, loss = 0.35839938\n",
      "Iteration 1208, loss = 0.35795270\n",
      "Iteration 1209, loss = 0.35832837\n",
      "Iteration 1210, loss = 0.35796932\n",
      "Iteration 1211, loss = 0.35822962\n",
      "Iteration 1212, loss = 0.35789897\n",
      "Iteration 1213, loss = 0.35788126\n",
      "Iteration 1214, loss = 0.35801575\n",
      "Iteration 1215, loss = 0.35797020\n",
      "Iteration 1216, loss = 0.35806885\n",
      "Iteration 1217, loss = 0.35824816\n",
      "Iteration 1218, loss = 0.35799354\n",
      "Iteration 1219, loss = 0.35788651\n",
      "Iteration 1220, loss = 0.35737864\n",
      "Iteration 1221, loss = 0.35736004\n",
      "Iteration 1222, loss = 0.35734726\n",
      "Iteration 1223, loss = 0.35778006\n",
      "Iteration 1224, loss = 0.35763665\n",
      "Iteration 1225, loss = 0.35736113\n",
      "Iteration 1226, loss = 0.35733227\n",
      "Iteration 1227, loss = 0.35742038\n",
      "Iteration 1228, loss = 0.35742611\n",
      "Iteration 1229, loss = 0.35673029\n",
      "Iteration 1230, loss = 0.35695109\n",
      "Iteration 1231, loss = 0.35679265\n",
      "Iteration 1232, loss = 0.35702485\n",
      "Iteration 1233, loss = 0.35699113\n",
      "Iteration 1234, loss = 0.35705207\n",
      "Iteration 1235, loss = 0.35694605\n",
      "Iteration 1236, loss = 0.35664184\n",
      "Iteration 1237, loss = 0.35624122\n",
      "Iteration 1238, loss = 0.35700533\n",
      "Iteration 1239, loss = 0.35690725\n",
      "Iteration 1240, loss = 0.35618312\n",
      "Iteration 1241, loss = 0.35662629\n",
      "Iteration 1242, loss = 0.35640693\n",
      "Iteration 1243, loss = 0.35670112\n",
      "Iteration 1244, loss = 0.35643617\n",
      "Iteration 1245, loss = 0.35607648\n",
      "Iteration 1246, loss = 0.35661154\n",
      "Iteration 1247, loss = 0.35679382\n",
      "Iteration 1248, loss = 0.35601205\n",
      "Iteration 1249, loss = 0.35611110\n",
      "Iteration 1250, loss = 0.35574937\n",
      "Iteration 1251, loss = 0.35608910\n",
      "Iteration 1252, loss = 0.35580657\n",
      "Iteration 1253, loss = 0.35559320\n",
      "Iteration 1254, loss = 0.35589874\n",
      "Iteration 1255, loss = 0.35599324\n",
      "Iteration 1256, loss = 0.35576134\n",
      "Iteration 1257, loss = 0.35585851\n",
      "Iteration 1258, loss = 0.35561151\n",
      "Iteration 1259, loss = 0.35533971\n",
      "Iteration 1260, loss = 0.35508977\n",
      "Iteration 1261, loss = 0.35566406\n",
      "Iteration 1262, loss = 0.35498267\n",
      "Iteration 1263, loss = 0.35520061\n",
      "Iteration 1264, loss = 0.35496978\n",
      "Iteration 1265, loss = 0.35518742\n",
      "Iteration 1266, loss = 0.35546846\n",
      "Iteration 1267, loss = 0.35534596\n",
      "Iteration 1268, loss = 0.35497977\n",
      "Iteration 1269, loss = 0.35490288\n",
      "Iteration 1270, loss = 0.35501579\n",
      "Iteration 1271, loss = 0.35487976\n",
      "Iteration 1272, loss = 0.35498249\n",
      "Iteration 1273, loss = 0.35462350\n",
      "Iteration 1274, loss = 0.35463268\n",
      "Iteration 1275, loss = 0.35503229\n",
      "Iteration 1276, loss = 0.35468355\n",
      "Iteration 1277, loss = 0.35449948\n",
      "Iteration 1278, loss = 0.35452218\n",
      "Iteration 1279, loss = 0.35436741\n",
      "Iteration 1280, loss = 0.35473257\n",
      "Iteration 1281, loss = 0.35444259\n",
      "Iteration 1282, loss = 0.35514884\n",
      "Iteration 1283, loss = 0.35423452\n",
      "Iteration 1284, loss = 0.35460664\n",
      "Iteration 1285, loss = 0.35427050\n",
      "Iteration 1286, loss = 0.35450011\n",
      "Iteration 1287, loss = 0.35464485\n",
      "Iteration 1288, loss = 0.35406987\n",
      "Iteration 1289, loss = 0.35441766\n",
      "Iteration 1290, loss = 0.35367394\n",
      "Iteration 1291, loss = 0.35430494\n",
      "Iteration 1292, loss = 0.35399350\n",
      "Iteration 1293, loss = 0.35374242\n",
      "Iteration 1294, loss = 0.35414179\n",
      "Iteration 1295, loss = 0.35397801\n",
      "Iteration 1296, loss = 0.35355798\n",
      "Iteration 1297, loss = 0.35397622\n",
      "Iteration 1298, loss = 0.35363563\n",
      "Iteration 1299, loss = 0.35327738\n",
      "Iteration 1300, loss = 0.35327109\n",
      "Iteration 1301, loss = 0.35355016\n",
      "Iteration 1302, loss = 0.35327555\n",
      "Iteration 1303, loss = 0.35360605\n",
      "Iteration 1304, loss = 0.35374614\n",
      "Iteration 1305, loss = 0.35328696\n",
      "Iteration 1306, loss = 0.35373090\n",
      "Iteration 1307, loss = 0.35330647\n",
      "Iteration 1308, loss = 0.35319401\n",
      "Iteration 1309, loss = 0.35275815\n",
      "Iteration 1310, loss = 0.35307214\n",
      "Iteration 1311, loss = 0.35316185\n",
      "Iteration 1312, loss = 0.35285154\n",
      "Iteration 1313, loss = 0.35275893\n",
      "Iteration 1314, loss = 0.35294452\n",
      "Iteration 1315, loss = 0.35260475\n",
      "Iteration 1316, loss = 0.35294435\n",
      "Iteration 1317, loss = 0.35291029\n",
      "Iteration 1318, loss = 0.35303736\n",
      "Iteration 1319, loss = 0.35304327\n",
      "Iteration 1320, loss = 0.35253237\n",
      "Iteration 1321, loss = 0.35290997\n",
      "Iteration 1322, loss = 0.35247387\n",
      "Iteration 1323, loss = 0.35235476\n",
      "Iteration 1324, loss = 0.35229282\n",
      "Iteration 1325, loss = 0.35233891\n",
      "Iteration 1326, loss = 0.35271124\n",
      "Iteration 1327, loss = 0.35231505\n",
      "Iteration 1328, loss = 0.35225557\n",
      "Iteration 1329, loss = 0.35236610\n",
      "Iteration 1330, loss = 0.35238808\n",
      "Iteration 1331, loss = 0.35207129\n",
      "Iteration 1332, loss = 0.35244462\n",
      "Iteration 1333, loss = 0.35211892\n",
      "Iteration 1334, loss = 0.35249971\n",
      "Iteration 1335, loss = 0.35221917\n",
      "Iteration 1336, loss = 0.35207334\n",
      "Iteration 1337, loss = 0.35189289\n",
      "Iteration 1338, loss = 0.35181287\n",
      "Iteration 1339, loss = 0.35221245\n",
      "Iteration 1340, loss = 0.35209155\n",
      "Iteration 1341, loss = 0.35171419\n",
      "Iteration 1342, loss = 0.35189117\n",
      "Iteration 1343, loss = 0.35147355\n",
      "Iteration 1344, loss = 0.35191007\n",
      "Iteration 1345, loss = 0.35164244\n",
      "Iteration 1346, loss = 0.35197740\n",
      "Iteration 1347, loss = 0.35151987\n",
      "Iteration 1348, loss = 0.35137133\n",
      "Iteration 1349, loss = 0.35149603\n",
      "Iteration 1350, loss = 0.35182434\n",
      "Iteration 1351, loss = 0.35136750\n",
      "Iteration 1352, loss = 0.35149202\n",
      "Iteration 1353, loss = 0.35107437\n",
      "Iteration 1354, loss = 0.35104026\n",
      "Iteration 1355, loss = 0.35107138\n",
      "Iteration 1356, loss = 0.35094210\n",
      "Iteration 1357, loss = 0.35089635\n",
      "Iteration 1358, loss = 0.35128831\n",
      "Iteration 1359, loss = 0.35086168\n",
      "Iteration 1360, loss = 0.35074468\n",
      "Iteration 1361, loss = 0.35076582\n",
      "Iteration 1362, loss = 0.35130942\n",
      "Iteration 1363, loss = 0.35091009\n",
      "Iteration 1364, loss = 0.35082228\n",
      "Iteration 1365, loss = 0.35096471\n",
      "Iteration 1366, loss = 0.35060923\n",
      "Iteration 1367, loss = 0.35088596\n",
      "Iteration 1368, loss = 0.35100230\n",
      "Iteration 1369, loss = 0.35067478\n",
      "Iteration 1370, loss = 0.35012372\n",
      "Iteration 1371, loss = 0.35019549\n",
      "Iteration 1372, loss = 0.35057962\n",
      "Iteration 1373, loss = 0.35029259\n",
      "Iteration 1374, loss = 0.35040296\n",
      "Iteration 1375, loss = 0.35036981\n",
      "Iteration 1376, loss = 0.35031933\n",
      "Iteration 1377, loss = 0.35023818\n",
      "Iteration 1378, loss = 0.35011401\n",
      "Iteration 1379, loss = 0.34991329\n",
      "Iteration 1380, loss = 0.35027356\n",
      "Iteration 1381, loss = 0.34995999\n",
      "Iteration 1382, loss = 0.35053794\n",
      "Iteration 1383, loss = 0.34984220\n",
      "Iteration 1384, loss = 0.35020531\n",
      "Iteration 1385, loss = 0.35009771\n",
      "Iteration 1386, loss = 0.35002059\n",
      "Iteration 1387, loss = 0.34994449\n",
      "Iteration 1388, loss = 0.35022691\n",
      "Iteration 1389, loss = 0.35016595\n",
      "Iteration 1390, loss = 0.34971030\n",
      "Iteration 1391, loss = 0.34985736\n",
      "Iteration 1392, loss = 0.34990888\n",
      "Iteration 1393, loss = 0.34966262\n",
      "Iteration 1394, loss = 0.35004130\n",
      "Iteration 1395, loss = 0.34957229\n",
      "Iteration 1396, loss = 0.34959838\n",
      "Iteration 1397, loss = 0.34928905\n",
      "Iteration 1398, loss = 0.34953272\n",
      "Iteration 1399, loss = 0.34962212\n",
      "Iteration 1400, loss = 0.34923008\n",
      "Iteration 1401, loss = 0.34973471\n",
      "Iteration 1402, loss = 0.34965092\n",
      "Iteration 1403, loss = 0.34943239\n",
      "Iteration 1404, loss = 0.34889434\n",
      "Iteration 1405, loss = 0.34948572\n",
      "Iteration 1406, loss = 0.34918149\n",
      "Iteration 1407, loss = 0.34922211\n",
      "Iteration 1408, loss = 0.34926186\n",
      "Iteration 1409, loss = 0.34887125\n",
      "Iteration 1410, loss = 0.34912366\n",
      "Iteration 1411, loss = 0.34888645\n",
      "Iteration 1412, loss = 0.34924053\n",
      "Iteration 1413, loss = 0.34898417\n",
      "Iteration 1414, loss = 0.34917737\n",
      "Iteration 1415, loss = 0.34840970\n",
      "Iteration 1416, loss = 0.34884531\n",
      "Iteration 1417, loss = 0.34866859\n",
      "Iteration 1418, loss = 0.34864237\n",
      "Iteration 1419, loss = 0.34871370\n",
      "Iteration 1420, loss = 0.34847408\n",
      "Iteration 1421, loss = 0.34858600\n",
      "Iteration 1422, loss = 0.34849887\n",
      "Iteration 1423, loss = 0.34812366\n",
      "Iteration 1424, loss = 0.34848954\n",
      "Iteration 1425, loss = 0.34842004\n",
      "Iteration 1426, loss = 0.34882809\n",
      "Iteration 1427, loss = 0.34815092\n",
      "Iteration 1428, loss = 0.34894392\n",
      "Iteration 1429, loss = 0.34819573\n",
      "Iteration 1430, loss = 0.34852060\n",
      "Iteration 1431, loss = 0.34809447\n",
      "Iteration 1432, loss = 0.34848540\n",
      "Iteration 1433, loss = 0.34822009\n",
      "Iteration 1434, loss = 0.34787148\n",
      "Iteration 1435, loss = 0.34864936\n",
      "Iteration 1436, loss = 0.34813311\n",
      "Iteration 1437, loss = 0.34806874\n",
      "Iteration 1438, loss = 0.34813695\n",
      "Iteration 1439, loss = 0.34840438\n",
      "Iteration 1440, loss = 0.34793596\n",
      "Iteration 1441, loss = 0.34821595\n",
      "Iteration 1442, loss = 0.34790854\n",
      "Iteration 1443, loss = 0.34786697\n",
      "Iteration 1444, loss = 0.34765871\n",
      "Iteration 1445, loss = 0.34787115\n",
      "Iteration 1446, loss = 0.34757640\n",
      "Iteration 1447, loss = 0.34817560\n",
      "Iteration 1448, loss = 0.34809377\n",
      "Iteration 1449, loss = 0.34746589\n",
      "Iteration 1450, loss = 0.34766341\n",
      "Iteration 1451, loss = 0.34757530\n",
      "Iteration 1452, loss = 0.34728531\n",
      "Iteration 1453, loss = 0.34754984\n",
      "Iteration 1454, loss = 0.34732352\n",
      "Iteration 1455, loss = 0.34730764\n",
      "Iteration 1456, loss = 0.34730315\n",
      "Iteration 1457, loss = 0.34720605\n",
      "Iteration 1458, loss = 0.34712013\n",
      "Iteration 1459, loss = 0.34746583\n",
      "Iteration 1460, loss = 0.34750750\n",
      "Iteration 1461, loss = 0.34714942\n",
      "Iteration 1462, loss = 0.34733333\n",
      "Iteration 1463, loss = 0.34714920\n",
      "Iteration 1464, loss = 0.34754601\n",
      "Iteration 1465, loss = 0.34696025\n",
      "Iteration 1466, loss = 0.34695701\n",
      "Iteration 1467, loss = 0.34715330\n",
      "Iteration 1468, loss = 0.34647400\n",
      "Iteration 1469, loss = 0.34693912\n",
      "Iteration 1470, loss = 0.34670614\n",
      "Iteration 1471, loss = 0.34651860\n",
      "Iteration 1472, loss = 0.34658952\n",
      "Iteration 1473, loss = 0.34673728\n",
      "Iteration 1474, loss = 0.34689438\n",
      "Iteration 1475, loss = 0.34701773\n",
      "Iteration 1476, loss = 0.34652305\n",
      "Iteration 1477, loss = 0.34705419\n",
      "Iteration 1478, loss = 0.34696031\n",
      "Iteration 1479, loss = 0.34646109\n",
      "Iteration 1480, loss = 0.34669408\n",
      "Iteration 1481, loss = 0.34680156\n",
      "Iteration 1482, loss = 0.34666311\n",
      "Iteration 1483, loss = 0.34630887\n",
      "Iteration 1484, loss = 0.34674775\n",
      "Iteration 1485, loss = 0.34674744\n",
      "Iteration 1486, loss = 0.34654549\n",
      "Iteration 1487, loss = 0.34620317\n",
      "Iteration 1488, loss = 0.34605260\n",
      "Iteration 1489, loss = 0.34673166\n",
      "Iteration 1490, loss = 0.34660096\n",
      "Iteration 1491, loss = 0.34648182\n",
      "Iteration 1492, loss = 0.34640734\n",
      "Iteration 1493, loss = 0.34628387\n",
      "Iteration 1494, loss = 0.34668215\n",
      "Iteration 1495, loss = 0.34637592\n",
      "Iteration 1496, loss = 0.34616580\n",
      "Iteration 1497, loss = 0.34627480\n",
      "Iteration 1498, loss = 0.34596551\n",
      "Iteration 1499, loss = 0.34628873\n",
      "Iteration 1500, loss = 0.34609802\n",
      "Iteration 1501, loss = 0.34578198\n",
      "Iteration 1502, loss = 0.34600413\n",
      "Iteration 1503, loss = 0.34615895\n",
      "Iteration 1504, loss = 0.34625258\n",
      "Iteration 1505, loss = 0.34578722\n",
      "Iteration 1506, loss = 0.34601561\n",
      "Iteration 1507, loss = 0.34616908\n",
      "Iteration 1508, loss = 0.34552627\n",
      "Iteration 1509, loss = 0.34585583\n",
      "Iteration 1510, loss = 0.34598190\n",
      "Iteration 1511, loss = 0.34578552\n",
      "Iteration 1512, loss = 0.34526423\n",
      "Iteration 1513, loss = 0.34528498\n",
      "Iteration 1514, loss = 0.34558711\n",
      "Iteration 1515, loss = 0.34582664\n",
      "Iteration 1516, loss = 0.34556653\n",
      "Iteration 1517, loss = 0.34580036\n",
      "Iteration 1518, loss = 0.34553731\n",
      "Iteration 1519, loss = 0.34526957\n",
      "Iteration 1520, loss = 0.34543631\n",
      "Iteration 1521, loss = 0.34555664\n",
      "Iteration 1522, loss = 0.34563129\n",
      "Iteration 1523, loss = 0.34525756\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67360753\n",
      "Iteration 2, loss = 0.65406693\n",
      "Iteration 3, loss = 0.65376302\n",
      "Iteration 4, loss = 0.65301670\n",
      "Iteration 5, loss = 0.65242116\n",
      "Iteration 6, loss = 0.65197465\n",
      "Iteration 7, loss = 0.65168457\n",
      "Iteration 8, loss = 0.65100167\n",
      "Iteration 9, loss = 0.65052341\n",
      "Iteration 10, loss = 0.64993105\n",
      "Iteration 11, loss = 0.64956593\n",
      "Iteration 12, loss = 0.64917084\n",
      "Iteration 13, loss = 0.64856048\n",
      "Iteration 14, loss = 0.64779598\n",
      "Iteration 15, loss = 0.64770288\n",
      "Iteration 16, loss = 0.64683664\n",
      "Iteration 17, loss = 0.64637458\n",
      "Iteration 18, loss = 0.64595496\n",
      "Iteration 19, loss = 0.64551077\n",
      "Iteration 20, loss = 0.64580404\n",
      "Iteration 21, loss = 0.64498652\n",
      "Iteration 22, loss = 0.64444128\n",
      "Iteration 23, loss = 0.64405300\n",
      "Iteration 24, loss = 0.64371590\n",
      "Iteration 25, loss = 0.64354501\n",
      "Iteration 26, loss = 0.64343873\n",
      "Iteration 27, loss = 0.64237923\n",
      "Iteration 28, loss = 0.64244580\n",
      "Iteration 29, loss = 0.64202664\n",
      "Iteration 30, loss = 0.64136770\n",
      "Iteration 31, loss = 0.64135496\n",
      "Iteration 32, loss = 0.64117608\n",
      "Iteration 33, loss = 0.64093526\n",
      "Iteration 34, loss = 0.64064390\n",
      "Iteration 35, loss = 0.64034468\n",
      "Iteration 36, loss = 0.63976143\n",
      "Iteration 37, loss = 0.63954107\n",
      "Iteration 38, loss = 0.63895811\n",
      "Iteration 39, loss = 0.63880341\n",
      "Iteration 40, loss = 0.63853365\n",
      "Iteration 41, loss = 0.63808233\n",
      "Iteration 42, loss = 0.63794560\n",
      "Iteration 43, loss = 0.63727083\n",
      "Iteration 44, loss = 0.63754944\n",
      "Iteration 45, loss = 0.63682427\n",
      "Iteration 46, loss = 0.63639209\n",
      "Iteration 47, loss = 0.63603692\n",
      "Iteration 48, loss = 0.63555516\n",
      "Iteration 49, loss = 0.63519095\n",
      "Iteration 50, loss = 0.63502136\n",
      "Iteration 51, loss = 0.63466148\n",
      "Iteration 52, loss = 0.63405836\n",
      "Iteration 53, loss = 0.63425670\n",
      "Iteration 54, loss = 0.63354461\n",
      "Iteration 55, loss = 0.63307520\n",
      "Iteration 56, loss = 0.63260554\n",
      "Iteration 57, loss = 0.63231325\n",
      "Iteration 58, loss = 0.63202027\n",
      "Iteration 59, loss = 0.63188142\n",
      "Iteration 60, loss = 0.63132923\n",
      "Iteration 61, loss = 0.63156832\n",
      "Iteration 62, loss = 0.63066031\n",
      "Iteration 63, loss = 0.63055223\n",
      "Iteration 64, loss = 0.63010119\n",
      "Iteration 65, loss = 0.62985215\n",
      "Iteration 66, loss = 0.62930809\n",
      "Iteration 67, loss = 0.62933232\n",
      "Iteration 68, loss = 0.62845449\n",
      "Iteration 69, loss = 0.62795681\n",
      "Iteration 70, loss = 0.62785048\n",
      "Iteration 71, loss = 0.62781763\n",
      "Iteration 72, loss = 0.62713272\n",
      "Iteration 73, loss = 0.62688764\n",
      "Iteration 74, loss = 0.62702617\n",
      "Iteration 75, loss = 0.62621366\n",
      "Iteration 76, loss = 0.62615310\n",
      "Iteration 77, loss = 0.62560801\n",
      "Iteration 78, loss = 0.62525191\n",
      "Iteration 79, loss = 0.62541966\n",
      "Iteration 80, loss = 0.62468474\n",
      "Iteration 81, loss = 0.62472161\n",
      "Iteration 82, loss = 0.62363928\n",
      "Iteration 83, loss = 0.62427712\n",
      "Iteration 84, loss = 0.62337988\n",
      "Iteration 85, loss = 0.62312694\n",
      "Iteration 86, loss = 0.62272445\n",
      "Iteration 87, loss = 0.62243957\n",
      "Iteration 88, loss = 0.62227074\n",
      "Iteration 89, loss = 0.62208228\n",
      "Iteration 90, loss = 0.62177246\n",
      "Iteration 91, loss = 0.62146825\n",
      "Iteration 92, loss = 0.62144230\n",
      "Iteration 93, loss = 0.62122861\n",
      "Iteration 94, loss = 0.62086154\n",
      "Iteration 95, loss = 0.62039311\n",
      "Iteration 96, loss = 0.62015971\n",
      "Iteration 97, loss = 0.61983193\n",
      "Iteration 98, loss = 0.61963223\n",
      "Iteration 99, loss = 0.61938016\n",
      "Iteration 100, loss = 0.61916438\n",
      "Iteration 101, loss = 0.61862607\n",
      "Iteration 102, loss = 0.61886172\n",
      "Iteration 103, loss = 0.61846360\n",
      "Iteration 104, loss = 0.61796352\n",
      "Iteration 105, loss = 0.61851566\n",
      "Iteration 106, loss = 0.61760746\n",
      "Iteration 107, loss = 0.61741643\n",
      "Iteration 108, loss = 0.61730683\n",
      "Iteration 109, loss = 0.61639178\n",
      "Iteration 110, loss = 0.61633607\n",
      "Iteration 111, loss = 0.61674165\n",
      "Iteration 112, loss = 0.61695925\n",
      "Iteration 113, loss = 0.61568243\n",
      "Iteration 114, loss = 0.61566177\n",
      "Iteration 115, loss = 0.61568961\n",
      "Iteration 116, loss = 0.61530887\n",
      "Iteration 117, loss = 0.61557675\n",
      "Iteration 118, loss = 0.61464038\n",
      "Iteration 119, loss = 0.61529492\n",
      "Iteration 120, loss = 0.61414139\n",
      "Iteration 121, loss = 0.61401670\n",
      "Iteration 122, loss = 0.61415491\n",
      "Iteration 123, loss = 0.61395726\n",
      "Iteration 124, loss = 0.61382587\n",
      "Iteration 125, loss = 0.61381009\n",
      "Iteration 126, loss = 0.61348407\n",
      "Iteration 127, loss = 0.61375823\n",
      "Iteration 128, loss = 0.61329096\n",
      "Iteration 129, loss = 0.61254972\n",
      "Iteration 130, loss = 0.61234232\n",
      "Iteration 131, loss = 0.61209771\n",
      "Iteration 132, loss = 0.61178796\n",
      "Iteration 133, loss = 0.61150977\n",
      "Iteration 134, loss = 0.61155628\n",
      "Iteration 135, loss = 0.61156570\n",
      "Iteration 136, loss = 0.61121934\n",
      "Iteration 137, loss = 0.61055072\n",
      "Iteration 138, loss = 0.61108741\n",
      "Iteration 139, loss = 0.61052368\n",
      "Iteration 140, loss = 0.61028096\n",
      "Iteration 141, loss = 0.61051196\n",
      "Iteration 142, loss = 0.61005018\n",
      "Iteration 143, loss = 0.60962113\n",
      "Iteration 144, loss = 0.60964683\n",
      "Iteration 145, loss = 0.60963508\n",
      "Iteration 146, loss = 0.60975397\n",
      "Iteration 147, loss = 0.60874290\n",
      "Iteration 148, loss = 0.60895302\n",
      "Iteration 149, loss = 0.60871552\n",
      "Iteration 150, loss = 0.60878447\n",
      "Iteration 151, loss = 0.60838134\n",
      "Iteration 152, loss = 0.60803900\n",
      "Iteration 153, loss = 0.60801932\n",
      "Iteration 154, loss = 0.60744944\n",
      "Iteration 155, loss = 0.60838456\n",
      "Iteration 156, loss = 0.60708796\n",
      "Iteration 157, loss = 0.60721165\n",
      "Iteration 158, loss = 0.60727044\n",
      "Iteration 159, loss = 0.60708009\n",
      "Iteration 160, loss = 0.60661680\n",
      "Iteration 161, loss = 0.60742003\n",
      "Iteration 162, loss = 0.60639855\n",
      "Iteration 163, loss = 0.60633253\n",
      "Iteration 164, loss = 0.60621956\n",
      "Iteration 165, loss = 0.60614248\n",
      "Iteration 166, loss = 0.60606755\n",
      "Iteration 167, loss = 0.60541436\n",
      "Iteration 168, loss = 0.60535555\n",
      "Iteration 169, loss = 0.60537126\n",
      "Iteration 170, loss = 0.60514864\n",
      "Iteration 171, loss = 0.60539925\n",
      "Iteration 172, loss = 0.60445941\n",
      "Iteration 173, loss = 0.60394797\n",
      "Iteration 174, loss = 0.60433301\n",
      "Iteration 175, loss = 0.60360047\n",
      "Iteration 176, loss = 0.60418298\n",
      "Iteration 177, loss = 0.60330648\n",
      "Iteration 178, loss = 0.60353189\n",
      "Iteration 179, loss = 0.60280802\n",
      "Iteration 180, loss = 0.60287002\n",
      "Iteration 181, loss = 0.60256679\n",
      "Iteration 182, loss = 0.60277803\n",
      "Iteration 183, loss = 0.60212599\n",
      "Iteration 184, loss = 0.60248632\n",
      "Iteration 185, loss = 0.60214311\n",
      "Iteration 186, loss = 0.60215803\n",
      "Iteration 187, loss = 0.60188341\n",
      "Iteration 188, loss = 0.60136101\n",
      "Iteration 189, loss = 0.60141548\n",
      "Iteration 190, loss = 0.60144290\n",
      "Iteration 191, loss = 0.60178751\n",
      "Iteration 192, loss = 0.60069574\n",
      "Iteration 193, loss = 0.60012565\n",
      "Iteration 194, loss = 0.60072366\n",
      "Iteration 195, loss = 0.60036295\n",
      "Iteration 196, loss = 0.60002307\n",
      "Iteration 197, loss = 0.59975389\n",
      "Iteration 198, loss = 0.59947036\n",
      "Iteration 199, loss = 0.59894458\n",
      "Iteration 200, loss = 0.59894788\n",
      "Iteration 201, loss = 0.59881845\n",
      "Iteration 202, loss = 0.59811136\n",
      "Iteration 203, loss = 0.59825888\n",
      "Iteration 204, loss = 0.59779415\n",
      "Iteration 205, loss = 0.59837825\n",
      "Iteration 206, loss = 0.59755649\n",
      "Iteration 207, loss = 0.59706208\n",
      "Iteration 208, loss = 0.59725239\n",
      "Iteration 209, loss = 0.59735014\n",
      "Iteration 210, loss = 0.59669915\n",
      "Iteration 211, loss = 0.59608993\n",
      "Iteration 212, loss = 0.59584635\n",
      "Iteration 213, loss = 0.59553285\n",
      "Iteration 214, loss = 0.59628941\n",
      "Iteration 215, loss = 0.59562457\n",
      "Iteration 216, loss = 0.59500541\n",
      "Iteration 217, loss = 0.59526907\n",
      "Iteration 218, loss = 0.59472254\n",
      "Iteration 219, loss = 0.59427572\n",
      "Iteration 220, loss = 0.59407193\n",
      "Iteration 221, loss = 0.59412132\n",
      "Iteration 222, loss = 0.59370659\n",
      "Iteration 223, loss = 0.59305511\n",
      "Iteration 224, loss = 0.59286659\n",
      "Iteration 225, loss = 0.59286452\n",
      "Iteration 226, loss = 0.59279044\n",
      "Iteration 227, loss = 0.59284095\n",
      "Iteration 228, loss = 0.59166019\n",
      "Iteration 229, loss = 0.59246884\n",
      "Iteration 230, loss = 0.59201087\n",
      "Iteration 231, loss = 0.59127863\n",
      "Iteration 232, loss = 0.59144026\n",
      "Iteration 233, loss = 0.59093651\n",
      "Iteration 234, loss = 0.59042908\n",
      "Iteration 235, loss = 0.59087215\n",
      "Iteration 236, loss = 0.59020527\n",
      "Iteration 237, loss = 0.59003388\n",
      "Iteration 238, loss = 0.58979681\n",
      "Iteration 239, loss = 0.58904637\n",
      "Iteration 240, loss = 0.58903882\n",
      "Iteration 241, loss = 0.58901037\n",
      "Iteration 242, loss = 0.58830960\n",
      "Iteration 243, loss = 0.58793982\n",
      "Iteration 244, loss = 0.58776118\n",
      "Iteration 245, loss = 0.58769787\n",
      "Iteration 246, loss = 0.58735379\n",
      "Iteration 247, loss = 0.58673186\n",
      "Iteration 248, loss = 0.58657364\n",
      "Iteration 249, loss = 0.58629316\n",
      "Iteration 250, loss = 0.58610194\n",
      "Iteration 251, loss = 0.58602140\n",
      "Iteration 252, loss = 0.58526157\n",
      "Iteration 253, loss = 0.58522559\n",
      "Iteration 254, loss = 0.58516121\n",
      "Iteration 255, loss = 0.58476691\n",
      "Iteration 256, loss = 0.58423684\n",
      "Iteration 257, loss = 0.58409193\n",
      "Iteration 258, loss = 0.58418241\n",
      "Iteration 259, loss = 0.58320563\n",
      "Iteration 260, loss = 0.58322482\n",
      "Iteration 261, loss = 0.58234208\n",
      "Iteration 262, loss = 0.58234672\n",
      "Iteration 263, loss = 0.58196843\n",
      "Iteration 264, loss = 0.58196913\n",
      "Iteration 265, loss = 0.58132708\n",
      "Iteration 266, loss = 0.58098195\n",
      "Iteration 267, loss = 0.58089950\n",
      "Iteration 268, loss = 0.58054647\n",
      "Iteration 269, loss = 0.58029219\n",
      "Iteration 270, loss = 0.57967284\n",
      "Iteration 271, loss = 0.57945917\n",
      "Iteration 272, loss = 0.57956329\n",
      "Iteration 273, loss = 0.57835353\n",
      "Iteration 274, loss = 0.57880909\n",
      "Iteration 275, loss = 0.57810384\n",
      "Iteration 276, loss = 0.57801511\n",
      "Iteration 277, loss = 0.57809124\n",
      "Iteration 278, loss = 0.57749288\n",
      "Iteration 279, loss = 0.57666272\n",
      "Iteration 280, loss = 0.57722347\n",
      "Iteration 281, loss = 0.57616834\n",
      "Iteration 282, loss = 0.57564003\n",
      "Iteration 283, loss = 0.57599130\n",
      "Iteration 284, loss = 0.57472060\n",
      "Iteration 285, loss = 0.57506206\n",
      "Iteration 286, loss = 0.57443246\n",
      "Iteration 287, loss = 0.57437333\n",
      "Iteration 288, loss = 0.57401024\n",
      "Iteration 289, loss = 0.57344150\n",
      "Iteration 290, loss = 0.57291614\n",
      "Iteration 291, loss = 0.57267841\n",
      "Iteration 292, loss = 0.57238286\n",
      "Iteration 293, loss = 0.57201500\n",
      "Iteration 294, loss = 0.57150477\n",
      "Iteration 295, loss = 0.57111137\n",
      "Iteration 296, loss = 0.57055913\n",
      "Iteration 297, loss = 0.57038599\n",
      "Iteration 298, loss = 0.56978209\n",
      "Iteration 299, loss = 0.57004279\n",
      "Iteration 300, loss = 0.56880923\n",
      "Iteration 301, loss = 0.56899256\n",
      "Iteration 302, loss = 0.56817616\n",
      "Iteration 303, loss = 0.56854194\n",
      "Iteration 304, loss = 0.56728937\n",
      "Iteration 305, loss = 0.56689930\n",
      "Iteration 306, loss = 0.56656832\n",
      "Iteration 307, loss = 0.56618003\n",
      "Iteration 308, loss = 0.56557309\n",
      "Iteration 309, loss = 0.56541311\n",
      "Iteration 310, loss = 0.56495092\n",
      "Iteration 311, loss = 0.56468061\n",
      "Iteration 312, loss = 0.56509338\n",
      "Iteration 313, loss = 0.56356757\n",
      "Iteration 314, loss = 0.56329598\n",
      "Iteration 315, loss = 0.56274783\n",
      "Iteration 316, loss = 0.56265735\n",
      "Iteration 317, loss = 0.56197023\n",
      "Iteration 318, loss = 0.56161600\n",
      "Iteration 319, loss = 0.56048689\n",
      "Iteration 320, loss = 0.56038729\n",
      "Iteration 321, loss = 0.55972230\n",
      "Iteration 322, loss = 0.55971528\n",
      "Iteration 323, loss = 0.55907150\n",
      "Iteration 324, loss = 0.55879487\n",
      "Iteration 325, loss = 0.55882241\n",
      "Iteration 326, loss = 0.55778303\n",
      "Iteration 327, loss = 0.55752570\n",
      "Iteration 328, loss = 0.55707599\n",
      "Iteration 329, loss = 0.55644171\n",
      "Iteration 330, loss = 0.55639718\n",
      "Iteration 331, loss = 0.55556882\n",
      "Iteration 332, loss = 0.55489956\n",
      "Iteration 333, loss = 0.55477501\n",
      "Iteration 334, loss = 0.55433308\n",
      "Iteration 335, loss = 0.55358339\n",
      "Iteration 336, loss = 0.55337338\n",
      "Iteration 337, loss = 0.55260379\n",
      "Iteration 338, loss = 0.55247411\n",
      "Iteration 339, loss = 0.55193152\n",
      "Iteration 340, loss = 0.55186415\n",
      "Iteration 341, loss = 0.55115485\n",
      "Iteration 342, loss = 0.55033016\n",
      "Iteration 343, loss = 0.55002133\n",
      "Iteration 344, loss = 0.54955932\n",
      "Iteration 345, loss = 0.54944809\n",
      "Iteration 346, loss = 0.54889199\n",
      "Iteration 347, loss = 0.54845675\n",
      "Iteration 348, loss = 0.54764584\n",
      "Iteration 349, loss = 0.54762138\n",
      "Iteration 350, loss = 0.54701150\n",
      "Iteration 351, loss = 0.54642588\n",
      "Iteration 352, loss = 0.54590208\n",
      "Iteration 353, loss = 0.54555847\n",
      "Iteration 354, loss = 0.54511568\n",
      "Iteration 355, loss = 0.54436184\n",
      "Iteration 356, loss = 0.54412355\n",
      "Iteration 357, loss = 0.54347267\n",
      "Iteration 358, loss = 0.54341858\n",
      "Iteration 359, loss = 0.54310014\n",
      "Iteration 360, loss = 0.54222505\n",
      "Iteration 361, loss = 0.54163253\n",
      "Iteration 362, loss = 0.54130624\n",
      "Iteration 363, loss = 0.54073710\n",
      "Iteration 364, loss = 0.54017871\n",
      "Iteration 365, loss = 0.53994443\n",
      "Iteration 366, loss = 0.53914568\n",
      "Iteration 367, loss = 0.53935281\n",
      "Iteration 368, loss = 0.53873765\n",
      "Iteration 369, loss = 0.53824968\n",
      "Iteration 370, loss = 0.53768946\n",
      "Iteration 371, loss = 0.53678144\n",
      "Iteration 372, loss = 0.53662487\n",
      "Iteration 373, loss = 0.53611430\n",
      "Iteration 374, loss = 0.53643306\n",
      "Iteration 375, loss = 0.53601140\n",
      "Iteration 376, loss = 0.53488769\n",
      "Iteration 377, loss = 0.53422310\n",
      "Iteration 378, loss = 0.53406198\n",
      "Iteration 379, loss = 0.53352968\n",
      "Iteration 380, loss = 0.53271340\n",
      "Iteration 381, loss = 0.53302832\n",
      "Iteration 382, loss = 0.53242848\n",
      "Iteration 383, loss = 0.53086666\n",
      "Iteration 384, loss = 0.53105383\n",
      "Iteration 385, loss = 0.53089542\n",
      "Iteration 386, loss = 0.52984282\n",
      "Iteration 387, loss = 0.52965965\n",
      "Iteration 388, loss = 0.52896679\n",
      "Iteration 389, loss = 0.52891338\n",
      "Iteration 390, loss = 0.52820715\n",
      "Iteration 391, loss = 0.52772248\n",
      "Iteration 392, loss = 0.52753675\n",
      "Iteration 393, loss = 0.52673528\n",
      "Iteration 394, loss = 0.52610637\n",
      "Iteration 395, loss = 0.52590216\n",
      "Iteration 396, loss = 0.52519944\n",
      "Iteration 397, loss = 0.52449200\n",
      "Iteration 398, loss = 0.52429317\n",
      "Iteration 399, loss = 0.52417438\n",
      "Iteration 400, loss = 0.52374086\n",
      "Iteration 401, loss = 0.52355133\n",
      "Iteration 402, loss = 0.52284086\n",
      "Iteration 403, loss = 0.52198864\n",
      "Iteration 404, loss = 0.52218569\n",
      "Iteration 405, loss = 0.52121266\n",
      "Iteration 406, loss = 0.52063748\n",
      "Iteration 407, loss = 0.52049125\n",
      "Iteration 408, loss = 0.52007198\n",
      "Iteration 409, loss = 0.51909356\n",
      "Iteration 410, loss = 0.51896210\n",
      "Iteration 411, loss = 0.51823987\n",
      "Iteration 412, loss = 0.51828341\n",
      "Iteration 413, loss = 0.51734671\n",
      "Iteration 414, loss = 0.51653326\n",
      "Iteration 415, loss = 0.51621556\n",
      "Iteration 416, loss = 0.51593317\n",
      "Iteration 417, loss = 0.51560493\n",
      "Iteration 418, loss = 0.51522159\n",
      "Iteration 419, loss = 0.51440128\n",
      "Iteration 420, loss = 0.51427905\n",
      "Iteration 421, loss = 0.51315193\n",
      "Iteration 422, loss = 0.51360796\n",
      "Iteration 423, loss = 0.51255797\n",
      "Iteration 424, loss = 0.51212619\n",
      "Iteration 425, loss = 0.51160564\n",
      "Iteration 426, loss = 0.51113364\n",
      "Iteration 427, loss = 0.51075495\n",
      "Iteration 428, loss = 0.51021783\n",
      "Iteration 429, loss = 0.50970455\n",
      "Iteration 430, loss = 0.50991378\n",
      "Iteration 431, loss = 0.50909483\n",
      "Iteration 432, loss = 0.50845452\n",
      "Iteration 433, loss = 0.50770955\n",
      "Iteration 434, loss = 0.50773925\n",
      "Iteration 435, loss = 0.50715650\n",
      "Iteration 436, loss = 0.50665471\n",
      "Iteration 437, loss = 0.50640779\n",
      "Iteration 438, loss = 0.50554314\n",
      "Iteration 439, loss = 0.50483080\n",
      "Iteration 440, loss = 0.50452616\n",
      "Iteration 441, loss = 0.50471741\n",
      "Iteration 442, loss = 0.50415653\n",
      "Iteration 443, loss = 0.50309708\n",
      "Iteration 444, loss = 0.50259902\n",
      "Iteration 445, loss = 0.50228032\n",
      "Iteration 446, loss = 0.50206617\n",
      "Iteration 447, loss = 0.50113731\n",
      "Iteration 448, loss = 0.50097789\n",
      "Iteration 449, loss = 0.50032812\n",
      "Iteration 450, loss = 0.50048431\n",
      "Iteration 451, loss = 0.49951841\n",
      "Iteration 452, loss = 0.49897123\n",
      "Iteration 453, loss = 0.49838028\n",
      "Iteration 454, loss = 0.49853169\n",
      "Iteration 455, loss = 0.49751947\n",
      "Iteration 456, loss = 0.49708804\n",
      "Iteration 457, loss = 0.49624265\n",
      "Iteration 458, loss = 0.49650264\n",
      "Iteration 459, loss = 0.49541317\n",
      "Iteration 460, loss = 0.49538250\n",
      "Iteration 461, loss = 0.49488908\n",
      "Iteration 462, loss = 0.49458569\n",
      "Iteration 463, loss = 0.49390974\n",
      "Iteration 464, loss = 0.49369638\n",
      "Iteration 465, loss = 0.49284737\n",
      "Iteration 466, loss = 0.49229659\n",
      "Iteration 467, loss = 0.49208717\n",
      "Iteration 468, loss = 0.49232009\n",
      "Iteration 469, loss = 0.49097691\n",
      "Iteration 470, loss = 0.49076026\n",
      "Iteration 471, loss = 0.49032397\n",
      "Iteration 472, loss = 0.48969988\n",
      "Iteration 473, loss = 0.48923089\n",
      "Iteration 474, loss = 0.48910418\n",
      "Iteration 475, loss = 0.48835904\n",
      "Iteration 476, loss = 0.48810201\n",
      "Iteration 477, loss = 0.48744399\n",
      "Iteration 478, loss = 0.48712718\n",
      "Iteration 479, loss = 0.48691229\n",
      "Iteration 480, loss = 0.48615595\n",
      "Iteration 481, loss = 0.48600849\n",
      "Iteration 482, loss = 0.48524319\n",
      "Iteration 483, loss = 0.48477956\n",
      "Iteration 484, loss = 0.48461179\n",
      "Iteration 485, loss = 0.48401203\n",
      "Iteration 486, loss = 0.48352236\n",
      "Iteration 487, loss = 0.48322925\n",
      "Iteration 488, loss = 0.48290484\n",
      "Iteration 489, loss = 0.48214236\n",
      "Iteration 490, loss = 0.48153897\n",
      "Iteration 491, loss = 0.48180398\n",
      "Iteration 492, loss = 0.48076288\n",
      "Iteration 493, loss = 0.48090322\n",
      "Iteration 494, loss = 0.48018001\n",
      "Iteration 495, loss = 0.47967779\n",
      "Iteration 496, loss = 0.47922339\n",
      "Iteration 497, loss = 0.47900751\n",
      "Iteration 498, loss = 0.47825239\n",
      "Iteration 499, loss = 0.47795515\n",
      "Iteration 500, loss = 0.47759365\n",
      "Iteration 501, loss = 0.47701002\n",
      "Iteration 502, loss = 0.47671472\n",
      "Iteration 503, loss = 0.47661313\n",
      "Iteration 504, loss = 0.47595266\n",
      "Iteration 505, loss = 0.47554219\n",
      "Iteration 506, loss = 0.47528757\n",
      "Iteration 507, loss = 0.47444539\n",
      "Iteration 508, loss = 0.47432351\n",
      "Iteration 509, loss = 0.47390341\n",
      "Iteration 510, loss = 0.47375245\n",
      "Iteration 511, loss = 0.47294170\n",
      "Iteration 512, loss = 0.47210083\n",
      "Iteration 513, loss = 0.47202517\n",
      "Iteration 514, loss = 0.47186291\n",
      "Iteration 515, loss = 0.47161083\n",
      "Iteration 516, loss = 0.47116101\n",
      "Iteration 517, loss = 0.47032613\n",
      "Iteration 518, loss = 0.47011433\n",
      "Iteration 519, loss = 0.46971454\n",
      "Iteration 520, loss = 0.46942652\n",
      "Iteration 521, loss = 0.46893084\n",
      "Iteration 522, loss = 0.46874484\n",
      "Iteration 523, loss = 0.46848752\n",
      "Iteration 524, loss = 0.46767568\n",
      "Iteration 525, loss = 0.46724300\n",
      "Iteration 526, loss = 0.46642989\n",
      "Iteration 527, loss = 0.46695037\n",
      "Iteration 528, loss = 0.46634277\n",
      "Iteration 529, loss = 0.46601287\n",
      "Iteration 530, loss = 0.46519138\n",
      "Iteration 531, loss = 0.46488649\n",
      "Iteration 532, loss = 0.46498537\n",
      "Iteration 533, loss = 0.46402219\n",
      "Iteration 534, loss = 0.46317182\n",
      "Iteration 535, loss = 0.46329381\n",
      "Iteration 536, loss = 0.46283140\n",
      "Iteration 537, loss = 0.46275356\n",
      "Iteration 538, loss = 0.46244166\n",
      "Iteration 539, loss = 0.46194437\n",
      "Iteration 540, loss = 0.46152634\n",
      "Iteration 541, loss = 0.46113447\n",
      "Iteration 542, loss = 0.46067979\n",
      "Iteration 543, loss = 0.46019568\n",
      "Iteration 544, loss = 0.46036084\n",
      "Iteration 545, loss = 0.45992699\n",
      "Iteration 546, loss = 0.45895095\n",
      "Iteration 547, loss = 0.45886738\n",
      "Iteration 548, loss = 0.45873332\n",
      "Iteration 549, loss = 0.45778514\n",
      "Iteration 550, loss = 0.45761289\n",
      "Iteration 551, loss = 0.45746408\n",
      "Iteration 552, loss = 0.45672289\n",
      "Iteration 553, loss = 0.45668771\n",
      "Iteration 554, loss = 0.45627878\n",
      "Iteration 555, loss = 0.45578154\n",
      "Iteration 556, loss = 0.45547738\n",
      "Iteration 557, loss = 0.45525183\n",
      "Iteration 558, loss = 0.45468292\n",
      "Iteration 559, loss = 0.45388264\n",
      "Iteration 560, loss = 0.45365308\n",
      "Iteration 561, loss = 0.45406707\n",
      "Iteration 562, loss = 0.45337290\n",
      "Iteration 563, loss = 0.45359205\n",
      "Iteration 564, loss = 0.45252831\n",
      "Iteration 565, loss = 0.45241934\n",
      "Iteration 566, loss = 0.45188391\n",
      "Iteration 567, loss = 0.45128855\n",
      "Iteration 568, loss = 0.45159346\n",
      "Iteration 569, loss = 0.45080037\n",
      "Iteration 570, loss = 0.45082444\n",
      "Iteration 571, loss = 0.45008005\n",
      "Iteration 572, loss = 0.44981848\n",
      "Iteration 573, loss = 0.44927768\n",
      "Iteration 574, loss = 0.44905755\n",
      "Iteration 575, loss = 0.44925976\n",
      "Iteration 576, loss = 0.44835948\n",
      "Iteration 577, loss = 0.44804147\n",
      "Iteration 578, loss = 0.44808526\n",
      "Iteration 579, loss = 0.44766470\n",
      "Iteration 580, loss = 0.44680068\n",
      "Iteration 581, loss = 0.44720527\n",
      "Iteration 582, loss = 0.44657587\n",
      "Iteration 583, loss = 0.44617672\n",
      "Iteration 584, loss = 0.44587072\n",
      "Iteration 585, loss = 0.44608182\n",
      "Iteration 586, loss = 0.44500919\n",
      "Iteration 587, loss = 0.44470018\n",
      "Iteration 588, loss = 0.44448115\n",
      "Iteration 589, loss = 0.44430667\n",
      "Iteration 590, loss = 0.44409701\n",
      "Iteration 591, loss = 0.44357516\n",
      "Iteration 592, loss = 0.44299726\n",
      "Iteration 593, loss = 0.44294763\n",
      "Iteration 594, loss = 0.44270745\n",
      "Iteration 595, loss = 0.44268288\n",
      "Iteration 596, loss = 0.44153737\n",
      "Iteration 597, loss = 0.44182550\n",
      "Iteration 598, loss = 0.44132832\n",
      "Iteration 599, loss = 0.44105845\n",
      "Iteration 600, loss = 0.44084180\n",
      "Iteration 601, loss = 0.44022203\n",
      "Iteration 602, loss = 0.43978119\n",
      "Iteration 603, loss = 0.43947812\n",
      "Iteration 604, loss = 0.43936272\n",
      "Iteration 605, loss = 0.43950132\n",
      "Iteration 606, loss = 0.43943021\n",
      "Iteration 607, loss = 0.43835332\n",
      "Iteration 608, loss = 0.43844376\n",
      "Iteration 609, loss = 0.43779379\n",
      "Iteration 610, loss = 0.43722437\n",
      "Iteration 611, loss = 0.43739114\n",
      "Iteration 612, loss = 0.43658579\n",
      "Iteration 613, loss = 0.43672146\n",
      "Iteration 614, loss = 0.43590200\n",
      "Iteration 615, loss = 0.43629926\n",
      "Iteration 616, loss = 0.43581982\n",
      "Iteration 617, loss = 0.43569062\n",
      "Iteration 618, loss = 0.43503465\n",
      "Iteration 619, loss = 0.43507352\n",
      "Iteration 620, loss = 0.43480182\n",
      "Iteration 621, loss = 0.43424485\n",
      "Iteration 622, loss = 0.43419627\n",
      "Iteration 623, loss = 0.43358848\n",
      "Iteration 624, loss = 0.43358446\n",
      "Iteration 625, loss = 0.43317189\n",
      "Iteration 626, loss = 0.43313802\n",
      "Iteration 627, loss = 0.43272663\n",
      "Iteration 628, loss = 0.43282113\n",
      "Iteration 629, loss = 0.43198027\n",
      "Iteration 630, loss = 0.43180398\n",
      "Iteration 631, loss = 0.43197182\n",
      "Iteration 632, loss = 0.43129770\n",
      "Iteration 633, loss = 0.43109766\n",
      "Iteration 634, loss = 0.43044667\n",
      "Iteration 635, loss = 0.43015514\n",
      "Iteration 636, loss = 0.42983986\n",
      "Iteration 637, loss = 0.42988742\n",
      "Iteration 638, loss = 0.42954801\n",
      "Iteration 639, loss = 0.42952098\n",
      "Iteration 640, loss = 0.42929423\n",
      "Iteration 641, loss = 0.42859788\n",
      "Iteration 642, loss = 0.42819148\n",
      "Iteration 643, loss = 0.42836882\n",
      "Iteration 644, loss = 0.42783393\n",
      "Iteration 645, loss = 0.42770899\n",
      "Iteration 646, loss = 0.42711416\n",
      "Iteration 647, loss = 0.42702220\n",
      "Iteration 648, loss = 0.42679222\n",
      "Iteration 649, loss = 0.42674096\n",
      "Iteration 650, loss = 0.42648441\n",
      "Iteration 651, loss = 0.42592559\n",
      "Iteration 652, loss = 0.42575597\n",
      "Iteration 653, loss = 0.42585384\n",
      "Iteration 654, loss = 0.42553909\n",
      "Iteration 655, loss = 0.42499100\n",
      "Iteration 656, loss = 0.42480283\n",
      "Iteration 657, loss = 0.42437109\n",
      "Iteration 658, loss = 0.42433745\n",
      "Iteration 659, loss = 0.42389878\n",
      "Iteration 660, loss = 0.42359240\n",
      "Iteration 661, loss = 0.42343587\n",
      "Iteration 662, loss = 0.42382875\n",
      "Iteration 663, loss = 0.42341786\n",
      "Iteration 664, loss = 0.42260596\n",
      "Iteration 665, loss = 0.42265619\n",
      "Iteration 666, loss = 0.42249532\n",
      "Iteration 667, loss = 0.42192938\n",
      "Iteration 668, loss = 0.42198136\n",
      "Iteration 669, loss = 0.42176037\n",
      "Iteration 670, loss = 0.42133527\n",
      "Iteration 671, loss = 0.42089369\n",
      "Iteration 672, loss = 0.42097307\n",
      "Iteration 673, loss = 0.42055974\n",
      "Iteration 674, loss = 0.42007114\n",
      "Iteration 675, loss = 0.42017386\n",
      "Iteration 676, loss = 0.41995002\n",
      "Iteration 677, loss = 0.41983024\n",
      "Iteration 678, loss = 0.41923331\n",
      "Iteration 679, loss = 0.41931985\n",
      "Iteration 680, loss = 0.41914487\n",
      "Iteration 681, loss = 0.41891929\n",
      "Iteration 682, loss = 0.41881487\n",
      "Iteration 683, loss = 0.41821547\n",
      "Iteration 684, loss = 0.41798298\n",
      "Iteration 685, loss = 0.41828022\n",
      "Iteration 686, loss = 0.41749616\n",
      "Iteration 687, loss = 0.41729934\n",
      "Iteration 688, loss = 0.41706459\n",
      "Iteration 689, loss = 0.41685785\n",
      "Iteration 690, loss = 0.41692270\n",
      "Iteration 691, loss = 0.41657458\n",
      "Iteration 692, loss = 0.41633427\n",
      "Iteration 693, loss = 0.41597387\n",
      "Iteration 694, loss = 0.41645797\n",
      "Iteration 695, loss = 0.41564551\n",
      "Iteration 696, loss = 0.41555743\n",
      "Iteration 697, loss = 0.41504224\n",
      "Iteration 698, loss = 0.41461680\n",
      "Iteration 699, loss = 0.41471480\n",
      "Iteration 700, loss = 0.41452561\n",
      "Iteration 701, loss = 0.41455318\n",
      "Iteration 702, loss = 0.41450542\n",
      "Iteration 703, loss = 0.41347394\n",
      "Iteration 704, loss = 0.41386515\n",
      "Iteration 705, loss = 0.41399284\n",
      "Iteration 706, loss = 0.41340909\n",
      "Iteration 707, loss = 0.41316336\n",
      "Iteration 708, loss = 0.41310747\n",
      "Iteration 709, loss = 0.41279164\n",
      "Iteration 710, loss = 0.41231793\n",
      "Iteration 711, loss = 0.41221893\n",
      "Iteration 712, loss = 0.41179253\n",
      "Iteration 713, loss = 0.41220841\n",
      "Iteration 714, loss = 0.41133227\n",
      "Iteration 715, loss = 0.41185651\n",
      "Iteration 716, loss = 0.41147155\n",
      "Iteration 717, loss = 0.41095702\n",
      "Iteration 718, loss = 0.41052728\n",
      "Iteration 719, loss = 0.41037014\n",
      "Iteration 720, loss = 0.41052371\n",
      "Iteration 721, loss = 0.41032145\n",
      "Iteration 722, loss = 0.41048205\n",
      "Iteration 723, loss = 0.40997658\n",
      "Iteration 724, loss = 0.41007451\n",
      "Iteration 725, loss = 0.40949008\n",
      "Iteration 726, loss = 0.40937186\n",
      "Iteration 727, loss = 0.40870080\n",
      "Iteration 728, loss = 0.40899418\n",
      "Iteration 729, loss = 0.40871135\n",
      "Iteration 730, loss = 0.40849499\n",
      "Iteration 731, loss = 0.40825014\n",
      "Iteration 732, loss = 0.40804763\n",
      "Iteration 733, loss = 0.40783190\n",
      "Iteration 734, loss = 0.40750104\n",
      "Iteration 735, loss = 0.40730662\n",
      "Iteration 736, loss = 0.40749055\n",
      "Iteration 737, loss = 0.40726866\n",
      "Iteration 738, loss = 0.40708488\n",
      "Iteration 739, loss = 0.40660653\n",
      "Iteration 740, loss = 0.40648458\n",
      "Iteration 741, loss = 0.40627929\n",
      "Iteration 742, loss = 0.40621616\n",
      "Iteration 743, loss = 0.40593037\n",
      "Iteration 744, loss = 0.40611163\n",
      "Iteration 745, loss = 0.40524077\n",
      "Iteration 746, loss = 0.40563673\n",
      "Iteration 747, loss = 0.40544906\n",
      "Iteration 748, loss = 0.40492581\n",
      "Iteration 749, loss = 0.40448092\n",
      "Iteration 750, loss = 0.40492170\n",
      "Iteration 751, loss = 0.40474096\n",
      "Iteration 752, loss = 0.40447855\n",
      "Iteration 753, loss = 0.40408223\n",
      "Iteration 754, loss = 0.40379161\n",
      "Iteration 755, loss = 0.40369747\n",
      "Iteration 756, loss = 0.40359686\n",
      "Iteration 757, loss = 0.40316075\n",
      "Iteration 758, loss = 0.40340028\n",
      "Iteration 759, loss = 0.40264242\n",
      "Iteration 760, loss = 0.40293951\n",
      "Iteration 761, loss = 0.40269930\n",
      "Iteration 762, loss = 0.40295006\n",
      "Iteration 763, loss = 0.40229124\n",
      "Iteration 764, loss = 0.40234203\n",
      "Iteration 765, loss = 0.40160213\n",
      "Iteration 766, loss = 0.40189605\n",
      "Iteration 767, loss = 0.40207550\n",
      "Iteration 768, loss = 0.40139076\n",
      "Iteration 769, loss = 0.40113892\n",
      "Iteration 770, loss = 0.40133307\n",
      "Iteration 771, loss = 0.40068753\n",
      "Iteration 772, loss = 0.40063074\n",
      "Iteration 773, loss = 0.40090474\n",
      "Iteration 774, loss = 0.40033047\n",
      "Iteration 775, loss = 0.40022705\n",
      "Iteration 776, loss = 0.39986904\n",
      "Iteration 777, loss = 0.39999956\n",
      "Iteration 778, loss = 0.39983213\n",
      "Iteration 779, loss = 0.39937177\n",
      "Iteration 780, loss = 0.39911959\n",
      "Iteration 781, loss = 0.39951335\n",
      "Iteration 782, loss = 0.39955535\n",
      "Iteration 783, loss = 0.39891460\n",
      "Iteration 784, loss = 0.39863021\n",
      "Iteration 785, loss = 0.39910120\n",
      "Iteration 786, loss = 0.39809374\n",
      "Iteration 787, loss = 0.39807355\n",
      "Iteration 788, loss = 0.39805566\n",
      "Iteration 789, loss = 0.39777568\n",
      "Iteration 790, loss = 0.39754515\n",
      "Iteration 791, loss = 0.39768731\n",
      "Iteration 792, loss = 0.39701945\n",
      "Iteration 793, loss = 0.39733793\n",
      "Iteration 794, loss = 0.39717332\n",
      "Iteration 795, loss = 0.39688610\n",
      "Iteration 796, loss = 0.39653677\n",
      "Iteration 797, loss = 0.39667034\n",
      "Iteration 798, loss = 0.39678591\n",
      "Iteration 799, loss = 0.39631025\n",
      "Iteration 800, loss = 0.39625650\n",
      "Iteration 801, loss = 0.39550126\n",
      "Iteration 802, loss = 0.39588423\n",
      "Iteration 803, loss = 0.39589964\n",
      "Iteration 804, loss = 0.39560894\n",
      "Iteration 805, loss = 0.39574406\n",
      "Iteration 806, loss = 0.39516419\n",
      "Iteration 807, loss = 0.39501536\n",
      "Iteration 808, loss = 0.39542804\n",
      "Iteration 809, loss = 0.39480971\n",
      "Iteration 810, loss = 0.39431061\n",
      "Iteration 811, loss = 0.39480981\n",
      "Iteration 812, loss = 0.39452969\n",
      "Iteration 813, loss = 0.39405088\n",
      "Iteration 814, loss = 0.39410139\n",
      "Iteration 815, loss = 0.39385622\n",
      "Iteration 816, loss = 0.39392655\n",
      "Iteration 817, loss = 0.39381943\n",
      "Iteration 818, loss = 0.39318589\n",
      "Iteration 819, loss = 0.39310426\n",
      "Iteration 820, loss = 0.39298224\n",
      "Iteration 821, loss = 0.39287511\n",
      "Iteration 822, loss = 0.39323218\n",
      "Iteration 823, loss = 0.39286758\n",
      "Iteration 824, loss = 0.39304577\n",
      "Iteration 825, loss = 0.39270066\n",
      "Iteration 826, loss = 0.39236095\n",
      "Iteration 827, loss = 0.39153548\n",
      "Iteration 828, loss = 0.39200071\n",
      "Iteration 829, loss = 0.39175882\n",
      "Iteration 830, loss = 0.39159015\n",
      "Iteration 831, loss = 0.39193160\n",
      "Iteration 832, loss = 0.39148611\n",
      "Iteration 833, loss = 0.39130810\n",
      "Iteration 834, loss = 0.39101093\n",
      "Iteration 835, loss = 0.39108315\n",
      "Iteration 836, loss = 0.39059851\n",
      "Iteration 837, loss = 0.39048508\n",
      "Iteration 838, loss = 0.39076245\n",
      "Iteration 839, loss = 0.39039400\n",
      "Iteration 840, loss = 0.39022914\n",
      "Iteration 841, loss = 0.39020013\n",
      "Iteration 842, loss = 0.39043598\n",
      "Iteration 843, loss = 0.38918871\n",
      "Iteration 844, loss = 0.38933825\n",
      "Iteration 845, loss = 0.38953125\n",
      "Iteration 846, loss = 0.38995611\n",
      "Iteration 847, loss = 0.38921160\n",
      "Iteration 848, loss = 0.38915395\n",
      "Iteration 849, loss = 0.38917490\n",
      "Iteration 850, loss = 0.38914187\n",
      "Iteration 851, loss = 0.38859485\n",
      "Iteration 852, loss = 0.38851019\n",
      "Iteration 853, loss = 0.38857922\n",
      "Iteration 854, loss = 0.38860901\n",
      "Iteration 855, loss = 0.38831648\n",
      "Iteration 856, loss = 0.38846383\n",
      "Iteration 857, loss = 0.38812395\n",
      "Iteration 858, loss = 0.38827347\n",
      "Iteration 859, loss = 0.38746719\n",
      "Iteration 860, loss = 0.38746092\n",
      "Iteration 861, loss = 0.38744786\n",
      "Iteration 862, loss = 0.38735196\n",
      "Iteration 863, loss = 0.38717139\n",
      "Iteration 864, loss = 0.38717494\n",
      "Iteration 865, loss = 0.38681311\n",
      "Iteration 866, loss = 0.38676707\n",
      "Iteration 867, loss = 0.38687817\n",
      "Iteration 868, loss = 0.38632874\n",
      "Iteration 869, loss = 0.38628076\n",
      "Iteration 870, loss = 0.38630800\n",
      "Iteration 871, loss = 0.38637229\n",
      "Iteration 872, loss = 0.38540753\n",
      "Iteration 873, loss = 0.38598599\n",
      "Iteration 874, loss = 0.38567896\n",
      "Iteration 875, loss = 0.38564918\n",
      "Iteration 876, loss = 0.38610673\n",
      "Iteration 877, loss = 0.38543397\n",
      "Iteration 878, loss = 0.38502564\n",
      "Iteration 879, loss = 0.38556589\n",
      "Iteration 880, loss = 0.38539028\n",
      "Iteration 881, loss = 0.38470357\n",
      "Iteration 882, loss = 0.38461466\n",
      "Iteration 883, loss = 0.38507720\n",
      "Iteration 884, loss = 0.38443473\n",
      "Iteration 885, loss = 0.38432984\n",
      "Iteration 886, loss = 0.38459846\n",
      "Iteration 887, loss = 0.38433947\n",
      "Iteration 888, loss = 0.38424084\n",
      "Iteration 889, loss = 0.38356374\n",
      "Iteration 890, loss = 0.38390013\n",
      "Iteration 891, loss = 0.38379117\n",
      "Iteration 892, loss = 0.38363487\n",
      "Iteration 893, loss = 0.38338706\n",
      "Iteration 894, loss = 0.38329816\n",
      "Iteration 895, loss = 0.38332528\n",
      "Iteration 896, loss = 0.38320833\n",
      "Iteration 897, loss = 0.38304115\n",
      "Iteration 898, loss = 0.38278687\n",
      "Iteration 899, loss = 0.38258483\n",
      "Iteration 900, loss = 0.38303937\n",
      "Iteration 901, loss = 0.38221037\n",
      "Iteration 902, loss = 0.38230147\n",
      "Iteration 903, loss = 0.38177977\n",
      "Iteration 904, loss = 0.38233186\n",
      "Iteration 905, loss = 0.38213370\n",
      "Iteration 906, loss = 0.38154909\n",
      "Iteration 907, loss = 0.38229112\n",
      "Iteration 908, loss = 0.38194245\n",
      "Iteration 909, loss = 0.38114959\n",
      "Iteration 910, loss = 0.38152337\n",
      "Iteration 911, loss = 0.38140547\n",
      "Iteration 912, loss = 0.38152910\n",
      "Iteration 913, loss = 0.38087843\n",
      "Iteration 914, loss = 0.38092362\n",
      "Iteration 915, loss = 0.38055330\n",
      "Iteration 916, loss = 0.38096435\n",
      "Iteration 917, loss = 0.38073076\n",
      "Iteration 918, loss = 0.38046994\n",
      "Iteration 919, loss = 0.38064279\n",
      "Iteration 920, loss = 0.38081982\n",
      "Iteration 921, loss = 0.38041088\n",
      "Iteration 922, loss = 0.38028281\n",
      "Iteration 923, loss = 0.37997877\n",
      "Iteration 924, loss = 0.38014091\n",
      "Iteration 925, loss = 0.37997169\n",
      "Iteration 926, loss = 0.37974275\n",
      "Iteration 927, loss = 0.37978671\n",
      "Iteration 928, loss = 0.37935038\n",
      "Iteration 929, loss = 0.37932855\n",
      "Iteration 930, loss = 0.37934167\n",
      "Iteration 931, loss = 0.37890612\n",
      "Iteration 932, loss = 0.37926008\n",
      "Iteration 933, loss = 0.37946250\n",
      "Iteration 934, loss = 0.37906355\n",
      "Iteration 935, loss = 0.37883996\n",
      "Iteration 936, loss = 0.37875808\n",
      "Iteration 937, loss = 0.37876646\n",
      "Iteration 938, loss = 0.37818272\n",
      "Iteration 939, loss = 0.37844473\n",
      "Iteration 940, loss = 0.37841819\n",
      "Iteration 941, loss = 0.37811909\n",
      "Iteration 942, loss = 0.37790252\n",
      "Iteration 943, loss = 0.37818961\n",
      "Iteration 944, loss = 0.37809510\n",
      "Iteration 945, loss = 0.37803063\n",
      "Iteration 946, loss = 0.37771539\n",
      "Iteration 947, loss = 0.37733086\n",
      "Iteration 948, loss = 0.37750673\n",
      "Iteration 949, loss = 0.37767782\n",
      "Iteration 950, loss = 0.37749043\n",
      "Iteration 951, loss = 0.37709181\n",
      "Iteration 952, loss = 0.37683524\n",
      "Iteration 953, loss = 0.37699851\n",
      "Iteration 954, loss = 0.37679026\n",
      "Iteration 955, loss = 0.37664339\n",
      "Iteration 956, loss = 0.37657740\n",
      "Iteration 957, loss = 0.37639924\n",
      "Iteration 958, loss = 0.37639096\n",
      "Iteration 959, loss = 0.37632506\n",
      "Iteration 960, loss = 0.37651794\n",
      "Iteration 961, loss = 0.37629472\n",
      "Iteration 962, loss = 0.37593935\n",
      "Iteration 963, loss = 0.37631263\n",
      "Iteration 964, loss = 0.37582992\n",
      "Iteration 965, loss = 0.37556012\n",
      "Iteration 966, loss = 0.37534505\n",
      "Iteration 967, loss = 0.37557328\n",
      "Iteration 968, loss = 0.37542947\n",
      "Iteration 969, loss = 0.37564907\n",
      "Iteration 970, loss = 0.37558921\n",
      "Iteration 971, loss = 0.37536819\n",
      "Iteration 972, loss = 0.37523275\n",
      "Iteration 973, loss = 0.37499009\n",
      "Iteration 974, loss = 0.37486565\n",
      "Iteration 975, loss = 0.37505978\n",
      "Iteration 976, loss = 0.37477646\n",
      "Iteration 977, loss = 0.37471318\n",
      "Iteration 978, loss = 0.37452896\n",
      "Iteration 979, loss = 0.37502328\n",
      "Iteration 980, loss = 0.37486157\n",
      "Iteration 981, loss = 0.37440141\n",
      "Iteration 982, loss = 0.37419617\n",
      "Iteration 983, loss = 0.37391287\n",
      "Iteration 984, loss = 0.37399811\n",
      "Iteration 985, loss = 0.37383011\n",
      "Iteration 986, loss = 0.37421541\n",
      "Iteration 987, loss = 0.37372794\n",
      "Iteration 988, loss = 0.37391559\n",
      "Iteration 989, loss = 0.37340174\n",
      "Iteration 990, loss = 0.37393086\n",
      "Iteration 991, loss = 0.37348955\n",
      "Iteration 992, loss = 0.37329163\n",
      "Iteration 993, loss = 0.37322581\n",
      "Iteration 994, loss = 0.37298331\n",
      "Iteration 995, loss = 0.37278902\n",
      "Iteration 996, loss = 0.37278221\n",
      "Iteration 997, loss = 0.37278746\n",
      "Iteration 998, loss = 0.37268585\n",
      "Iteration 999, loss = 0.37252976\n",
      "Iteration 1000, loss = 0.37280376\n",
      "Iteration 1001, loss = 0.37282289\n",
      "Iteration 1002, loss = 0.37244399\n",
      "Iteration 1003, loss = 0.37233557\n",
      "Iteration 1004, loss = 0.37233510\n",
      "Iteration 1005, loss = 0.37274167\n",
      "Iteration 1006, loss = 0.37251601\n",
      "Iteration 1007, loss = 0.37223564\n",
      "Iteration 1008, loss = 0.37201325\n",
      "Iteration 1009, loss = 0.37208536\n",
      "Iteration 1010, loss = 0.37159560\n",
      "Iteration 1011, loss = 0.37138380\n",
      "Iteration 1012, loss = 0.37141025\n",
      "Iteration 1013, loss = 0.37156253\n",
      "Iteration 1014, loss = 0.37126960\n",
      "Iteration 1015, loss = 0.37165030\n",
      "Iteration 1016, loss = 0.37144390\n",
      "Iteration 1017, loss = 0.37091247\n",
      "Iteration 1018, loss = 0.37113257\n",
      "Iteration 1019, loss = 0.37080697\n",
      "Iteration 1020, loss = 0.37100865\n",
      "Iteration 1021, loss = 0.37057714\n",
      "Iteration 1022, loss = 0.37063796\n",
      "Iteration 1023, loss = 0.37087983\n",
      "Iteration 1024, loss = 0.37034914\n",
      "Iteration 1025, loss = 0.37027787\n",
      "Iteration 1026, loss = 0.37029091\n",
      "Iteration 1027, loss = 0.37086522\n",
      "Iteration 1028, loss = 0.36991738\n",
      "Iteration 1029, loss = 0.37041973\n",
      "Iteration 1030, loss = 0.37016653\n",
      "Iteration 1031, loss = 0.36943735\n",
      "Iteration 1032, loss = 0.36991629\n",
      "Iteration 1033, loss = 0.36987454\n",
      "Iteration 1034, loss = 0.37000679\n",
      "Iteration 1035, loss = 0.36981910\n",
      "Iteration 1036, loss = 0.36938088\n",
      "Iteration 1037, loss = 0.36930210\n",
      "Iteration 1038, loss = 0.36942375\n",
      "Iteration 1039, loss = 0.36951327\n",
      "Iteration 1040, loss = 0.36920943\n",
      "Iteration 1041, loss = 0.36919582\n",
      "Iteration 1042, loss = 0.36905888\n",
      "Iteration 1043, loss = 0.36946314\n",
      "Iteration 1044, loss = 0.36856594\n",
      "Iteration 1045, loss = 0.36932144\n",
      "Iteration 1046, loss = 0.36872756\n",
      "Iteration 1047, loss = 0.36893956\n",
      "Iteration 1048, loss = 0.36867014\n",
      "Iteration 1049, loss = 0.36867607\n",
      "Iteration 1050, loss = 0.36830037\n",
      "Iteration 1051, loss = 0.36838748\n",
      "Iteration 1052, loss = 0.36815055\n",
      "Iteration 1053, loss = 0.36837134\n",
      "Iteration 1054, loss = 0.36825005\n",
      "Iteration 1055, loss = 0.36838669\n",
      "Iteration 1056, loss = 0.36805173\n",
      "Iteration 1057, loss = 0.36779293\n",
      "Iteration 1058, loss = 0.36797975\n",
      "Iteration 1059, loss = 0.36805245\n",
      "Iteration 1060, loss = 0.36766001\n",
      "Iteration 1061, loss = 0.36795570\n",
      "Iteration 1062, loss = 0.36761693\n",
      "Iteration 1063, loss = 0.36789179\n",
      "Iteration 1064, loss = 0.36741833\n",
      "Iteration 1065, loss = 0.36773655\n",
      "Iteration 1066, loss = 0.36735537\n",
      "Iteration 1067, loss = 0.36717534\n",
      "Iteration 1068, loss = 0.36699407\n",
      "Iteration 1069, loss = 0.36721752\n",
      "Iteration 1070, loss = 0.36737711\n",
      "Iteration 1071, loss = 0.36694954\n",
      "Iteration 1072, loss = 0.36681076\n",
      "Iteration 1073, loss = 0.36664326\n",
      "Iteration 1074, loss = 0.36644209\n",
      "Iteration 1075, loss = 0.36639841\n",
      "Iteration 1076, loss = 0.36656566\n",
      "Iteration 1077, loss = 0.36675886\n",
      "Iteration 1078, loss = 0.36665902\n",
      "Iteration 1079, loss = 0.36682396\n",
      "Iteration 1080, loss = 0.36585573\n",
      "Iteration 1081, loss = 0.36634180\n",
      "Iteration 1082, loss = 0.36613767\n",
      "Iteration 1083, loss = 0.36642518\n",
      "Iteration 1084, loss = 0.36626882\n",
      "Iteration 1085, loss = 0.36573198\n",
      "Iteration 1086, loss = 0.36613948\n",
      "Iteration 1087, loss = 0.36565370\n",
      "Iteration 1088, loss = 0.36563502\n",
      "Iteration 1089, loss = 0.36575831\n",
      "Iteration 1090, loss = 0.36629716\n",
      "Iteration 1091, loss = 0.36522440\n",
      "Iteration 1092, loss = 0.36575559\n",
      "Iteration 1093, loss = 0.36521601\n",
      "Iteration 1094, loss = 0.36542479\n",
      "Iteration 1095, loss = 0.36484200\n",
      "Iteration 1096, loss = 0.36510315\n",
      "Iteration 1097, loss = 0.36567405\n",
      "Iteration 1098, loss = 0.36537192\n",
      "Iteration 1099, loss = 0.36523261\n",
      "Iteration 1100, loss = 0.36527431\n",
      "Iteration 1101, loss = 0.36514912\n",
      "Iteration 1102, loss = 0.36474971\n",
      "Iteration 1103, loss = 0.36500116\n",
      "Iteration 1104, loss = 0.36451578\n",
      "Iteration 1105, loss = 0.36466965\n",
      "Iteration 1106, loss = 0.36429036\n",
      "Iteration 1107, loss = 0.36426764\n",
      "Iteration 1108, loss = 0.36414519\n",
      "Iteration 1109, loss = 0.36465638\n",
      "Iteration 1110, loss = 0.36450431\n",
      "Iteration 1111, loss = 0.36432827\n",
      "Iteration 1112, loss = 0.36384813\n",
      "Iteration 1113, loss = 0.36419845\n",
      "Iteration 1114, loss = 0.36401438\n",
      "Iteration 1115, loss = 0.36404078\n",
      "Iteration 1116, loss = 0.36348557\n",
      "Iteration 1117, loss = 0.36371692\n",
      "Iteration 1118, loss = 0.36376576\n",
      "Iteration 1119, loss = 0.36428715\n",
      "Iteration 1120, loss = 0.36406888\n",
      "Iteration 1121, loss = 0.36369470\n",
      "Iteration 1122, loss = 0.36340878\n",
      "Iteration 1123, loss = 0.36334571\n",
      "Iteration 1124, loss = 0.36294819\n",
      "Iteration 1125, loss = 0.36291272\n",
      "Iteration 1126, loss = 0.36355738\n",
      "Iteration 1127, loss = 0.36343746\n",
      "Iteration 1128, loss = 0.36331343\n",
      "Iteration 1129, loss = 0.36289246\n",
      "Iteration 1130, loss = 0.36317893\n",
      "Iteration 1131, loss = 0.36291348\n",
      "Iteration 1132, loss = 0.36307813\n",
      "Iteration 1133, loss = 0.36275745\n",
      "Iteration 1134, loss = 0.36268568\n",
      "Iteration 1135, loss = 0.36261500\n",
      "Iteration 1136, loss = 0.36240449\n",
      "Iteration 1137, loss = 0.36243563\n",
      "Iteration 1138, loss = 0.36235914\n",
      "Iteration 1139, loss = 0.36270429\n",
      "Iteration 1140, loss = 0.36190492\n",
      "Iteration 1141, loss = 0.36224811\n",
      "Iteration 1142, loss = 0.36200345\n",
      "Iteration 1143, loss = 0.36235549\n",
      "Iteration 1144, loss = 0.36216492\n",
      "Iteration 1145, loss = 0.36198750\n",
      "Iteration 1146, loss = 0.36175792\n",
      "Iteration 1147, loss = 0.36191966\n",
      "Iteration 1148, loss = 0.36238435\n",
      "Iteration 1149, loss = 0.36170855\n",
      "Iteration 1150, loss = 0.36183564\n",
      "Iteration 1151, loss = 0.36163478\n",
      "Iteration 1152, loss = 0.36216799\n",
      "Iteration 1153, loss = 0.36115204\n",
      "Iteration 1154, loss = 0.36144437\n",
      "Iteration 1155, loss = 0.36130496\n",
      "Iteration 1156, loss = 0.36132270\n",
      "Iteration 1157, loss = 0.36117603\n",
      "Iteration 1158, loss = 0.36130027\n",
      "Iteration 1159, loss = 0.36102599\n",
      "Iteration 1160, loss = 0.36108633\n",
      "Iteration 1161, loss = 0.36102675\n",
      "Iteration 1162, loss = 0.36130836\n",
      "Iteration 1163, loss = 0.36051255\n",
      "Iteration 1164, loss = 0.36067278\n",
      "Iteration 1165, loss = 0.36116844\n",
      "Iteration 1166, loss = 0.36088648\n",
      "Iteration 1167, loss = 0.36050097\n",
      "Iteration 1168, loss = 0.36053632\n",
      "Iteration 1169, loss = 0.36081980\n",
      "Iteration 1170, loss = 0.36039691\n",
      "Iteration 1171, loss = 0.36090499\n",
      "Iteration 1172, loss = 0.36065493\n",
      "Iteration 1173, loss = 0.36012825\n",
      "Iteration 1174, loss = 0.36038543\n",
      "Iteration 1175, loss = 0.36038538\n",
      "Iteration 1176, loss = 0.36002546\n",
      "Iteration 1177, loss = 0.36019102\n",
      "Iteration 1178, loss = 0.36003181\n",
      "Iteration 1179, loss = 0.35973746\n",
      "Iteration 1180, loss = 0.36053837\n",
      "Iteration 1181, loss = 0.35968896\n",
      "Iteration 1182, loss = 0.35971663\n",
      "Iteration 1183, loss = 0.36005886\n",
      "Iteration 1184, loss = 0.35965340\n",
      "Iteration 1185, loss = 0.35987497\n",
      "Iteration 1186, loss = 0.35921667\n",
      "Iteration 1187, loss = 0.35921331\n",
      "Iteration 1188, loss = 0.35960460\n",
      "Iteration 1189, loss = 0.35973772\n",
      "Iteration 1190, loss = 0.35927934\n",
      "Iteration 1191, loss = 0.35952391\n",
      "Iteration 1192, loss = 0.35903743\n",
      "Iteration 1193, loss = 0.35934855\n",
      "Iteration 1194, loss = 0.35903672\n",
      "Iteration 1195, loss = 0.35950734\n",
      "Iteration 1196, loss = 0.35865663\n",
      "Iteration 1197, loss = 0.35913801\n",
      "Iteration 1198, loss = 0.35962934\n",
      "Iteration 1199, loss = 0.35904804\n",
      "Iteration 1200, loss = 0.35865336\n",
      "Iteration 1201, loss = 0.35883134\n",
      "Iteration 1202, loss = 0.35872268\n",
      "Iteration 1203, loss = 0.35849758\n",
      "Iteration 1204, loss = 0.35903729\n",
      "Iteration 1205, loss = 0.35843215\n",
      "Iteration 1206, loss = 0.35835013\n",
      "Iteration 1207, loss = 0.35839938\n",
      "Iteration 1208, loss = 0.35795270\n",
      "Iteration 1209, loss = 0.35832837\n",
      "Iteration 1210, loss = 0.35796932\n",
      "Iteration 1211, loss = 0.35822962\n",
      "Iteration 1212, loss = 0.35789897\n",
      "Iteration 1213, loss = 0.35788126\n",
      "Iteration 1214, loss = 0.35801575\n",
      "Iteration 1215, loss = 0.35797020\n",
      "Iteration 1216, loss = 0.35806885\n",
      "Iteration 1217, loss = 0.35824816\n",
      "Iteration 1218, loss = 0.35799354\n",
      "Iteration 1219, loss = 0.35788651\n",
      "Iteration 1220, loss = 0.35737864\n",
      "Iteration 1221, loss = 0.35736004\n",
      "Iteration 1222, loss = 0.35734726\n",
      "Iteration 1223, loss = 0.35778006\n",
      "Iteration 1224, loss = 0.35763665\n",
      "Iteration 1225, loss = 0.35736113\n",
      "Iteration 1226, loss = 0.35733227\n",
      "Iteration 1227, loss = 0.35742038\n",
      "Iteration 1228, loss = 0.35742611\n",
      "Iteration 1229, loss = 0.35673029\n",
      "Iteration 1230, loss = 0.35695109\n",
      "Iteration 1231, loss = 0.35679265\n",
      "Iteration 1232, loss = 0.35702485\n",
      "Iteration 1233, loss = 0.35699113\n",
      "Iteration 1234, loss = 0.35705207\n",
      "Iteration 1235, loss = 0.35694605\n",
      "Iteration 1236, loss = 0.35664184\n",
      "Iteration 1237, loss = 0.35624122\n",
      "Iteration 1238, loss = 0.35700533\n",
      "Iteration 1239, loss = 0.35690725\n",
      "Iteration 1240, loss = 0.35618312\n",
      "Iteration 1241, loss = 0.35662629\n",
      "Iteration 1242, loss = 0.35640693\n",
      "Iteration 1243, loss = 0.35670112\n",
      "Iteration 1244, loss = 0.35643617\n",
      "Iteration 1245, loss = 0.35607648\n",
      "Iteration 1246, loss = 0.35661154\n",
      "Iteration 1247, loss = 0.35679382\n",
      "Iteration 1248, loss = 0.35601205\n",
      "Iteration 1249, loss = 0.35611110\n",
      "Iteration 1250, loss = 0.35574937\n",
      "Iteration 1251, loss = 0.35608910\n",
      "Iteration 1252, loss = 0.35580657\n",
      "Iteration 1253, loss = 0.35559320\n",
      "Iteration 1254, loss = 0.35589874\n",
      "Iteration 1255, loss = 0.35599324\n",
      "Iteration 1256, loss = 0.35576134\n",
      "Iteration 1257, loss = 0.35585851\n",
      "Iteration 1258, loss = 0.35561151\n",
      "Iteration 1259, loss = 0.35533971\n",
      "Iteration 1260, loss = 0.35508977\n",
      "Iteration 1261, loss = 0.35566406\n",
      "Iteration 1262, loss = 0.35498267\n",
      "Iteration 1263, loss = 0.35520061\n",
      "Iteration 1264, loss = 0.35496978\n",
      "Iteration 1265, loss = 0.35518742\n",
      "Iteration 1266, loss = 0.35546846\n",
      "Iteration 1267, loss = 0.35534596\n",
      "Iteration 1268, loss = 0.35497977\n",
      "Iteration 1269, loss = 0.35490288\n",
      "Iteration 1270, loss = 0.35501579\n",
      "Iteration 1271, loss = 0.35487976\n",
      "Iteration 1272, loss = 0.35498249\n",
      "Iteration 1273, loss = 0.35462350\n",
      "Iteration 1274, loss = 0.35463268\n",
      "Iteration 1275, loss = 0.35503229\n",
      "Iteration 1276, loss = 0.35468355\n",
      "Iteration 1277, loss = 0.35449948\n",
      "Iteration 1278, loss = 0.35452218\n",
      "Iteration 1279, loss = 0.35436741\n",
      "Iteration 1280, loss = 0.35473257\n",
      "Iteration 1281, loss = 0.35444259\n",
      "Iteration 1282, loss = 0.35514884\n",
      "Iteration 1283, loss = 0.35423452\n",
      "Iteration 1284, loss = 0.35460664\n",
      "Iteration 1285, loss = 0.35427050\n",
      "Iteration 1286, loss = 0.35450011\n",
      "Iteration 1287, loss = 0.35464485\n",
      "Iteration 1288, loss = 0.35406987\n",
      "Iteration 1289, loss = 0.35441766\n",
      "Iteration 1290, loss = 0.35367394\n",
      "Iteration 1291, loss = 0.35430494\n",
      "Iteration 1292, loss = 0.35399350\n",
      "Iteration 1293, loss = 0.35374242\n",
      "Iteration 1294, loss = 0.35414179\n",
      "Iteration 1295, loss = 0.35397801\n",
      "Iteration 1296, loss = 0.35355798\n",
      "Iteration 1297, loss = 0.35397622\n",
      "Iteration 1298, loss = 0.35363563\n",
      "Iteration 1299, loss = 0.35327738\n",
      "Iteration 1300, loss = 0.35327109\n",
      "Iteration 1301, loss = 0.35355016\n",
      "Iteration 1302, loss = 0.35327555\n",
      "Iteration 1303, loss = 0.35360605\n",
      "Iteration 1304, loss = 0.35374614\n",
      "Iteration 1305, loss = 0.35328696\n",
      "Iteration 1306, loss = 0.35373090\n",
      "Iteration 1307, loss = 0.35330647\n",
      "Iteration 1308, loss = 0.35319401\n",
      "Iteration 1309, loss = 0.35275815\n",
      "Iteration 1310, loss = 0.35307214\n",
      "Iteration 1311, loss = 0.35316185\n",
      "Iteration 1312, loss = 0.35285154\n",
      "Iteration 1313, loss = 0.35275893\n",
      "Iteration 1314, loss = 0.35294452\n",
      "Iteration 1315, loss = 0.35260475\n",
      "Iteration 1316, loss = 0.35294435\n",
      "Iteration 1317, loss = 0.35291029\n",
      "Iteration 1318, loss = 0.35303736\n",
      "Iteration 1319, loss = 0.35304327\n",
      "Iteration 1320, loss = 0.35253237\n",
      "Iteration 1321, loss = 0.35290997\n",
      "Iteration 1322, loss = 0.35247387\n",
      "Iteration 1323, loss = 0.35235476\n",
      "Iteration 1324, loss = 0.35229282\n",
      "Iteration 1325, loss = 0.35233891\n",
      "Iteration 1326, loss = 0.35271124\n",
      "Iteration 1327, loss = 0.35231505\n",
      "Iteration 1328, loss = 0.35225557\n",
      "Iteration 1329, loss = 0.35236610\n",
      "Iteration 1330, loss = 0.35238808\n",
      "Iteration 1331, loss = 0.35207129\n",
      "Iteration 1332, loss = 0.35244462\n",
      "Iteration 1333, loss = 0.35211892\n",
      "Iteration 1334, loss = 0.35249971\n",
      "Iteration 1335, loss = 0.35221917\n",
      "Iteration 1336, loss = 0.35207334\n",
      "Iteration 1337, loss = 0.35189289\n",
      "Iteration 1338, loss = 0.35181287\n",
      "Iteration 1339, loss = 0.35221245\n",
      "Iteration 1340, loss = 0.35209155\n",
      "Iteration 1341, loss = 0.35171419\n",
      "Iteration 1342, loss = 0.35189117\n",
      "Iteration 1343, loss = 0.35147355\n",
      "Iteration 1344, loss = 0.35191007\n",
      "Iteration 1345, loss = 0.35164244\n",
      "Iteration 1346, loss = 0.35197740\n",
      "Iteration 1347, loss = 0.35151987\n",
      "Iteration 1348, loss = 0.35137133\n",
      "Iteration 1349, loss = 0.35149603\n",
      "Iteration 1350, loss = 0.35182434\n",
      "Iteration 1351, loss = 0.35136750\n",
      "Iteration 1352, loss = 0.35149202\n",
      "Iteration 1353, loss = 0.35107437\n",
      "Iteration 1354, loss = 0.35104026\n",
      "Iteration 1355, loss = 0.35107138\n",
      "Iteration 1356, loss = 0.35094210\n",
      "Iteration 1357, loss = 0.35089635\n",
      "Iteration 1358, loss = 0.35128831\n",
      "Iteration 1359, loss = 0.35086168\n",
      "Iteration 1360, loss = 0.35074468\n",
      "Iteration 1361, loss = 0.35076582\n",
      "Iteration 1362, loss = 0.35130942\n",
      "Iteration 1363, loss = 0.35091009\n",
      "Iteration 1364, loss = 0.35082228\n",
      "Iteration 1365, loss = 0.35096471\n",
      "Iteration 1366, loss = 0.35060923\n",
      "Iteration 1367, loss = 0.35088596\n",
      "Iteration 1368, loss = 0.35100230\n",
      "Iteration 1369, loss = 0.35067478\n",
      "Iteration 1370, loss = 0.35012372\n",
      "Iteration 1371, loss = 0.35019549\n",
      "Iteration 1372, loss = 0.35057962\n",
      "Iteration 1373, loss = 0.35029259\n",
      "Iteration 1374, loss = 0.35040296\n",
      "Iteration 1375, loss = 0.35036981\n",
      "Iteration 1376, loss = 0.35031933\n",
      "Iteration 1377, loss = 0.35023818\n",
      "Iteration 1378, loss = 0.35011401\n",
      "Iteration 1379, loss = 0.34991329\n",
      "Iteration 1380, loss = 0.35027356\n",
      "Iteration 1381, loss = 0.34995999\n",
      "Iteration 1382, loss = 0.35053794\n",
      "Iteration 1383, loss = 0.34984220\n",
      "Iteration 1384, loss = 0.35020531\n",
      "Iteration 1385, loss = 0.35009771\n",
      "Iteration 1386, loss = 0.35002059\n",
      "Iteration 1387, loss = 0.34994449\n",
      "Iteration 1388, loss = 0.35022691\n",
      "Iteration 1389, loss = 0.35016595\n",
      "Iteration 1390, loss = 0.34971030\n",
      "Iteration 1391, loss = 0.34985736\n",
      "Iteration 1392, loss = 0.34990888\n",
      "Iteration 1393, loss = 0.34966262\n",
      "Iteration 1394, loss = 0.35004130\n",
      "Iteration 1395, loss = 0.34957229\n",
      "Iteration 1396, loss = 0.34959838\n",
      "Iteration 1397, loss = 0.34928905\n",
      "Iteration 1398, loss = 0.34953272\n",
      "Iteration 1399, loss = 0.34962212\n",
      "Iteration 1400, loss = 0.34923008\n",
      "Iteration 1401, loss = 0.34973471\n",
      "Iteration 1402, loss = 0.34965092\n",
      "Iteration 1403, loss = 0.34943239\n",
      "Iteration 1404, loss = 0.34889434\n",
      "Iteration 1405, loss = 0.34948572\n",
      "Iteration 1406, loss = 0.34918149\n",
      "Iteration 1407, loss = 0.34922211\n",
      "Iteration 1408, loss = 0.34926186\n",
      "Iteration 1409, loss = 0.34887125\n",
      "Iteration 1410, loss = 0.34912366\n",
      "Iteration 1411, loss = 0.34888645\n",
      "Iteration 1412, loss = 0.34924053\n",
      "Iteration 1413, loss = 0.34898417\n",
      "Iteration 1414, loss = 0.34917737\n",
      "Iteration 1415, loss = 0.34840970\n",
      "Iteration 1416, loss = 0.34884531\n",
      "Iteration 1417, loss = 0.34866859\n",
      "Iteration 1418, loss = 0.34864237\n",
      "Iteration 1419, loss = 0.34871370\n",
      "Iteration 1420, loss = 0.34847408\n",
      "Iteration 1421, loss = 0.34858600\n",
      "Iteration 1422, loss = 0.34849887\n",
      "Iteration 1423, loss = 0.34812366\n",
      "Iteration 1424, loss = 0.34848954\n",
      "Iteration 1425, loss = 0.34842004\n",
      "Iteration 1426, loss = 0.34882809\n",
      "Iteration 1427, loss = 0.34815092\n",
      "Iteration 1428, loss = 0.34894392\n",
      "Iteration 1429, loss = 0.34819573\n",
      "Iteration 1430, loss = 0.34852060\n",
      "Iteration 1431, loss = 0.34809447\n",
      "Iteration 1432, loss = 0.34848540\n",
      "Iteration 1433, loss = 0.34822009\n",
      "Iteration 1434, loss = 0.34787148\n",
      "Iteration 1435, loss = 0.34864936\n",
      "Iteration 1436, loss = 0.34813311\n",
      "Iteration 1437, loss = 0.34806874\n",
      "Iteration 1438, loss = 0.34813695\n",
      "Iteration 1439, loss = 0.34840438\n",
      "Iteration 1440, loss = 0.34793596\n",
      "Iteration 1441, loss = 0.34821595\n",
      "Iteration 1442, loss = 0.34790854\n",
      "Iteration 1443, loss = 0.34786697\n",
      "Iteration 1444, loss = 0.34765871\n",
      "Iteration 1445, loss = 0.34787115\n",
      "Iteration 1446, loss = 0.34757640\n",
      "Iteration 1447, loss = 0.34817560\n",
      "Iteration 1448, loss = 0.34809377\n",
      "Iteration 1449, loss = 0.34746589\n",
      "Iteration 1450, loss = 0.34766341\n",
      "Iteration 1451, loss = 0.34757530\n",
      "Iteration 1452, loss = 0.34728531\n",
      "Iteration 1453, loss = 0.34754984\n",
      "Iteration 1454, loss = 0.34732352\n",
      "Iteration 1455, loss = 0.34730764\n",
      "Iteration 1456, loss = 0.34730315\n",
      "Iteration 1457, loss = 0.34720605\n",
      "Iteration 1458, loss = 0.34712013\n",
      "Iteration 1459, loss = 0.34746583\n",
      "Iteration 1460, loss = 0.34750750\n",
      "Iteration 1461, loss = 0.34714942\n",
      "Iteration 1462, loss = 0.34733333\n",
      "Iteration 1463, loss = 0.34714920\n",
      "Iteration 1464, loss = 0.34754601\n",
      "Iteration 1465, loss = 0.34696025\n",
      "Iteration 1466, loss = 0.34695701\n",
      "Iteration 1467, loss = 0.34715330\n",
      "Iteration 1468, loss = 0.34647400\n",
      "Iteration 1469, loss = 0.34693912\n",
      "Iteration 1470, loss = 0.34670614\n",
      "Iteration 1471, loss = 0.34651860\n",
      "Iteration 1472, loss = 0.34658952\n",
      "Iteration 1473, loss = 0.34673728\n",
      "Iteration 1474, loss = 0.34689438\n",
      "Iteration 1475, loss = 0.34701773\n",
      "Iteration 1476, loss = 0.34652305\n",
      "Iteration 1477, loss = 0.34705419\n",
      "Iteration 1478, loss = 0.34696031\n",
      "Iteration 1479, loss = 0.34646109\n",
      "Iteration 1480, loss = 0.34669408\n",
      "Iteration 1481, loss = 0.34680156\n",
      "Iteration 1482, loss = 0.34666311\n",
      "Iteration 1483, loss = 0.34630887\n",
      "Iteration 1484, loss = 0.34674775\n",
      "Iteration 1485, loss = 0.34674744\n",
      "Iteration 1486, loss = 0.34654549\n",
      "Iteration 1487, loss = 0.34620317\n",
      "Iteration 1488, loss = 0.34605260\n",
      "Iteration 1489, loss = 0.34673166\n",
      "Iteration 1490, loss = 0.34660096\n",
      "Iteration 1491, loss = 0.34648182\n",
      "Iteration 1492, loss = 0.34640734\n",
      "Iteration 1493, loss = 0.34628387\n",
      "Iteration 1494, loss = 0.34668215\n",
      "Iteration 1495, loss = 0.34637592\n",
      "Iteration 1496, loss = 0.34616580\n",
      "Iteration 1497, loss = 0.34627480\n",
      "Iteration 1498, loss = 0.34596551\n",
      "Iteration 1499, loss = 0.34628873\n",
      "Iteration 1500, loss = 0.34609802\n",
      "Iteration 1501, loss = 0.34578198\n",
      "Iteration 1502, loss = 0.34600413\n",
      "Iteration 1503, loss = 0.34615895\n",
      "Iteration 1504, loss = 0.34625258\n",
      "Iteration 1505, loss = 0.34578722\n",
      "Iteration 1506, loss = 0.34601561\n",
      "Iteration 1507, loss = 0.34616908\n",
      "Iteration 1508, loss = 0.34552627\n",
      "Iteration 1509, loss = 0.34585583\n",
      "Iteration 1510, loss = 0.34598190\n",
      "Iteration 1511, loss = 0.34578552\n",
      "Iteration 1512, loss = 0.34526423\n",
      "Iteration 1513, loss = 0.34528498\n",
      "Iteration 1514, loss = 0.34558711\n",
      "Iteration 1515, loss = 0.34582664\n",
      "Iteration 1516, loss = 0.34556653\n",
      "Iteration 1517, loss = 0.34580036\n",
      "Iteration 1518, loss = 0.34553731\n",
      "Iteration 1519, loss = 0.34526957\n",
      "Iteration 1520, loss = 0.34543631\n",
      "Iteration 1521, loss = 0.34555664\n",
      "Iteration 1522, loss = 0.34563129\n",
      "Iteration 1523, loss = 0.34525756\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66265450\n",
      "Iteration 2, loss = 0.65098943\n",
      "Iteration 3, loss = 0.64930884\n",
      "Iteration 4, loss = 0.64684742\n",
      "Iteration 5, loss = 0.64503498\n",
      "Iteration 6, loss = 0.64352805\n",
      "Iteration 7, loss = 0.64235046\n",
      "Iteration 8, loss = 0.64025491\n",
      "Iteration 9, loss = 0.63881571\n",
      "Iteration 10, loss = 0.63735662\n",
      "Iteration 11, loss = 0.63652443\n",
      "Iteration 12, loss = 0.63421725\n",
      "Iteration 13, loss = 0.63301585\n",
      "Iteration 14, loss = 0.63096879\n",
      "Iteration 15, loss = 0.62981794\n",
      "Iteration 16, loss = 0.62808961\n",
      "Iteration 17, loss = 0.62633439\n",
      "Iteration 18, loss = 0.62458810\n",
      "Iteration 19, loss = 0.62302274\n",
      "Iteration 20, loss = 0.62183221\n",
      "Iteration 21, loss = 0.62076059\n",
      "Iteration 22, loss = 0.61855169\n",
      "Iteration 23, loss = 0.61653987\n",
      "Iteration 24, loss = 0.61470872\n",
      "Iteration 25, loss = 0.61340760\n",
      "Iteration 26, loss = 0.61148642\n",
      "Iteration 27, loss = 0.60901673\n",
      "Iteration 28, loss = 0.60865844\n",
      "Iteration 29, loss = 0.60616791\n",
      "Iteration 30, loss = 0.60369482\n",
      "Iteration 31, loss = 0.60268772\n",
      "Iteration 32, loss = 0.60077958\n",
      "Iteration 33, loss = 0.59887743\n",
      "Iteration 34, loss = 0.59696891\n",
      "Iteration 35, loss = 0.59548351\n",
      "Iteration 36, loss = 0.59339950\n",
      "Iteration 37, loss = 0.59142503\n",
      "Iteration 38, loss = 0.58953182\n",
      "Iteration 39, loss = 0.58826526\n",
      "Iteration 40, loss = 0.58638475\n",
      "Iteration 41, loss = 0.58446129\n",
      "Iteration 42, loss = 0.58275595\n",
      "Iteration 43, loss = 0.58002575\n",
      "Iteration 44, loss = 0.57944810\n",
      "Iteration 45, loss = 0.57697596\n",
      "Iteration 46, loss = 0.57501551\n",
      "Iteration 47, loss = 0.57366795\n",
      "Iteration 48, loss = 0.57195331\n",
      "Iteration 49, loss = 0.57038173\n",
      "Iteration 50, loss = 0.56811393\n",
      "Iteration 51, loss = 0.56657216\n",
      "Iteration 52, loss = 0.56446192\n",
      "Iteration 53, loss = 0.56317208\n",
      "Iteration 54, loss = 0.56139669\n",
      "Iteration 55, loss = 0.55999502\n",
      "Iteration 56, loss = 0.55789948\n",
      "Iteration 57, loss = 0.55639513\n",
      "Iteration 58, loss = 0.55501801\n",
      "Iteration 59, loss = 0.55373556\n",
      "Iteration 60, loss = 0.55145037\n",
      "Iteration 61, loss = 0.55053612\n",
      "Iteration 62, loss = 0.54896184\n",
      "Iteration 63, loss = 0.54737494\n",
      "Iteration 64, loss = 0.54562768\n",
      "Iteration 65, loss = 0.54421546\n",
      "Iteration 66, loss = 0.54215863\n",
      "Iteration 67, loss = 0.54111806\n",
      "Iteration 68, loss = 0.53961821\n",
      "Iteration 69, loss = 0.53781884\n",
      "Iteration 70, loss = 0.53650330\n",
      "Iteration 71, loss = 0.53622402\n",
      "Iteration 72, loss = 0.53355181\n",
      "Iteration 73, loss = 0.53258581\n",
      "Iteration 74, loss = 0.53104463\n",
      "Iteration 75, loss = 0.52995450\n",
      "Iteration 76, loss = 0.52847064\n",
      "Iteration 77, loss = 0.52744615\n",
      "Iteration 78, loss = 0.52582897\n",
      "Iteration 79, loss = 0.52499455\n",
      "Iteration 80, loss = 0.52348728\n",
      "Iteration 81, loss = 0.52262726\n",
      "Iteration 82, loss = 0.52073176\n",
      "Iteration 83, loss = 0.52021702\n",
      "Iteration 84, loss = 0.51846436\n",
      "Iteration 85, loss = 0.51720376\n",
      "Iteration 86, loss = 0.51644947\n",
      "Iteration 87, loss = 0.51523560\n",
      "Iteration 88, loss = 0.51403476\n",
      "Iteration 89, loss = 0.51292929\n",
      "Iteration 90, loss = 0.51206804\n",
      "Iteration 91, loss = 0.51098343\n",
      "Iteration 92, loss = 0.50957809\n",
      "Iteration 93, loss = 0.50947777\n",
      "Iteration 94, loss = 0.50748456\n",
      "Iteration 95, loss = 0.50652804\n",
      "Iteration 96, loss = 0.50605465\n",
      "Iteration 97, loss = 0.50437406\n",
      "Iteration 98, loss = 0.50341255\n",
      "Iteration 99, loss = 0.50301063\n",
      "Iteration 100, loss = 0.50201687\n",
      "Iteration 101, loss = 0.50115979\n",
      "Iteration 102, loss = 0.50029790\n",
      "Iteration 103, loss = 0.49930488\n",
      "Iteration 104, loss = 0.49783353\n",
      "Iteration 105, loss = 0.49781723\n",
      "Iteration 106, loss = 0.49623693\n",
      "Iteration 107, loss = 0.49492955\n",
      "Iteration 108, loss = 0.49415013\n",
      "Iteration 109, loss = 0.49336983\n",
      "Iteration 110, loss = 0.49254430\n",
      "Iteration 111, loss = 0.49200539\n",
      "Iteration 112, loss = 0.49102207\n",
      "Iteration 113, loss = 0.48944482\n",
      "Iteration 114, loss = 0.48919553\n",
      "Iteration 115, loss = 0.48839001\n",
      "Iteration 116, loss = 0.48733269\n",
      "Iteration 117, loss = 0.48729497\n",
      "Iteration 118, loss = 0.48601379\n",
      "Iteration 119, loss = 0.48578219\n",
      "Iteration 120, loss = 0.48395768\n",
      "Iteration 121, loss = 0.48281239\n",
      "Iteration 122, loss = 0.48261484\n",
      "Iteration 123, loss = 0.48184712\n",
      "Iteration 124, loss = 0.48106249\n",
      "Iteration 125, loss = 0.48070065\n",
      "Iteration 126, loss = 0.47954270\n",
      "Iteration 127, loss = 0.47913244\n",
      "Iteration 128, loss = 0.47837420\n",
      "Iteration 129, loss = 0.47694970\n",
      "Iteration 130, loss = 0.47685592\n",
      "Iteration 131, loss = 0.47527626\n",
      "Iteration 132, loss = 0.47465970\n",
      "Iteration 133, loss = 0.47403085\n",
      "Iteration 134, loss = 0.47414076\n",
      "Iteration 135, loss = 0.47251148\n",
      "Iteration 136, loss = 0.47233594\n",
      "Iteration 137, loss = 0.47128336\n",
      "Iteration 138, loss = 0.47114282\n",
      "Iteration 139, loss = 0.47047126\n",
      "Iteration 140, loss = 0.46934549\n",
      "Iteration 141, loss = 0.46899004\n",
      "Iteration 142, loss = 0.46813729\n",
      "Iteration 143, loss = 0.46809574\n",
      "Iteration 144, loss = 0.46680724\n",
      "Iteration 145, loss = 0.46718002\n",
      "Iteration 146, loss = 0.46608511\n",
      "Iteration 147, loss = 0.46470951\n",
      "Iteration 148, loss = 0.46475634\n",
      "Iteration 149, loss = 0.46473393\n",
      "Iteration 150, loss = 0.46266498\n",
      "Iteration 151, loss = 0.46308521\n",
      "Iteration 152, loss = 0.46243048\n",
      "Iteration 153, loss = 0.46179644\n",
      "Iteration 154, loss = 0.46076715\n",
      "Iteration 155, loss = 0.46065440\n",
      "Iteration 156, loss = 0.45950308\n",
      "Iteration 157, loss = 0.45921490\n",
      "Iteration 158, loss = 0.45893781\n",
      "Iteration 159, loss = 0.45860457\n",
      "Iteration 160, loss = 0.45715560\n",
      "Iteration 161, loss = 0.45690417\n",
      "Iteration 162, loss = 0.45672980\n",
      "Iteration 163, loss = 0.45592527\n",
      "Iteration 164, loss = 0.45538122\n",
      "Iteration 165, loss = 0.45507882\n",
      "Iteration 166, loss = 0.45510374\n",
      "Iteration 167, loss = 0.45373704\n",
      "Iteration 168, loss = 0.45368984\n",
      "Iteration 169, loss = 0.45326514\n",
      "Iteration 170, loss = 0.45241259\n",
      "Iteration 171, loss = 0.45252966\n",
      "Iteration 172, loss = 0.45134712\n",
      "Iteration 173, loss = 0.45100856\n",
      "Iteration 174, loss = 0.45102061\n",
      "Iteration 175, loss = 0.45072576\n",
      "Iteration 176, loss = 0.44948862\n",
      "Iteration 177, loss = 0.44877510\n",
      "Iteration 178, loss = 0.44896716\n",
      "Iteration 179, loss = 0.44823699\n",
      "Iteration 180, loss = 0.44745755\n",
      "Iteration 181, loss = 0.44675397\n",
      "Iteration 182, loss = 0.44694791\n",
      "Iteration 183, loss = 0.44597472\n",
      "Iteration 184, loss = 0.44605630\n",
      "Iteration 185, loss = 0.44549840\n",
      "Iteration 186, loss = 0.44548931\n",
      "Iteration 187, loss = 0.44458013\n",
      "Iteration 188, loss = 0.44446848\n",
      "Iteration 189, loss = 0.44382657\n",
      "Iteration 190, loss = 0.44274149\n",
      "Iteration 191, loss = 0.44347643\n",
      "Iteration 192, loss = 0.44272829\n",
      "Iteration 193, loss = 0.44135139\n",
      "Iteration 194, loss = 0.44187579\n",
      "Iteration 195, loss = 0.44158048\n",
      "Iteration 196, loss = 0.44081517\n",
      "Iteration 197, loss = 0.44072499\n",
      "Iteration 198, loss = 0.44028792\n",
      "Iteration 199, loss = 0.43988801\n",
      "Iteration 200, loss = 0.43935742\n",
      "Iteration 201, loss = 0.43890462\n",
      "Iteration 202, loss = 0.43824847\n",
      "Iteration 203, loss = 0.43858752\n",
      "Iteration 204, loss = 0.43742551\n",
      "Iteration 205, loss = 0.43688903\n",
      "Iteration 206, loss = 0.43673367\n",
      "Iteration 207, loss = 0.43583091\n",
      "Iteration 208, loss = 0.43571609\n",
      "Iteration 209, loss = 0.43674576\n",
      "Iteration 210, loss = 0.43564009\n",
      "Iteration 211, loss = 0.43476029\n",
      "Iteration 212, loss = 0.43508203\n",
      "Iteration 213, loss = 0.43377975\n",
      "Iteration 214, loss = 0.43438795\n",
      "Iteration 215, loss = 0.43422855\n",
      "Iteration 216, loss = 0.43268805\n",
      "Iteration 217, loss = 0.43248845\n",
      "Iteration 218, loss = 0.43270769\n",
      "Iteration 219, loss = 0.43245893\n",
      "Iteration 220, loss = 0.43160055\n",
      "Iteration 221, loss = 0.43174462\n",
      "Iteration 222, loss = 0.43110092\n",
      "Iteration 223, loss = 0.43054945\n",
      "Iteration 224, loss = 0.42979865\n",
      "Iteration 225, loss = 0.42955521\n",
      "Iteration 226, loss = 0.42946029\n",
      "Iteration 227, loss = 0.42968269\n",
      "Iteration 228, loss = 0.42911887\n",
      "Iteration 229, loss = 0.42906270\n",
      "Iteration 230, loss = 0.42968144\n",
      "Iteration 231, loss = 0.42770314\n",
      "Iteration 232, loss = 0.42737594\n",
      "Iteration 233, loss = 0.42750602\n",
      "Iteration 234, loss = 0.42737448\n",
      "Iteration 235, loss = 0.42716766\n",
      "Iteration 236, loss = 0.42635435\n",
      "Iteration 237, loss = 0.42673851\n",
      "Iteration 238, loss = 0.42598840\n",
      "Iteration 239, loss = 0.42537748\n",
      "Iteration 240, loss = 0.42574629\n",
      "Iteration 241, loss = 0.42546845\n",
      "Iteration 242, loss = 0.42497296\n",
      "Iteration 243, loss = 0.42388369\n",
      "Iteration 244, loss = 0.42436318\n",
      "Iteration 245, loss = 0.42431343\n",
      "Iteration 246, loss = 0.42356486\n",
      "Iteration 247, loss = 0.42312429\n",
      "Iteration 248, loss = 0.42297778\n",
      "Iteration 249, loss = 0.42265932\n",
      "Iteration 250, loss = 0.42287978\n",
      "Iteration 251, loss = 0.42337573\n",
      "Iteration 252, loss = 0.42208209\n",
      "Iteration 253, loss = 0.42193560\n",
      "Iteration 254, loss = 0.42137493\n",
      "Iteration 255, loss = 0.42110294\n",
      "Iteration 256, loss = 0.42171029\n",
      "Iteration 257, loss = 0.42106398\n",
      "Iteration 258, loss = 0.42108459\n",
      "Iteration 259, loss = 0.42015509\n",
      "Iteration 260, loss = 0.41987283\n",
      "Iteration 261, loss = 0.41966667\n",
      "Iteration 262, loss = 0.41920307\n",
      "Iteration 263, loss = 0.41906676\n",
      "Iteration 264, loss = 0.41918212\n",
      "Iteration 265, loss = 0.41838076\n",
      "Iteration 266, loss = 0.41799069\n",
      "Iteration 267, loss = 0.41825590\n",
      "Iteration 268, loss = 0.41805316\n",
      "Iteration 269, loss = 0.41783908\n",
      "Iteration 270, loss = 0.41761645\n",
      "Iteration 271, loss = 0.41697071\n",
      "Iteration 272, loss = 0.41695385\n",
      "Iteration 273, loss = 0.41614656\n",
      "Iteration 274, loss = 0.41666998\n",
      "Iteration 275, loss = 0.41585408\n",
      "Iteration 276, loss = 0.41594413\n",
      "Iteration 277, loss = 0.41669243\n",
      "Iteration 278, loss = 0.41566332\n",
      "Iteration 279, loss = 0.41625630\n",
      "Iteration 280, loss = 0.41535366\n",
      "Iteration 281, loss = 0.41500870\n",
      "Iteration 282, loss = 0.41443403\n",
      "Iteration 283, loss = 0.41488785\n",
      "Iteration 284, loss = 0.41400932\n",
      "Iteration 285, loss = 0.41392053\n",
      "Iteration 286, loss = 0.41304512\n",
      "Iteration 287, loss = 0.41407026\n",
      "Iteration 288, loss = 0.41277011\n",
      "Iteration 289, loss = 0.41354350\n",
      "Iteration 290, loss = 0.41238780\n",
      "Iteration 291, loss = 0.41275579\n",
      "Iteration 292, loss = 0.41246392\n",
      "Iteration 293, loss = 0.41205344\n",
      "Iteration 294, loss = 0.41215498\n",
      "Iteration 295, loss = 0.41137935\n",
      "Iteration 296, loss = 0.41073125\n",
      "Iteration 297, loss = 0.41162072\n",
      "Iteration 298, loss = 0.41114290\n",
      "Iteration 299, loss = 0.41096748\n",
      "Iteration 300, loss = 0.41091333\n",
      "Iteration 301, loss = 0.41043754\n",
      "Iteration 302, loss = 0.41043624\n",
      "Iteration 303, loss = 0.41082621\n",
      "Iteration 304, loss = 0.41018330\n",
      "Iteration 305, loss = 0.40914060\n",
      "Iteration 306, loss = 0.40926067\n",
      "Iteration 307, loss = 0.40909627\n",
      "Iteration 308, loss = 0.40998138\n",
      "Iteration 309, loss = 0.40840944\n",
      "Iteration 310, loss = 0.40848436\n",
      "Iteration 311, loss = 0.40856485\n",
      "Iteration 312, loss = 0.40820735\n",
      "Iteration 313, loss = 0.40848333\n",
      "Iteration 314, loss = 0.40802209\n",
      "Iteration 315, loss = 0.40742148\n",
      "Iteration 316, loss = 0.40724357\n",
      "Iteration 317, loss = 0.40689473\n",
      "Iteration 318, loss = 0.40680544\n",
      "Iteration 319, loss = 0.40646319\n",
      "Iteration 320, loss = 0.40582992\n",
      "Iteration 321, loss = 0.40614720\n",
      "Iteration 322, loss = 0.40488928\n",
      "Iteration 323, loss = 0.40591851\n",
      "Iteration 324, loss = 0.40610459\n",
      "Iteration 325, loss = 0.40585057\n",
      "Iteration 326, loss = 0.40509078\n",
      "Iteration 327, loss = 0.40469845\n",
      "Iteration 328, loss = 0.40526586\n",
      "Iteration 329, loss = 0.40408727\n",
      "Iteration 330, loss = 0.40433948\n",
      "Iteration 331, loss = 0.40380965\n",
      "Iteration 332, loss = 0.40394323\n",
      "Iteration 333, loss = 0.40390571\n",
      "Iteration 334, loss = 0.40314008\n",
      "Iteration 335, loss = 0.40307227\n",
      "Iteration 336, loss = 0.40382029\n",
      "Iteration 337, loss = 0.40329768\n",
      "Iteration 338, loss = 0.40237270\n",
      "Iteration 339, loss = 0.40287837\n",
      "Iteration 340, loss = 0.40211866\n",
      "Iteration 341, loss = 0.40253209\n",
      "Iteration 342, loss = 0.40239750\n",
      "Iteration 343, loss = 0.40229920\n",
      "Iteration 344, loss = 0.40239293\n",
      "Iteration 345, loss = 0.40148015\n",
      "Iteration 346, loss = 0.40081075\n",
      "Iteration 347, loss = 0.40186649\n",
      "Iteration 348, loss = 0.40110735\n",
      "Iteration 349, loss = 0.40102411\n",
      "Iteration 350, loss = 0.40087261\n",
      "Iteration 351, loss = 0.40001984\n",
      "Iteration 352, loss = 0.40095756\n",
      "Iteration 353, loss = 0.40055009\n",
      "Iteration 354, loss = 0.40060904\n",
      "Iteration 355, loss = 0.39943935\n",
      "Iteration 356, loss = 0.39949343\n",
      "Iteration 357, loss = 0.39996838\n",
      "Iteration 358, loss = 0.39951882\n",
      "Iteration 359, loss = 0.39956162\n",
      "Iteration 360, loss = 0.39862677\n",
      "Iteration 361, loss = 0.39915532\n",
      "Iteration 362, loss = 0.39934158\n",
      "Iteration 363, loss = 0.39851355\n",
      "Iteration 364, loss = 0.39808034\n",
      "Iteration 365, loss = 0.39887147\n",
      "Iteration 366, loss = 0.39769968\n",
      "Iteration 367, loss = 0.39828519\n",
      "Iteration 368, loss = 0.39787632\n",
      "Iteration 369, loss = 0.39764428\n",
      "Iteration 370, loss = 0.39750430\n",
      "Iteration 371, loss = 0.39721635\n",
      "Iteration 372, loss = 0.39802519\n",
      "Iteration 373, loss = 0.39660352\n",
      "Iteration 374, loss = 0.39670871\n",
      "Iteration 375, loss = 0.39695438\n",
      "Iteration 376, loss = 0.39633685\n",
      "Iteration 377, loss = 0.39616343\n",
      "Iteration 378, loss = 0.39637216\n",
      "Iteration 379, loss = 0.39682642\n",
      "Iteration 380, loss = 0.39630093\n",
      "Iteration 381, loss = 0.39558797\n",
      "Iteration 382, loss = 0.39596110\n",
      "Iteration 383, loss = 0.39532895\n",
      "Iteration 384, loss = 0.39533015\n",
      "Iteration 385, loss = 0.39520368\n",
      "Iteration 386, loss = 0.39546218\n",
      "Iteration 387, loss = 0.39536785\n",
      "Iteration 388, loss = 0.39471776\n",
      "Iteration 389, loss = 0.39507525\n",
      "Iteration 390, loss = 0.39507249\n",
      "Iteration 391, loss = 0.39437177\n",
      "Iteration 392, loss = 0.39441125\n",
      "Iteration 393, loss = 0.39417897\n",
      "Iteration 394, loss = 0.39385986\n",
      "Iteration 395, loss = 0.39405711\n",
      "Iteration 396, loss = 0.39383927\n",
      "Iteration 397, loss = 0.39327840\n",
      "Iteration 398, loss = 0.39290998\n",
      "Iteration 399, loss = 0.39388132\n",
      "Iteration 400, loss = 0.39388572\n",
      "Iteration 401, loss = 0.39358276\n",
      "Iteration 402, loss = 0.39307375\n",
      "Iteration 403, loss = 0.39332417\n",
      "Iteration 404, loss = 0.39288122\n",
      "Iteration 405, loss = 0.39235727\n",
      "Iteration 406, loss = 0.39239702\n",
      "Iteration 407, loss = 0.39229555\n",
      "Iteration 408, loss = 0.39296405\n",
      "Iteration 409, loss = 0.39201602\n",
      "Iteration 410, loss = 0.39315424\n",
      "Iteration 411, loss = 0.39175510\n",
      "Iteration 412, loss = 0.39214859\n",
      "Iteration 413, loss = 0.39129477\n",
      "Iteration 414, loss = 0.39139903\n",
      "Iteration 415, loss = 0.39158496\n",
      "Iteration 416, loss = 0.39105118\n",
      "Iteration 417, loss = 0.39144283\n",
      "Iteration 418, loss = 0.39118349\n",
      "Iteration 419, loss = 0.39097142\n",
      "Iteration 420, loss = 0.39048171\n",
      "Iteration 421, loss = 0.39091763\n",
      "Iteration 422, loss = 0.39108358\n",
      "Iteration 423, loss = 0.39025841\n",
      "Iteration 424, loss = 0.39081594\n",
      "Iteration 425, loss = 0.38991409\n",
      "Iteration 426, loss = 0.38982704\n",
      "Iteration 427, loss = 0.39009989\n",
      "Iteration 428, loss = 0.38971213\n",
      "Iteration 429, loss = 0.38943000\n",
      "Iteration 430, loss = 0.38981805\n",
      "Iteration 431, loss = 0.39006718\n",
      "Iteration 432, loss = 0.38920126\n",
      "Iteration 433, loss = 0.38931491\n",
      "Iteration 434, loss = 0.38883623\n",
      "Iteration 435, loss = 0.38987015\n",
      "Iteration 436, loss = 0.38905276\n",
      "Iteration 437, loss = 0.38884338\n",
      "Iteration 438, loss = 0.38900906\n",
      "Iteration 439, loss = 0.38888122\n",
      "Iteration 440, loss = 0.38896322\n",
      "Iteration 441, loss = 0.38896204\n",
      "Iteration 442, loss = 0.38858619\n",
      "Iteration 443, loss = 0.38796626\n",
      "Iteration 444, loss = 0.38852798\n",
      "Iteration 445, loss = 0.38854274\n",
      "Iteration 446, loss = 0.38862770\n",
      "Iteration 447, loss = 0.38789421\n",
      "Iteration 448, loss = 0.38748065\n",
      "Iteration 449, loss = 0.38779074\n",
      "Iteration 450, loss = 0.38845188\n",
      "Iteration 451, loss = 0.38762190\n",
      "Iteration 452, loss = 0.38745941\n",
      "Iteration 453, loss = 0.38780003\n",
      "Iteration 454, loss = 0.38751499\n",
      "Iteration 455, loss = 0.38657365\n",
      "Iteration 456, loss = 0.38691031\n",
      "Iteration 457, loss = 0.38664931\n",
      "Iteration 458, loss = 0.38716848\n",
      "Iteration 459, loss = 0.38623376\n",
      "Iteration 460, loss = 0.38754950\n",
      "Iteration 461, loss = 0.38673192\n",
      "Iteration 462, loss = 0.38669212\n",
      "Iteration 463, loss = 0.38650705\n",
      "Iteration 464, loss = 0.38663737\n",
      "Iteration 465, loss = 0.38637679\n",
      "Iteration 466, loss = 0.38625183\n",
      "Iteration 467, loss = 0.38639334\n",
      "Iteration 468, loss = 0.38716514\n",
      "Iteration 469, loss = 0.38622565\n",
      "Iteration 470, loss = 0.38584973\n",
      "Iteration 471, loss = 0.38631317\n",
      "Iteration 472, loss = 0.38563140\n",
      "Iteration 473, loss = 0.38570838\n",
      "Iteration 474, loss = 0.38471162\n",
      "Iteration 475, loss = 0.38482316\n",
      "Iteration 476, loss = 0.38530155\n",
      "Iteration 477, loss = 0.38495114\n",
      "Iteration 478, loss = 0.38520664\n",
      "Iteration 479, loss = 0.38518190\n",
      "Iteration 480, loss = 0.38483790\n",
      "Iteration 481, loss = 0.38438228\n",
      "Iteration 482, loss = 0.38410160\n",
      "Iteration 483, loss = 0.38467711\n",
      "Iteration 484, loss = 0.38448002\n",
      "Iteration 485, loss = 0.38425668\n",
      "Iteration 486, loss = 0.38360405\n",
      "Iteration 487, loss = 0.38397527\n",
      "Iteration 488, loss = 0.38338715\n",
      "Iteration 489, loss = 0.38384297\n",
      "Iteration 490, loss = 0.38386005\n",
      "Iteration 491, loss = 0.38330760\n",
      "Iteration 492, loss = 0.38303485\n",
      "Iteration 493, loss = 0.38329605\n",
      "Iteration 494, loss = 0.38361935\n",
      "Iteration 495, loss = 0.38288304\n",
      "Iteration 496, loss = 0.38285296\n",
      "Iteration 497, loss = 0.38356536\n",
      "Iteration 498, loss = 0.38179859\n",
      "Iteration 499, loss = 0.38283724\n",
      "Iteration 500, loss = 0.38245276\n",
      "Iteration 501, loss = 0.38275690\n",
      "Iteration 502, loss = 0.38265945\n",
      "Iteration 503, loss = 0.38163092\n",
      "Iteration 504, loss = 0.38250138\n",
      "Iteration 505, loss = 0.38122301\n",
      "Iteration 506, loss = 0.38104517\n",
      "Iteration 507, loss = 0.38216677\n",
      "Iteration 508, loss = 0.38129424\n",
      "Iteration 509, loss = 0.38161784\n",
      "Iteration 510, loss = 0.38178327\n",
      "Iteration 511, loss = 0.38126460\n",
      "Iteration 512, loss = 0.38025153\n",
      "Iteration 513, loss = 0.38041010\n",
      "Iteration 514, loss = 0.38091589\n",
      "Iteration 515, loss = 0.38155764\n",
      "Iteration 516, loss = 0.38069641\n",
      "Iteration 517, loss = 0.38070163\n",
      "Iteration 518, loss = 0.38047461\n",
      "Iteration 519, loss = 0.38101233\n",
      "Iteration 520, loss = 0.38012025\n",
      "Iteration 521, loss = 0.37966346\n",
      "Iteration 522, loss = 0.38014251\n",
      "Iteration 523, loss = 0.37998892\n",
      "Iteration 524, loss = 0.37977009\n",
      "Iteration 525, loss = 0.37946999\n",
      "Iteration 526, loss = 0.37903220\n",
      "Iteration 527, loss = 0.37948087\n",
      "Iteration 528, loss = 0.38006868\n",
      "Iteration 529, loss = 0.37936861\n",
      "Iteration 530, loss = 0.37954554\n",
      "Iteration 531, loss = 0.37871846\n",
      "Iteration 532, loss = 0.37918441\n",
      "Iteration 533, loss = 0.37831742\n",
      "Iteration 534, loss = 0.37841494\n",
      "Iteration 535, loss = 0.37942988\n",
      "Iteration 536, loss = 0.37854321\n",
      "Iteration 537, loss = 0.37819772\n",
      "Iteration 538, loss = 0.37857474\n",
      "Iteration 539, loss = 0.37876246\n",
      "Iteration 540, loss = 0.37752379\n",
      "Iteration 541, loss = 0.37824956\n",
      "Iteration 542, loss = 0.37790818\n",
      "Iteration 543, loss = 0.37763863\n",
      "Iteration 544, loss = 0.37809697\n",
      "Iteration 545, loss = 0.37790888\n",
      "Iteration 546, loss = 0.37780037\n",
      "Iteration 547, loss = 0.37790924\n",
      "Iteration 548, loss = 0.37716473\n",
      "Iteration 549, loss = 0.37780287\n",
      "Iteration 550, loss = 0.37738239\n",
      "Iteration 551, loss = 0.37753107\n",
      "Iteration 552, loss = 0.37686857\n",
      "Iteration 553, loss = 0.37693150\n",
      "Iteration 554, loss = 0.37690085\n",
      "Iteration 555, loss = 0.37667181\n",
      "Iteration 556, loss = 0.37708078\n",
      "Iteration 557, loss = 0.37686770\n",
      "Iteration 558, loss = 0.37614765\n",
      "Iteration 559, loss = 0.37605257\n",
      "Iteration 560, loss = 0.37599408\n",
      "Iteration 561, loss = 0.37592259\n",
      "Iteration 562, loss = 0.37595945\n",
      "Iteration 563, loss = 0.37661704\n",
      "Iteration 564, loss = 0.37606062\n",
      "Iteration 565, loss = 0.37522441\n",
      "Iteration 566, loss = 0.37563340\n",
      "Iteration 567, loss = 0.37573190\n",
      "Iteration 568, loss = 0.37499480\n",
      "Iteration 569, loss = 0.37516775\n",
      "Iteration 570, loss = 0.37545137\n",
      "Iteration 571, loss = 0.37570920\n",
      "Iteration 572, loss = 0.37453998\n",
      "Iteration 573, loss = 0.37437258\n",
      "Iteration 574, loss = 0.37518209\n",
      "Iteration 575, loss = 0.37459473\n",
      "Iteration 576, loss = 0.37529445\n",
      "Iteration 577, loss = 0.37453268\n",
      "Iteration 578, loss = 0.37403057\n",
      "Iteration 579, loss = 0.37380745\n",
      "Iteration 580, loss = 0.37350882\n",
      "Iteration 581, loss = 0.37295333\n",
      "Iteration 582, loss = 0.37359301\n",
      "Iteration 583, loss = 0.37459722\n",
      "Iteration 584, loss = 0.37333196\n",
      "Iteration 585, loss = 0.37335131\n",
      "Iteration 586, loss = 0.37333448\n",
      "Iteration 587, loss = 0.37278454\n",
      "Iteration 588, loss = 0.37341677\n",
      "Iteration 589, loss = 0.37332199\n",
      "Iteration 590, loss = 0.37312590\n",
      "Iteration 591, loss = 0.37267674\n",
      "Iteration 592, loss = 0.37282806\n",
      "Iteration 593, loss = 0.37250429\n",
      "Iteration 594, loss = 0.37242419\n",
      "Iteration 595, loss = 0.37186247\n",
      "Iteration 596, loss = 0.37214018\n",
      "Iteration 597, loss = 0.37208164\n",
      "Iteration 598, loss = 0.37184974\n",
      "Iteration 599, loss = 0.37190496\n",
      "Iteration 600, loss = 0.37214525\n",
      "Iteration 601, loss = 0.37203503\n",
      "Iteration 602, loss = 0.37154880\n",
      "Iteration 603, loss = 0.37150819\n",
      "Iteration 604, loss = 0.37112068\n",
      "Iteration 605, loss = 0.37163563\n",
      "Iteration 606, loss = 0.37135706\n",
      "Iteration 607, loss = 0.37134949\n",
      "Iteration 608, loss = 0.37093409\n",
      "Iteration 609, loss = 0.37028927\n",
      "Iteration 610, loss = 0.37157755\n",
      "Iteration 611, loss = 0.37022096\n",
      "Iteration 612, loss = 0.37013231\n",
      "Iteration 613, loss = 0.37033037\n",
      "Iteration 614, loss = 0.37025029\n",
      "Iteration 615, loss = 0.37079280\n",
      "Iteration 616, loss = 0.37074220\n",
      "Iteration 617, loss = 0.37013099\n",
      "Iteration 618, loss = 0.37066359\n",
      "Iteration 619, loss = 0.36966949\n",
      "Iteration 620, loss = 0.36965523\n",
      "Iteration 621, loss = 0.37024194\n",
      "Iteration 622, loss = 0.36964172\n",
      "Iteration 623, loss = 0.36961002\n",
      "Iteration 624, loss = 0.37057046\n",
      "Iteration 625, loss = 0.36977279\n",
      "Iteration 626, loss = 0.36987158\n",
      "Iteration 627, loss = 0.36916414\n",
      "Iteration 628, loss = 0.36949149\n",
      "Iteration 629, loss = 0.36887466\n",
      "Iteration 630, loss = 0.36936560\n",
      "Iteration 631, loss = 0.36959138\n",
      "Iteration 632, loss = 0.36891620\n",
      "Iteration 633, loss = 0.36921287\n",
      "Iteration 634, loss = 0.36895918\n",
      "Iteration 635, loss = 0.36883024\n",
      "Iteration 636, loss = 0.36885832\n",
      "Iteration 637, loss = 0.36850775\n",
      "Iteration 638, loss = 0.36926616\n",
      "Iteration 639, loss = 0.36864922\n",
      "Iteration 640, loss = 0.36855781\n",
      "Iteration 641, loss = 0.36829373\n",
      "Iteration 642, loss = 0.36879117\n",
      "Iteration 643, loss = 0.36880954\n",
      "Iteration 644, loss = 0.36812791\n",
      "Iteration 645, loss = 0.36754230\n",
      "Iteration 646, loss = 0.36797185\n",
      "Iteration 647, loss = 0.36799514\n",
      "Iteration 648, loss = 0.36713123\n",
      "Iteration 649, loss = 0.36887362\n",
      "Iteration 650, loss = 0.36783150\n",
      "Iteration 651, loss = 0.36738688\n",
      "Iteration 652, loss = 0.36786362\n",
      "Iteration 653, loss = 0.36806998\n",
      "Iteration 654, loss = 0.36723280\n",
      "Iteration 655, loss = 0.36689594\n",
      "Iteration 656, loss = 0.36675302\n",
      "Iteration 657, loss = 0.36723811\n",
      "Iteration 658, loss = 0.36661920\n",
      "Iteration 659, loss = 0.36673226\n",
      "Iteration 660, loss = 0.36725373\n",
      "Iteration 661, loss = 0.36682079\n",
      "Iteration 662, loss = 0.36775482\n",
      "Iteration 663, loss = 0.36706171\n",
      "Iteration 664, loss = 0.36677664\n",
      "Iteration 665, loss = 0.36599832\n",
      "Iteration 666, loss = 0.36617347\n",
      "Iteration 667, loss = 0.36641848\n",
      "Iteration 668, loss = 0.36665822\n",
      "Iteration 669, loss = 0.36680082\n",
      "Iteration 670, loss = 0.36693984\n",
      "Iteration 671, loss = 0.36664781\n",
      "Iteration 672, loss = 0.36601162\n",
      "Iteration 673, loss = 0.36600773\n",
      "Iteration 674, loss = 0.36594442\n",
      "Iteration 675, loss = 0.36625789\n",
      "Iteration 676, loss = 0.36572914\n",
      "Iteration 677, loss = 0.36594600\n",
      "Iteration 678, loss = 0.36564632\n",
      "Iteration 679, loss = 0.36588369\n",
      "Iteration 680, loss = 0.36592477\n",
      "Iteration 681, loss = 0.36569280\n",
      "Iteration 682, loss = 0.36534851\n",
      "Iteration 683, loss = 0.36596572\n",
      "Iteration 684, loss = 0.36536145\n",
      "Iteration 685, loss = 0.36599540\n",
      "Iteration 686, loss = 0.36548183\n",
      "Iteration 687, loss = 0.36493451\n",
      "Iteration 688, loss = 0.36509729\n",
      "Iteration 689, loss = 0.36561493\n",
      "Iteration 690, loss = 0.36529856\n",
      "Iteration 691, loss = 0.36535277\n",
      "Iteration 692, loss = 0.36507121\n",
      "Iteration 693, loss = 0.36434449\n",
      "Iteration 694, loss = 0.36497377\n",
      "Iteration 695, loss = 0.36494159\n",
      "Iteration 696, loss = 0.36474161\n",
      "Iteration 697, loss = 0.36461534\n",
      "Iteration 698, loss = 0.36446249\n",
      "Iteration 699, loss = 0.36503564\n",
      "Iteration 700, loss = 0.36463356\n",
      "Iteration 701, loss = 0.36448978\n",
      "Iteration 702, loss = 0.36425135\n",
      "Iteration 703, loss = 0.36480866\n",
      "Iteration 704, loss = 0.36465762\n",
      "Iteration 705, loss = 0.36455157\n",
      "Iteration 706, loss = 0.36458442\n",
      "Iteration 707, loss = 0.36401453\n",
      "Iteration 708, loss = 0.36362000\n",
      "Iteration 709, loss = 0.36415806\n",
      "Iteration 710, loss = 0.36359042\n",
      "Iteration 711, loss = 0.36319429\n",
      "Iteration 712, loss = 0.36373790\n",
      "Iteration 713, loss = 0.36386790\n",
      "Iteration 714, loss = 0.36356321\n",
      "Iteration 715, loss = 0.36321366\n",
      "Iteration 716, loss = 0.36382955\n",
      "Iteration 717, loss = 0.36398252\n",
      "Iteration 718, loss = 0.36291121\n",
      "Iteration 719, loss = 0.36285844\n",
      "Iteration 720, loss = 0.36371674\n",
      "Iteration 721, loss = 0.36359384\n",
      "Iteration 722, loss = 0.36320095\n",
      "Iteration 723, loss = 0.36339010\n",
      "Iteration 724, loss = 0.36285604\n",
      "Iteration 725, loss = 0.36250300\n",
      "Iteration 726, loss = 0.36281040\n",
      "Iteration 727, loss = 0.36262297\n",
      "Iteration 728, loss = 0.36261791\n",
      "Iteration 729, loss = 0.36234833\n",
      "Iteration 730, loss = 0.36187357\n",
      "Iteration 731, loss = 0.36180187\n",
      "Iteration 732, loss = 0.36238720\n",
      "Iteration 733, loss = 0.36178833\n",
      "Iteration 734, loss = 0.36159930\n",
      "Iteration 735, loss = 0.36127434\n",
      "Iteration 736, loss = 0.36183465\n",
      "Iteration 737, loss = 0.36133544\n",
      "Iteration 738, loss = 0.36175864\n",
      "Iteration 739, loss = 0.36112002\n",
      "Iteration 740, loss = 0.36159049\n",
      "Iteration 741, loss = 0.36106761\n",
      "Iteration 742, loss = 0.36092622\n",
      "Iteration 743, loss = 0.36168893\n",
      "Iteration 744, loss = 0.36138084\n",
      "Iteration 745, loss = 0.36013042\n",
      "Iteration 746, loss = 0.36070653\n",
      "Iteration 747, loss = 0.36044369\n",
      "Iteration 748, loss = 0.36005096\n",
      "Iteration 749, loss = 0.36001271\n",
      "Iteration 750, loss = 0.36088531\n",
      "Iteration 751, loss = 0.35994107\n",
      "Iteration 752, loss = 0.35950276\n",
      "Iteration 753, loss = 0.35948664\n",
      "Iteration 754, loss = 0.35944214\n",
      "Iteration 755, loss = 0.35893132\n",
      "Iteration 756, loss = 0.36017666\n",
      "Iteration 757, loss = 0.35956004\n",
      "Iteration 758, loss = 0.35923979\n",
      "Iteration 759, loss = 0.35894407\n",
      "Iteration 760, loss = 0.35883090\n",
      "Iteration 761, loss = 0.35892085\n",
      "Iteration 762, loss = 0.35917709\n",
      "Iteration 763, loss = 0.35869796\n",
      "Iteration 764, loss = 0.35872504\n",
      "Iteration 765, loss = 0.35893678\n",
      "Iteration 766, loss = 0.35882289\n",
      "Iteration 767, loss = 0.35858130\n",
      "Iteration 768, loss = 0.35788514\n",
      "Iteration 769, loss = 0.35917442\n",
      "Iteration 770, loss = 0.35886971\n",
      "Iteration 771, loss = 0.35729779\n",
      "Iteration 772, loss = 0.35811102\n",
      "Iteration 773, loss = 0.35811849\n",
      "Iteration 774, loss = 0.35815304\n",
      "Iteration 775, loss = 0.35783042\n",
      "Iteration 776, loss = 0.35774990\n",
      "Iteration 777, loss = 0.35765192\n",
      "Iteration 778, loss = 0.35766538\n",
      "Iteration 779, loss = 0.35756796\n",
      "Iteration 780, loss = 0.35685255\n",
      "Iteration 781, loss = 0.35739866\n",
      "Iteration 782, loss = 0.35753233\n",
      "Iteration 783, loss = 0.35668079\n",
      "Iteration 784, loss = 0.35725377\n",
      "Iteration 785, loss = 0.35693560\n",
      "Iteration 786, loss = 0.35690344\n",
      "Iteration 787, loss = 0.35669419\n",
      "Iteration 788, loss = 0.35626088\n",
      "Iteration 789, loss = 0.35705524\n",
      "Iteration 790, loss = 0.35641221\n",
      "Iteration 791, loss = 0.35600780\n",
      "Iteration 792, loss = 0.35689586\n",
      "Iteration 793, loss = 0.35635824\n",
      "Iteration 794, loss = 0.35603381\n",
      "Iteration 795, loss = 0.35647970\n",
      "Iteration 796, loss = 0.35640642\n",
      "Iteration 797, loss = 0.35622740\n",
      "Iteration 798, loss = 0.35607672\n",
      "Iteration 799, loss = 0.35655927\n",
      "Iteration 800, loss = 0.35660799\n",
      "Iteration 801, loss = 0.35554706\n",
      "Iteration 802, loss = 0.35572741\n",
      "Iteration 803, loss = 0.35560987\n",
      "Iteration 804, loss = 0.35571479\n",
      "Iteration 805, loss = 0.35584013\n",
      "Iteration 806, loss = 0.35497739\n",
      "Iteration 807, loss = 0.35574789\n",
      "Iteration 808, loss = 0.35558075\n",
      "Iteration 809, loss = 0.35528136\n",
      "Iteration 810, loss = 0.35549583\n",
      "Iteration 811, loss = 0.35454697\n",
      "Iteration 812, loss = 0.35479670\n",
      "Iteration 813, loss = 0.35495066\n",
      "Iteration 814, loss = 0.35503504\n",
      "Iteration 815, loss = 0.35456439\n",
      "Iteration 816, loss = 0.35430585\n",
      "Iteration 817, loss = 0.35528163\n",
      "Iteration 818, loss = 0.35464882\n",
      "Iteration 819, loss = 0.35498026\n",
      "Iteration 820, loss = 0.35407316\n",
      "Iteration 821, loss = 0.35426656\n",
      "Iteration 822, loss = 0.35448785\n",
      "Iteration 823, loss = 0.35560215\n",
      "Iteration 824, loss = 0.35434095\n",
      "Iteration 825, loss = 0.35524506\n",
      "Iteration 826, loss = 0.35439220\n",
      "Iteration 827, loss = 0.35386869\n",
      "Iteration 828, loss = 0.35408785\n",
      "Iteration 829, loss = 0.35529210\n",
      "Iteration 830, loss = 0.35396169\n",
      "Iteration 831, loss = 0.35413611\n",
      "Iteration 832, loss = 0.35454935\n",
      "Iteration 833, loss = 0.35366362\n",
      "Iteration 834, loss = 0.35352839\n",
      "Iteration 835, loss = 0.35464716\n",
      "Iteration 836, loss = 0.35390131\n",
      "Iteration 837, loss = 0.35379210\n",
      "Iteration 838, loss = 0.35361269\n",
      "Iteration 839, loss = 0.35329561\n",
      "Iteration 840, loss = 0.35337597\n",
      "Iteration 841, loss = 0.35374329\n",
      "Iteration 842, loss = 0.35389595\n",
      "Iteration 843, loss = 0.35292768\n",
      "Iteration 844, loss = 0.35284599\n",
      "Iteration 845, loss = 0.35196660\n",
      "Iteration 846, loss = 0.35302402\n",
      "Iteration 847, loss = 0.35275656\n",
      "Iteration 848, loss = 0.35280021\n",
      "Iteration 849, loss = 0.35209842\n",
      "Iteration 850, loss = 0.35236496\n",
      "Iteration 851, loss = 0.35248076\n",
      "Iteration 852, loss = 0.35261522\n",
      "Iteration 853, loss = 0.35245395\n",
      "Iteration 854, loss = 0.35265072\n",
      "Iteration 855, loss = 0.35202925\n",
      "Iteration 856, loss = 0.35186092\n",
      "Iteration 857, loss = 0.35142494\n",
      "Iteration 858, loss = 0.35155874\n",
      "Iteration 859, loss = 0.35177297\n",
      "Iteration 860, loss = 0.35168655\n",
      "Iteration 861, loss = 0.35157927\n",
      "Iteration 862, loss = 0.35200024\n",
      "Iteration 863, loss = 0.35169141\n",
      "Iteration 864, loss = 0.35091714\n",
      "Iteration 865, loss = 0.35057378\n",
      "Iteration 866, loss = 0.35078475\n",
      "Iteration 867, loss = 0.35106980\n",
      "Iteration 868, loss = 0.35102684\n",
      "Iteration 869, loss = 0.35133277\n",
      "Iteration 870, loss = 0.35023997\n",
      "Iteration 871, loss = 0.34978835\n",
      "Iteration 872, loss = 0.34997598\n",
      "Iteration 873, loss = 0.35050071\n",
      "Iteration 874, loss = 0.34989002\n",
      "Iteration 875, loss = 0.34978720\n",
      "Iteration 876, loss = 0.34980387\n",
      "Iteration 877, loss = 0.34982767\n",
      "Iteration 878, loss = 0.34919763\n",
      "Iteration 879, loss = 0.34983172\n",
      "Iteration 880, loss = 0.34939444\n",
      "Iteration 881, loss = 0.34908185\n",
      "Iteration 882, loss = 0.35001320\n",
      "Iteration 883, loss = 0.34863077\n",
      "Iteration 884, loss = 0.34944953\n",
      "Iteration 885, loss = 0.34863983\n",
      "Iteration 886, loss = 0.34940336\n",
      "Iteration 887, loss = 0.34869183\n",
      "Iteration 888, loss = 0.34835209\n",
      "Iteration 889, loss = 0.34821815\n",
      "Iteration 890, loss = 0.34865256\n",
      "Iteration 891, loss = 0.34888209\n",
      "Iteration 892, loss = 0.34834360\n",
      "Iteration 893, loss = 0.34769044\n",
      "Iteration 894, loss = 0.34708902\n",
      "Iteration 895, loss = 0.34781848\n",
      "Iteration 896, loss = 0.34793337\n",
      "Iteration 897, loss = 0.34756806\n",
      "Iteration 898, loss = 0.34784476\n",
      "Iteration 899, loss = 0.34784107\n",
      "Iteration 900, loss = 0.34749297\n",
      "Iteration 901, loss = 0.34720797\n",
      "Iteration 902, loss = 0.34732738\n",
      "Iteration 903, loss = 0.34746736\n",
      "Iteration 904, loss = 0.34773989\n",
      "Iteration 905, loss = 0.34703123\n",
      "Iteration 906, loss = 0.34662590\n",
      "Iteration 907, loss = 0.34716015\n",
      "Iteration 908, loss = 0.34731505\n",
      "Iteration 909, loss = 0.34661820\n",
      "Iteration 910, loss = 0.34673515\n",
      "Iteration 911, loss = 0.34603650\n",
      "Iteration 912, loss = 0.34654498\n",
      "Iteration 913, loss = 0.34685706\n",
      "Iteration 914, loss = 0.34618837\n",
      "Iteration 915, loss = 0.34615728\n",
      "Iteration 916, loss = 0.34695023\n",
      "Iteration 917, loss = 0.34601685\n",
      "Iteration 918, loss = 0.34613773\n",
      "Iteration 919, loss = 0.34583468\n",
      "Iteration 920, loss = 0.34601910\n",
      "Iteration 921, loss = 0.34625779\n",
      "Iteration 922, loss = 0.34564520\n",
      "Iteration 923, loss = 0.34580974\n",
      "Iteration 924, loss = 0.34639279\n",
      "Iteration 925, loss = 0.34574941\n",
      "Iteration 926, loss = 0.34469395\n",
      "Iteration 927, loss = 0.34548756\n",
      "Iteration 928, loss = 0.34555384\n",
      "Iteration 929, loss = 0.34562686\n",
      "Iteration 930, loss = 0.34574593\n",
      "Iteration 931, loss = 0.34469031\n",
      "Iteration 932, loss = 0.34541458\n",
      "Iteration 933, loss = 0.34552515\n",
      "Iteration 934, loss = 0.34470059\n",
      "Iteration 935, loss = 0.34481404\n",
      "Iteration 936, loss = 0.34532562\n",
      "Iteration 937, loss = 0.34483867\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66962208\n",
      "Iteration 2, loss = 0.65371319\n",
      "Iteration 3, loss = 0.65077089\n",
      "Iteration 4, loss = 0.65028911\n",
      "Iteration 5, loss = 0.64584676\n",
      "Iteration 6, loss = 0.64466511\n",
      "Iteration 7, loss = 0.64395786\n",
      "Iteration 8, loss = 0.64055024\n",
      "Iteration 9, loss = 0.63768105\n",
      "Iteration 10, loss = 0.63573707\n",
      "Iteration 11, loss = 0.63374625\n",
      "Iteration 12, loss = 0.63129756\n",
      "Iteration 13, loss = 0.62945076\n",
      "Iteration 14, loss = 0.62695186\n",
      "Iteration 15, loss = 0.62452133\n",
      "Iteration 16, loss = 0.62276409\n",
      "Iteration 17, loss = 0.62017856\n",
      "Iteration 18, loss = 0.61832774\n",
      "Iteration 19, loss = 0.61587542\n",
      "Iteration 20, loss = 0.61454857\n",
      "Iteration 21, loss = 0.61306768\n",
      "Iteration 22, loss = 0.61062052\n",
      "Iteration 23, loss = 0.60954578\n",
      "Iteration 24, loss = 0.60645140\n",
      "Iteration 25, loss = 0.60483524\n",
      "Iteration 26, loss = 0.60273060\n",
      "Iteration 27, loss = 0.60008759\n",
      "Iteration 28, loss = 0.60005692\n",
      "Iteration 29, loss = 0.59674926\n",
      "Iteration 30, loss = 0.59472561\n",
      "Iteration 31, loss = 0.59404382\n",
      "Iteration 32, loss = 0.59242887\n",
      "Iteration 33, loss = 0.59067191\n",
      "Iteration 34, loss = 0.58924085\n",
      "Iteration 35, loss = 0.58758016\n",
      "Iteration 36, loss = 0.58503939\n",
      "Iteration 37, loss = 0.58455535\n",
      "Iteration 38, loss = 0.58326131\n",
      "Iteration 39, loss = 0.58115500\n",
      "Iteration 40, loss = 0.58076050\n",
      "Iteration 41, loss = 0.57897040\n",
      "Iteration 42, loss = 0.57704299\n",
      "Iteration 43, loss = 0.57560391\n",
      "Iteration 44, loss = 0.57494663\n",
      "Iteration 45, loss = 0.57334235\n",
      "Iteration 46, loss = 0.57155078\n",
      "Iteration 47, loss = 0.57100635\n",
      "Iteration 48, loss = 0.56973209\n",
      "Iteration 49, loss = 0.56840419\n",
      "Iteration 50, loss = 0.56760451\n",
      "Iteration 51, loss = 0.56633118\n",
      "Iteration 52, loss = 0.56480719\n",
      "Iteration 53, loss = 0.56333616\n",
      "Iteration 54, loss = 0.56205829\n",
      "Iteration 55, loss = 0.56217735\n",
      "Iteration 56, loss = 0.56054929\n",
      "Iteration 57, loss = 0.55965984\n",
      "Iteration 58, loss = 0.55814489\n",
      "Iteration 59, loss = 0.55814099\n",
      "Iteration 60, loss = 0.55709378\n",
      "Iteration 61, loss = 0.55644474\n",
      "Iteration 62, loss = 0.55402830\n",
      "Iteration 63, loss = 0.55484477\n",
      "Iteration 64, loss = 0.55314391\n",
      "Iteration 65, loss = 0.55251725\n",
      "Iteration 66, loss = 0.55190254\n",
      "Iteration 67, loss = 0.55138713\n",
      "Iteration 68, loss = 0.54957377\n",
      "Iteration 69, loss = 0.54902439\n",
      "Iteration 70, loss = 0.54842346\n",
      "Iteration 71, loss = 0.54950596\n",
      "Iteration 72, loss = 0.54708294\n",
      "Iteration 73, loss = 0.54619470\n",
      "Iteration 74, loss = 0.54548341\n",
      "Iteration 75, loss = 0.54550735\n",
      "Iteration 76, loss = 0.54354205\n",
      "Iteration 77, loss = 0.54438428\n",
      "Iteration 78, loss = 0.54195759\n",
      "Iteration 79, loss = 0.54290030\n",
      "Iteration 80, loss = 0.54186778\n",
      "Iteration 81, loss = 0.54135764\n",
      "Iteration 82, loss = 0.53997602\n",
      "Iteration 83, loss = 0.54116708\n",
      "Iteration 84, loss = 0.53899955\n",
      "Iteration 85, loss = 0.53839355\n",
      "Iteration 86, loss = 0.53765019\n",
      "Iteration 87, loss = 0.53765705\n",
      "Iteration 88, loss = 0.53607524\n",
      "Iteration 89, loss = 0.53616092\n",
      "Iteration 90, loss = 0.53521439\n",
      "Iteration 91, loss = 0.53528867\n",
      "Iteration 92, loss = 0.53372184\n",
      "Iteration 93, loss = 0.53472578\n",
      "Iteration 94, loss = 0.53255521\n",
      "Iteration 95, loss = 0.53247269\n",
      "Iteration 96, loss = 0.53131283\n",
      "Iteration 97, loss = 0.53102180\n",
      "Iteration 98, loss = 0.53038064\n",
      "Iteration 99, loss = 0.52975990\n",
      "Iteration 100, loss = 0.52919912\n",
      "Iteration 101, loss = 0.52822650\n",
      "Iteration 102, loss = 0.53105341\n",
      "Iteration 103, loss = 0.52813775\n",
      "Iteration 104, loss = 0.52685354\n",
      "Iteration 105, loss = 0.52681904\n",
      "Iteration 106, loss = 0.52455856\n",
      "Iteration 107, loss = 0.52508698\n",
      "Iteration 108, loss = 0.52550191\n",
      "Iteration 109, loss = 0.52490913\n",
      "Iteration 110, loss = 0.52415798\n",
      "Iteration 111, loss = 0.52248366\n",
      "Iteration 112, loss = 0.52215510\n",
      "Iteration 113, loss = 0.52197876\n",
      "Iteration 114, loss = 0.52106224\n",
      "Iteration 115, loss = 0.52047833\n",
      "Iteration 116, loss = 0.52042256\n",
      "Iteration 117, loss = 0.52033910\n",
      "Iteration 118, loss = 0.51956757\n",
      "Iteration 119, loss = 0.51870490\n",
      "Iteration 120, loss = 0.51765731\n",
      "Iteration 121, loss = 0.51765135\n",
      "Iteration 122, loss = 0.51657027\n",
      "Iteration 123, loss = 0.51877040\n",
      "Iteration 124, loss = 0.51674883\n",
      "Iteration 125, loss = 0.51594625\n",
      "Iteration 126, loss = 0.51562740\n",
      "Iteration 127, loss = 0.51545804\n",
      "Iteration 128, loss = 0.51467287\n",
      "Iteration 129, loss = 0.51330166\n",
      "Iteration 130, loss = 0.51215204\n",
      "Iteration 131, loss = 0.51216072\n",
      "Iteration 132, loss = 0.51327217\n",
      "Iteration 133, loss = 0.51183942\n",
      "Iteration 134, loss = 0.51224006\n",
      "Iteration 135, loss = 0.51275848\n",
      "Iteration 136, loss = 0.50956900\n",
      "Iteration 137, loss = 0.50939263\n",
      "Iteration 138, loss = 0.50973003\n",
      "Iteration 139, loss = 0.51121998\n",
      "Iteration 140, loss = 0.51074873\n",
      "Iteration 141, loss = 0.50887474\n",
      "Iteration 142, loss = 0.50814871\n",
      "Iteration 143, loss = 0.50714333\n",
      "Iteration 144, loss = 0.50729688\n",
      "Iteration 145, loss = 0.50771944\n",
      "Iteration 146, loss = 0.50651447\n",
      "Iteration 147, loss = 0.50634312\n",
      "Iteration 148, loss = 0.50531259\n",
      "Iteration 149, loss = 0.50582427\n",
      "Iteration 150, loss = 0.50557820\n",
      "Iteration 151, loss = 0.50759315\n",
      "Iteration 152, loss = 0.50469320\n",
      "Iteration 153, loss = 0.50332735\n",
      "Iteration 154, loss = 0.50358396\n",
      "Iteration 155, loss = 0.50327305\n",
      "Iteration 156, loss = 0.50169155\n",
      "Iteration 157, loss = 0.50343990\n",
      "Iteration 158, loss = 0.50182920\n",
      "Iteration 159, loss = 0.50257655\n",
      "Iteration 160, loss = 0.50153427\n",
      "Iteration 161, loss = 0.50114149\n",
      "Iteration 162, loss = 0.50142231\n",
      "Iteration 163, loss = 0.50195092\n",
      "Iteration 164, loss = 0.50060952\n",
      "Iteration 165, loss = 0.50147672\n",
      "Iteration 166, loss = 0.49985085\n",
      "Iteration 167, loss = 0.49951129\n",
      "Iteration 168, loss = 0.50378209\n",
      "Iteration 169, loss = 0.49963856\n",
      "Iteration 170, loss = 0.49815484\n",
      "Iteration 171, loss = 0.49693547\n",
      "Iteration 172, loss = 0.49798290\n",
      "Iteration 173, loss = 0.49679113\n",
      "Iteration 174, loss = 0.49880761\n",
      "Iteration 175, loss = 0.49643417\n",
      "Iteration 176, loss = 0.49716859\n",
      "Iteration 177, loss = 0.49496363\n",
      "Iteration 178, loss = 0.49530958\n",
      "Iteration 179, loss = 0.49502948\n",
      "Iteration 180, loss = 0.49447332\n",
      "Iteration 181, loss = 0.49526975\n",
      "Iteration 182, loss = 0.49593070\n",
      "Iteration 183, loss = 0.49465397\n",
      "Iteration 184, loss = 0.49430252\n",
      "Iteration 185, loss = 0.49242575\n",
      "Iteration 186, loss = 0.49526255\n",
      "Iteration 187, loss = 0.49340754\n",
      "Iteration 188, loss = 0.49330605\n",
      "Iteration 189, loss = 0.49311797\n",
      "Iteration 190, loss = 0.49271832\n",
      "Iteration 191, loss = 0.49129493\n",
      "Iteration 192, loss = 0.49202877\n",
      "Iteration 193, loss = 0.49031899\n",
      "Iteration 194, loss = 0.49136460\n",
      "Iteration 195, loss = 0.49072293\n",
      "Iteration 196, loss = 0.49010296\n",
      "Iteration 197, loss = 0.48992067\n",
      "Iteration 198, loss = 0.48982510\n",
      "Iteration 199, loss = 0.48971422\n",
      "Iteration 200, loss = 0.48806302\n",
      "Iteration 201, loss = 0.48787868\n",
      "Iteration 202, loss = 0.48870895\n",
      "Iteration 203, loss = 0.48759892\n",
      "Iteration 204, loss = 0.48743867\n",
      "Iteration 205, loss = 0.48780197\n",
      "Iteration 206, loss = 0.48696660\n",
      "Iteration 207, loss = 0.48559006\n",
      "Iteration 208, loss = 0.48616612\n",
      "Iteration 209, loss = 0.48481907\n",
      "Iteration 210, loss = 0.48639215\n",
      "Iteration 211, loss = 0.48566417\n",
      "Iteration 212, loss = 0.48550362\n",
      "Iteration 213, loss = 0.48469805\n",
      "Iteration 214, loss = 0.48603056\n",
      "Iteration 215, loss = 0.48341444\n",
      "Iteration 216, loss = 0.48519745\n",
      "Iteration 217, loss = 0.48417527\n",
      "Iteration 218, loss = 0.48283825\n",
      "Iteration 219, loss = 0.48260875\n",
      "Iteration 220, loss = 0.48356051\n",
      "Iteration 221, loss = 0.48365390\n",
      "Iteration 222, loss = 0.48362196\n",
      "Iteration 223, loss = 0.48143738\n",
      "Iteration 224, loss = 0.48310129\n",
      "Iteration 225, loss = 0.48237275\n",
      "Iteration 226, loss = 0.48158189\n",
      "Iteration 227, loss = 0.48137556\n",
      "Iteration 228, loss = 0.48089878\n",
      "Iteration 229, loss = 0.48192589\n",
      "Iteration 230, loss = 0.48331961\n",
      "Iteration 231, loss = 0.48038386\n",
      "Iteration 232, loss = 0.47920882\n",
      "Iteration 233, loss = 0.47987921\n",
      "Iteration 234, loss = 0.47922540\n",
      "Iteration 235, loss = 0.47864680\n",
      "Iteration 236, loss = 0.48041883\n",
      "Iteration 237, loss = 0.47809078\n",
      "Iteration 238, loss = 0.47874560\n",
      "Iteration 239, loss = 0.47963060\n",
      "Iteration 240, loss = 0.48008099\n",
      "Iteration 241, loss = 0.47940365\n",
      "Iteration 242, loss = 0.47675449\n",
      "Iteration 243, loss = 0.47730219\n",
      "Iteration 244, loss = 0.47640071\n",
      "Iteration 245, loss = 0.47651010\n",
      "Iteration 246, loss = 0.47666295\n",
      "Iteration 247, loss = 0.47860730\n",
      "Iteration 248, loss = 0.47888968\n",
      "Iteration 249, loss = 0.47595872\n",
      "Iteration 250, loss = 0.47631901\n",
      "Iteration 251, loss = 0.47644276\n",
      "Iteration 252, loss = 0.47694925\n",
      "Iteration 253, loss = 0.47457682\n",
      "Iteration 254, loss = 0.47355321\n",
      "Iteration 255, loss = 0.47417932\n",
      "Iteration 256, loss = 0.47364125\n",
      "Iteration 257, loss = 0.47914377\n",
      "Iteration 258, loss = 0.47571900\n",
      "Iteration 259, loss = 0.47571772\n",
      "Iteration 260, loss = 0.47385896\n",
      "Iteration 261, loss = 0.47324259\n",
      "Iteration 262, loss = 0.47287179\n",
      "Iteration 263, loss = 0.47282968\n",
      "Iteration 264, loss = 0.47390059\n",
      "Iteration 265, loss = 0.47273533\n",
      "Iteration 266, loss = 0.47197479\n",
      "Iteration 267, loss = 0.47254606\n",
      "Iteration 268, loss = 0.47207619\n",
      "Iteration 269, loss = 0.47213100\n",
      "Iteration 270, loss = 0.47091353\n",
      "Iteration 271, loss = 0.47095812\n",
      "Iteration 272, loss = 0.47040218\n",
      "Iteration 273, loss = 0.47097878\n",
      "Iteration 274, loss = 0.46998947\n",
      "Iteration 275, loss = 0.47032586\n",
      "Iteration 276, loss = 0.47370058\n",
      "Iteration 277, loss = 0.47205379\n",
      "Iteration 278, loss = 0.47059337\n",
      "Iteration 279, loss = 0.46854321\n",
      "Iteration 280, loss = 0.46832262\n",
      "Iteration 281, loss = 0.47010931\n",
      "Iteration 282, loss = 0.46876512\n",
      "Iteration 283, loss = 0.46983518\n",
      "Iteration 284, loss = 0.46949230\n",
      "Iteration 285, loss = 0.46708754\n",
      "Iteration 286, loss = 0.46847934\n",
      "Iteration 287, loss = 0.46733988\n",
      "Iteration 288, loss = 0.46804055\n",
      "Iteration 289, loss = 0.46876287\n",
      "Iteration 290, loss = 0.46798474\n",
      "Iteration 291, loss = 0.46773118\n",
      "Iteration 292, loss = 0.46632499\n",
      "Iteration 293, loss = 0.46775424\n",
      "Iteration 294, loss = 0.46614368\n",
      "Iteration 295, loss = 0.46908331\n",
      "Iteration 296, loss = 0.46652889\n",
      "Iteration 297, loss = 0.46682466\n",
      "Iteration 298, loss = 0.46710549\n",
      "Iteration 299, loss = 0.46730248\n",
      "Iteration 300, loss = 0.46509798\n",
      "Iteration 301, loss = 0.46558222\n",
      "Iteration 302, loss = 0.46732221\n",
      "Iteration 303, loss = 0.46615457\n",
      "Iteration 304, loss = 0.46643365\n",
      "Iteration 305, loss = 0.46581938\n",
      "Iteration 306, loss = 0.46687016\n",
      "Iteration 307, loss = 0.46426193\n",
      "Iteration 308, loss = 0.46411136\n",
      "Iteration 309, loss = 0.46420564\n",
      "Iteration 310, loss = 0.46318868\n",
      "Iteration 311, loss = 0.46572175\n",
      "Iteration 312, loss = 0.46310655\n",
      "Iteration 313, loss = 0.46282449\n",
      "Iteration 314, loss = 0.46230818\n",
      "Iteration 315, loss = 0.46239934\n",
      "Iteration 316, loss = 0.46431076\n",
      "Iteration 317, loss = 0.46340341\n",
      "Iteration 318, loss = 0.46219462\n",
      "Iteration 319, loss = 0.46346959\n",
      "Iteration 320, loss = 0.46379083\n",
      "Iteration 321, loss = 0.46418195\n",
      "Iteration 322, loss = 0.46050755\n",
      "Iteration 323, loss = 0.46228108\n",
      "Iteration 324, loss = 0.46050489\n",
      "Iteration 325, loss = 0.46237715\n",
      "Iteration 326, loss = 0.46131601\n",
      "Iteration 327, loss = 0.46411781\n",
      "Iteration 328, loss = 0.46192971\n",
      "Iteration 329, loss = 0.46319300\n",
      "Iteration 330, loss = 0.46186276\n",
      "Iteration 331, loss = 0.46005731\n",
      "Iteration 332, loss = 0.46093456\n",
      "Iteration 333, loss = 0.46029366\n",
      "Iteration 334, loss = 0.45944041\n",
      "Iteration 335, loss = 0.46241063\n",
      "Iteration 336, loss = 0.46066666\n",
      "Iteration 337, loss = 0.45928581\n",
      "Iteration 338, loss = 0.46075831\n",
      "Iteration 339, loss = 0.45884769\n",
      "Iteration 340, loss = 0.45931508\n",
      "Iteration 341, loss = 0.46059115\n",
      "Iteration 342, loss = 0.46162968\n",
      "Iteration 343, loss = 0.46009374\n",
      "Iteration 344, loss = 0.45958996\n",
      "Iteration 345, loss = 0.45848517\n",
      "Iteration 346, loss = 0.45861115\n",
      "Iteration 347, loss = 0.45993636\n",
      "Iteration 348, loss = 0.45977486\n",
      "Iteration 349, loss = 0.45846095\n",
      "Iteration 350, loss = 0.45875554\n",
      "Iteration 351, loss = 0.45916222\n",
      "Iteration 352, loss = 0.45960206\n",
      "Iteration 353, loss = 0.45909960\n",
      "Iteration 354, loss = 0.46003214\n",
      "Iteration 355, loss = 0.45741450\n",
      "Iteration 356, loss = 0.45976671\n",
      "Iteration 357, loss = 0.45901202\n",
      "Iteration 358, loss = 0.45634568\n",
      "Iteration 359, loss = 0.45676841\n",
      "Iteration 360, loss = 0.45790886\n",
      "Iteration 361, loss = 0.45648441\n",
      "Iteration 362, loss = 0.45809559\n",
      "Iteration 363, loss = 0.45926678\n",
      "Iteration 364, loss = 0.45927161\n",
      "Iteration 365, loss = 0.45589471\n",
      "Iteration 366, loss = 0.45846698\n",
      "Iteration 367, loss = 0.45761896\n",
      "Iteration 368, loss = 0.45609566\n",
      "Iteration 369, loss = 0.45695628\n",
      "Iteration 370, loss = 0.45534362\n",
      "Iteration 371, loss = 0.45503659\n",
      "Iteration 372, loss = 0.45943276\n",
      "Iteration 373, loss = 0.45558957\n",
      "Iteration 374, loss = 0.46086645\n",
      "Iteration 375, loss = 0.45592090\n",
      "Iteration 376, loss = 0.45585671\n",
      "Iteration 377, loss = 0.45360970\n",
      "Iteration 378, loss = 0.45612751\n",
      "Iteration 379, loss = 0.45658249\n",
      "Iteration 380, loss = 0.45477927\n",
      "Iteration 381, loss = 0.45563344\n",
      "Iteration 382, loss = 0.45420194\n",
      "Iteration 383, loss = 0.45386222\n",
      "Iteration 384, loss = 0.45406162\n",
      "Iteration 385, loss = 0.45604940\n",
      "Iteration 386, loss = 0.45375428\n",
      "Iteration 387, loss = 0.45441939\n",
      "Iteration 388, loss = 0.45609818\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66285843\n",
      "Iteration 2, loss = 0.65615823\n",
      "Iteration 3, loss = 0.65390034\n",
      "Iteration 4, loss = 0.65175088\n",
      "Iteration 5, loss = 0.65031050\n",
      "Iteration 6, loss = 0.64879121\n",
      "Iteration 7, loss = 0.64685075\n",
      "Iteration 8, loss = 0.64494455\n",
      "Iteration 9, loss = 0.64261051\n",
      "Iteration 10, loss = 0.64085118\n",
      "Iteration 11, loss = 0.63823820\n",
      "Iteration 12, loss = 0.63602363\n",
      "Iteration 13, loss = 0.63366986\n",
      "Iteration 14, loss = 0.63185062\n",
      "Iteration 15, loss = 0.62933951\n",
      "Iteration 16, loss = 0.62640821\n",
      "Iteration 17, loss = 0.62398209\n",
      "Iteration 18, loss = 0.62097473\n",
      "Iteration 19, loss = 0.61764012\n",
      "Iteration 20, loss = 0.61414679\n",
      "Iteration 21, loss = 0.61125227\n",
      "Iteration 22, loss = 0.60731496\n",
      "Iteration 23, loss = 0.60388211\n",
      "Iteration 24, loss = 0.60053707\n",
      "Iteration 25, loss = 0.59654378\n",
      "Iteration 26, loss = 0.59280581\n",
      "Iteration 27, loss = 0.58915254\n",
      "Iteration 28, loss = 0.58524247\n",
      "Iteration 29, loss = 0.58188998\n",
      "Iteration 30, loss = 0.57838711\n",
      "Iteration 31, loss = 0.57451524\n",
      "Iteration 32, loss = 0.57123806\n",
      "Iteration 33, loss = 0.56707982\n",
      "Iteration 34, loss = 0.56314205\n",
      "Iteration 35, loss = 0.56006846\n",
      "Iteration 36, loss = 0.55556031\n",
      "Iteration 37, loss = 0.55189642\n",
      "Iteration 38, loss = 0.54855019\n",
      "Iteration 39, loss = 0.54459870\n",
      "Iteration 40, loss = 0.54190148\n",
      "Iteration 41, loss = 0.53778544\n",
      "Iteration 42, loss = 0.53427670\n",
      "Iteration 43, loss = 0.53038489\n",
      "Iteration 44, loss = 0.52755110\n",
      "Iteration 45, loss = 0.52351192\n",
      "Iteration 46, loss = 0.52033630\n",
      "Iteration 47, loss = 0.51744895\n",
      "Iteration 48, loss = 0.51370374\n",
      "Iteration 49, loss = 0.51065902\n",
      "Iteration 50, loss = 0.50742028\n",
      "Iteration 51, loss = 0.50440600\n",
      "Iteration 52, loss = 0.50088703\n",
      "Iteration 53, loss = 0.49833338\n",
      "Iteration 54, loss = 0.49496522\n",
      "Iteration 55, loss = 0.49231525\n",
      "Iteration 56, loss = 0.48935922\n",
      "Iteration 57, loss = 0.48688697\n",
      "Iteration 58, loss = 0.48353633\n",
      "Iteration 59, loss = 0.48090758\n",
      "Iteration 60, loss = 0.47791938\n",
      "Iteration 61, loss = 0.47470193\n",
      "Iteration 62, loss = 0.47208099\n",
      "Iteration 63, loss = 0.46969732\n",
      "Iteration 64, loss = 0.46643672\n",
      "Iteration 65, loss = 0.46405104\n",
      "Iteration 66, loss = 0.46106596\n",
      "Iteration 67, loss = 0.45855625\n",
      "Iteration 68, loss = 0.45658742\n",
      "Iteration 69, loss = 0.45331532\n",
      "Iteration 70, loss = 0.45115728\n",
      "Iteration 71, loss = 0.44834266\n",
      "Iteration 72, loss = 0.44623045\n",
      "Iteration 73, loss = 0.44347398\n",
      "Iteration 74, loss = 0.44155408\n",
      "Iteration 75, loss = 0.43890945\n",
      "Iteration 76, loss = 0.43609468\n",
      "Iteration 77, loss = 0.43367214\n",
      "Iteration 78, loss = 0.43190823\n",
      "Iteration 79, loss = 0.42913632\n",
      "Iteration 80, loss = 0.42733886\n",
      "Iteration 81, loss = 0.42464930\n",
      "Iteration 82, loss = 0.42279888\n",
      "Iteration 83, loss = 0.42072190\n",
      "Iteration 84, loss = 0.41808386\n",
      "Iteration 85, loss = 0.41641430\n",
      "Iteration 86, loss = 0.41443833\n",
      "Iteration 87, loss = 0.41129556\n",
      "Iteration 88, loss = 0.40966246\n",
      "Iteration 89, loss = 0.40723698\n",
      "Iteration 90, loss = 0.40600347\n",
      "Iteration 91, loss = 0.40439411\n",
      "Iteration 92, loss = 0.40172696\n",
      "Iteration 93, loss = 0.40014820\n",
      "Iteration 94, loss = 0.39819831\n",
      "Iteration 95, loss = 0.39644875\n",
      "Iteration 96, loss = 0.39364543\n",
      "Iteration 97, loss = 0.39250077\n",
      "Iteration 98, loss = 0.39082406\n",
      "Iteration 99, loss = 0.38877184\n",
      "Iteration 100, loss = 0.38653460\n",
      "Iteration 101, loss = 0.38547118\n",
      "Iteration 102, loss = 0.38401228\n",
      "Iteration 103, loss = 0.38190178\n",
      "Iteration 104, loss = 0.38010282\n",
      "Iteration 105, loss = 0.37863067\n",
      "Iteration 106, loss = 0.37729760\n",
      "Iteration 107, loss = 0.37490393\n",
      "Iteration 108, loss = 0.37273897\n",
      "Iteration 109, loss = 0.37153848\n",
      "Iteration 110, loss = 0.36935592\n",
      "Iteration 111, loss = 0.36820461\n",
      "Iteration 112, loss = 0.36587870\n",
      "Iteration 113, loss = 0.36470984\n",
      "Iteration 114, loss = 0.36320581\n",
      "Iteration 115, loss = 0.36198444\n",
      "Iteration 116, loss = 0.36003522\n",
      "Iteration 117, loss = 0.35853351\n",
      "Iteration 118, loss = 0.35658800\n",
      "Iteration 119, loss = 0.35577680\n",
      "Iteration 120, loss = 0.35327641\n",
      "Iteration 121, loss = 0.35278526\n",
      "Iteration 122, loss = 0.35060688\n",
      "Iteration 123, loss = 0.34887731\n",
      "Iteration 124, loss = 0.34724679\n",
      "Iteration 125, loss = 0.34664511\n",
      "Iteration 126, loss = 0.34491985\n",
      "Iteration 127, loss = 0.34306900\n",
      "Iteration 128, loss = 0.34161841\n",
      "Iteration 129, loss = 0.34024888\n",
      "Iteration 130, loss = 0.33911683\n",
      "Iteration 131, loss = 0.33768334\n",
      "Iteration 132, loss = 0.33562229\n",
      "Iteration 133, loss = 0.33513746\n",
      "Iteration 134, loss = 0.33358734\n",
      "Iteration 135, loss = 0.33214180\n",
      "Iteration 136, loss = 0.33061659\n",
      "Iteration 137, loss = 0.32937360\n",
      "Iteration 138, loss = 0.32863951\n",
      "Iteration 139, loss = 0.32686339\n",
      "Iteration 140, loss = 0.32593647\n",
      "Iteration 141, loss = 0.32449719\n",
      "Iteration 142, loss = 0.32315894\n",
      "Iteration 143, loss = 0.32196695\n",
      "Iteration 144, loss = 0.32040188\n",
      "Iteration 145, loss = 0.31986346\n",
      "Iteration 146, loss = 0.31845536\n",
      "Iteration 147, loss = 0.31724456\n",
      "Iteration 148, loss = 0.31628452\n",
      "Iteration 149, loss = 0.31486309\n",
      "Iteration 150, loss = 0.31366897\n",
      "Iteration 151, loss = 0.31269865\n",
      "Iteration 152, loss = 0.31139498\n",
      "Iteration 153, loss = 0.31017942\n",
      "Iteration 154, loss = 0.30986420\n",
      "Iteration 155, loss = 0.30794068\n",
      "Iteration 156, loss = 0.30738373\n",
      "Iteration 157, loss = 0.30521763\n",
      "Iteration 158, loss = 0.30435427\n",
      "Iteration 159, loss = 0.30331210\n",
      "Iteration 160, loss = 0.30226772\n",
      "Iteration 161, loss = 0.30196272\n",
      "Iteration 162, loss = 0.29988334\n",
      "Iteration 163, loss = 0.29933050\n",
      "Iteration 164, loss = 0.29792518\n",
      "Iteration 165, loss = 0.29682308\n",
      "Iteration 166, loss = 0.29641817\n",
      "Iteration 167, loss = 0.29480059\n",
      "Iteration 168, loss = 0.29380733\n",
      "Iteration 169, loss = 0.29331309\n",
      "Iteration 170, loss = 0.29204919\n",
      "Iteration 171, loss = 0.29083716\n",
      "Iteration 172, loss = 0.29030908\n",
      "Iteration 173, loss = 0.28896905\n",
      "Iteration 174, loss = 0.28822887\n",
      "Iteration 175, loss = 0.28723235\n",
      "Iteration 176, loss = 0.28638975\n",
      "Iteration 177, loss = 0.28447440\n",
      "Iteration 178, loss = 0.28440824\n",
      "Iteration 179, loss = 0.28340467\n",
      "Iteration 180, loss = 0.28240543\n",
      "Iteration 181, loss = 0.28213034\n",
      "Iteration 182, loss = 0.28065613\n",
      "Iteration 183, loss = 0.27990789\n",
      "Iteration 184, loss = 0.27829843\n",
      "Iteration 185, loss = 0.27725322\n",
      "Iteration 186, loss = 0.27742674\n",
      "Iteration 187, loss = 0.27606318\n",
      "Iteration 188, loss = 0.27522634\n",
      "Iteration 189, loss = 0.27391334\n",
      "Iteration 190, loss = 0.27304895\n",
      "Iteration 191, loss = 0.27303299\n",
      "Iteration 192, loss = 0.27167519\n",
      "Iteration 193, loss = 0.27099367\n",
      "Iteration 194, loss = 0.27015043\n",
      "Iteration 195, loss = 0.26910484\n",
      "Iteration 196, loss = 0.26801986\n",
      "Iteration 197, loss = 0.26760785\n",
      "Iteration 198, loss = 0.26614418\n",
      "Iteration 199, loss = 0.26577922\n",
      "Iteration 200, loss = 0.26543749\n",
      "Iteration 201, loss = 0.26461142\n",
      "Iteration 202, loss = 0.26339201\n",
      "Iteration 203, loss = 0.26352095\n",
      "Iteration 204, loss = 0.26156147\n",
      "Iteration 205, loss = 0.26126717\n",
      "Iteration 206, loss = 0.26009706\n",
      "Iteration 207, loss = 0.25960103\n",
      "Iteration 208, loss = 0.25936282\n",
      "Iteration 209, loss = 0.25864705\n",
      "Iteration 210, loss = 0.25776956\n",
      "Iteration 211, loss = 0.25674679\n",
      "Iteration 212, loss = 0.25689562\n",
      "Iteration 213, loss = 0.25474062\n",
      "Iteration 214, loss = 0.25436111\n",
      "Iteration 215, loss = 0.25405005\n",
      "Iteration 216, loss = 0.25280025\n",
      "Iteration 217, loss = 0.25278510\n",
      "Iteration 218, loss = 0.25158507\n",
      "Iteration 219, loss = 0.25125173\n",
      "Iteration 220, loss = 0.25039168\n",
      "Iteration 221, loss = 0.24895965\n",
      "Iteration 222, loss = 0.24916552\n",
      "Iteration 223, loss = 0.24801274\n",
      "Iteration 224, loss = 0.24694787\n",
      "Iteration 225, loss = 0.24660881\n",
      "Iteration 226, loss = 0.24571417\n",
      "Iteration 227, loss = 0.24457301\n",
      "Iteration 228, loss = 0.24485448\n",
      "Iteration 229, loss = 0.24375939\n",
      "Iteration 230, loss = 0.24338229\n",
      "Iteration 231, loss = 0.24255123\n",
      "Iteration 232, loss = 0.24206376\n",
      "Iteration 233, loss = 0.24150953\n",
      "Iteration 234, loss = 0.24031925\n",
      "Iteration 235, loss = 0.24032374\n",
      "Iteration 236, loss = 0.23894264\n",
      "Iteration 237, loss = 0.23816158\n",
      "Iteration 238, loss = 0.23829351\n",
      "Iteration 239, loss = 0.23697673\n",
      "Iteration 240, loss = 0.23663756\n",
      "Iteration 241, loss = 0.23547986\n",
      "Iteration 242, loss = 0.23496185\n",
      "Iteration 243, loss = 0.23490583\n",
      "Iteration 244, loss = 0.23435283\n",
      "Iteration 245, loss = 0.23366156\n",
      "Iteration 246, loss = 0.23328266\n",
      "Iteration 247, loss = 0.23213165\n",
      "Iteration 248, loss = 0.23149499\n",
      "Iteration 249, loss = 0.23117964\n",
      "Iteration 250, loss = 0.23068196\n",
      "Iteration 251, loss = 0.22948274\n",
      "Iteration 252, loss = 0.22895411\n",
      "Iteration 253, loss = 0.22865766\n",
      "Iteration 254, loss = 0.22774715\n",
      "Iteration 255, loss = 0.22726620\n",
      "Iteration 256, loss = 0.22697760\n",
      "Iteration 257, loss = 0.22555623\n",
      "Iteration 258, loss = 0.22595225\n",
      "Iteration 259, loss = 0.22493550\n",
      "Iteration 260, loss = 0.22359436\n",
      "Iteration 261, loss = 0.22380555\n",
      "Iteration 262, loss = 0.22311953\n",
      "Iteration 263, loss = 0.22188571\n",
      "Iteration 264, loss = 0.22231209\n",
      "Iteration 265, loss = 0.22131446\n",
      "Iteration 266, loss = 0.22063594\n",
      "Iteration 267, loss = 0.21993135\n",
      "Iteration 268, loss = 0.21966719\n",
      "Iteration 269, loss = 0.21870415\n",
      "Iteration 270, loss = 0.21907502\n",
      "Iteration 271, loss = 0.21754327\n",
      "Iteration 272, loss = 0.21758511\n",
      "Iteration 273, loss = 0.21737979\n",
      "Iteration 274, loss = 0.21590647\n",
      "Iteration 275, loss = 0.21560949\n",
      "Iteration 276, loss = 0.21531937\n",
      "Iteration 277, loss = 0.21448586\n",
      "Iteration 278, loss = 0.21418404\n",
      "Iteration 279, loss = 0.21302095\n",
      "Iteration 280, loss = 0.21260932\n",
      "Iteration 281, loss = 0.21219405\n",
      "Iteration 282, loss = 0.21133982\n",
      "Iteration 283, loss = 0.21114575\n",
      "Iteration 284, loss = 0.21062513\n",
      "Iteration 285, loss = 0.21002317\n",
      "Iteration 286, loss = 0.20966464\n",
      "Iteration 287, loss = 0.20912248\n",
      "Iteration 288, loss = 0.20899517\n",
      "Iteration 289, loss = 0.20800352\n",
      "Iteration 290, loss = 0.20743931\n",
      "Iteration 291, loss = 0.20683877\n",
      "Iteration 292, loss = 0.20631235\n",
      "Iteration 293, loss = 0.20574179\n",
      "Iteration 294, loss = 0.20504536\n",
      "Iteration 295, loss = 0.20536083\n",
      "Iteration 296, loss = 0.20447706\n",
      "Iteration 297, loss = 0.20432693\n",
      "Iteration 298, loss = 0.20412427\n",
      "Iteration 299, loss = 0.20198157\n",
      "Iteration 300, loss = 0.20226095\n",
      "Iteration 301, loss = 0.20229106\n",
      "Iteration 302, loss = 0.20135708\n",
      "Iteration 303, loss = 0.20099519\n",
      "Iteration 304, loss = 0.20035888\n",
      "Iteration 305, loss = 0.19971376\n",
      "Iteration 306, loss = 0.19928385\n",
      "Iteration 307, loss = 0.19902338\n",
      "Iteration 308, loss = 0.19818308\n",
      "Iteration 309, loss = 0.19851057\n",
      "Iteration 310, loss = 0.19771702\n",
      "Iteration 311, loss = 0.19731867\n",
      "Iteration 312, loss = 0.19666348\n",
      "Iteration 313, loss = 0.19678163\n",
      "Iteration 314, loss = 0.19588288\n",
      "Iteration 315, loss = 0.19584953\n",
      "Iteration 316, loss = 0.19416501\n",
      "Iteration 317, loss = 0.19471323\n",
      "Iteration 318, loss = 0.19433589\n",
      "Iteration 319, loss = 0.19363659\n",
      "Iteration 320, loss = 0.19322995\n",
      "Iteration 321, loss = 0.19358293\n",
      "Iteration 322, loss = 0.19281661\n",
      "Iteration 323, loss = 0.19233280\n",
      "Iteration 324, loss = 0.19180481\n",
      "Iteration 325, loss = 0.19161055\n",
      "Iteration 326, loss = 0.19037983\n",
      "Iteration 327, loss = 0.19046827\n",
      "Iteration 328, loss = 0.18928557\n",
      "Iteration 329, loss = 0.18996174\n",
      "Iteration 330, loss = 0.18998648\n",
      "Iteration 331, loss = 0.18856457\n",
      "Iteration 332, loss = 0.18781908\n",
      "Iteration 333, loss = 0.18873780\n",
      "Iteration 334, loss = 0.18729151\n",
      "Iteration 335, loss = 0.18697689\n",
      "Iteration 336, loss = 0.18682568\n",
      "Iteration 337, loss = 0.18651197\n",
      "Iteration 338, loss = 0.18606774\n",
      "Iteration 339, loss = 0.18578411\n",
      "Iteration 340, loss = 0.18496042\n",
      "Iteration 341, loss = 0.18490322\n",
      "Iteration 342, loss = 0.18526251\n",
      "Iteration 343, loss = 0.18365186\n",
      "Iteration 344, loss = 0.18357269\n",
      "Iteration 345, loss = 0.18358553\n",
      "Iteration 346, loss = 0.18282036\n",
      "Iteration 347, loss = 0.18241142\n",
      "Iteration 348, loss = 0.18231085\n",
      "Iteration 349, loss = 0.18247726\n",
      "Iteration 350, loss = 0.18114151\n",
      "Iteration 351, loss = 0.18106042\n",
      "Iteration 352, loss = 0.18075028\n",
      "Iteration 353, loss = 0.18043470\n",
      "Iteration 354, loss = 0.18009729\n",
      "Iteration 355, loss = 0.17967396\n",
      "Iteration 356, loss = 0.17886216\n",
      "Iteration 357, loss = 0.17893614\n",
      "Iteration 358, loss = 0.17882582\n",
      "Iteration 359, loss = 0.17814823\n",
      "Iteration 360, loss = 0.17810936\n",
      "Iteration 361, loss = 0.17776798\n",
      "Iteration 362, loss = 0.17720152\n",
      "Iteration 363, loss = 0.17697539\n",
      "Iteration 364, loss = 0.17652056\n",
      "Iteration 365, loss = 0.17608796\n",
      "Iteration 366, loss = 0.17561442\n",
      "Iteration 367, loss = 0.17531383\n",
      "Iteration 368, loss = 0.17506582\n",
      "Iteration 369, loss = 0.17492549\n",
      "Iteration 370, loss = 0.17455900\n",
      "Iteration 371, loss = 0.17397165\n",
      "Iteration 372, loss = 0.17382506\n",
      "Iteration 373, loss = 0.17340693\n",
      "Iteration 374, loss = 0.17294458\n",
      "Iteration 375, loss = 0.17281749\n",
      "Iteration 376, loss = 0.17229054\n",
      "Iteration 377, loss = 0.17251912\n",
      "Iteration 378, loss = 0.17142878\n",
      "Iteration 379, loss = 0.17168714\n",
      "Iteration 380, loss = 0.17108160\n",
      "Iteration 381, loss = 0.17070725\n",
      "Iteration 382, loss = 0.17028904\n",
      "Iteration 383, loss = 0.17047519\n",
      "Iteration 384, loss = 0.16985597\n",
      "Iteration 385, loss = 0.17018515\n",
      "Iteration 386, loss = 0.16915020\n",
      "Iteration 387, loss = 0.16872027\n",
      "Iteration 388, loss = 0.16872629\n",
      "Iteration 389, loss = 0.16914163\n",
      "Iteration 390, loss = 0.16763652\n",
      "Iteration 391, loss = 0.16753872\n",
      "Iteration 392, loss = 0.16790743\n",
      "Iteration 393, loss = 0.16669665\n",
      "Iteration 394, loss = 0.16664285\n",
      "Iteration 395, loss = 0.16617263\n",
      "Iteration 396, loss = 0.16595693\n",
      "Iteration 397, loss = 0.16591123\n",
      "Iteration 398, loss = 0.16560772\n",
      "Iteration 399, loss = 0.16547831\n",
      "Iteration 400, loss = 0.16510511\n",
      "Iteration 401, loss = 0.16459410\n",
      "Iteration 402, loss = 0.16432744\n",
      "Iteration 403, loss = 0.16423065\n",
      "Iteration 404, loss = 0.16394715\n",
      "Iteration 405, loss = 0.16369664\n",
      "Iteration 406, loss = 0.16376832\n",
      "Iteration 407, loss = 0.16339203\n",
      "Iteration 408, loss = 0.16256809\n",
      "Iteration 409, loss = 0.16192667\n",
      "Iteration 410, loss = 0.16183823\n",
      "Iteration 411, loss = 0.16203800\n",
      "Iteration 412, loss = 0.16103256\n",
      "Iteration 413, loss = 0.16089954\n",
      "Iteration 414, loss = 0.16079495\n",
      "Iteration 415, loss = 0.16087804\n",
      "Iteration 416, loss = 0.16022031\n",
      "Iteration 417, loss = 0.16026628\n",
      "Iteration 418, loss = 0.15958515\n",
      "Iteration 419, loss = 0.15868919\n",
      "Iteration 420, loss = 0.15999358\n",
      "Iteration 421, loss = 0.15868264\n",
      "Iteration 422, loss = 0.15899618\n",
      "Iteration 423, loss = 0.15808960\n",
      "Iteration 424, loss = 0.15776840\n",
      "Iteration 425, loss = 0.15810269\n",
      "Iteration 426, loss = 0.15695700\n",
      "Iteration 427, loss = 0.15711668\n",
      "Iteration 428, loss = 0.15709235\n",
      "Iteration 429, loss = 0.15765145\n",
      "Iteration 430, loss = 0.15621242\n",
      "Iteration 431, loss = 0.15599809\n",
      "Iteration 432, loss = 0.15536482\n",
      "Iteration 433, loss = 0.15560285\n",
      "Iteration 434, loss = 0.15525074\n",
      "Iteration 435, loss = 0.15513892\n",
      "Iteration 436, loss = 0.15414993\n",
      "Iteration 437, loss = 0.15453764\n",
      "Iteration 438, loss = 0.15349255\n",
      "Iteration 439, loss = 0.15358313\n",
      "Iteration 440, loss = 0.15396303\n",
      "Iteration 441, loss = 0.15297668\n",
      "Iteration 442, loss = 0.15326794\n",
      "Iteration 443, loss = 0.15289417\n",
      "Iteration 444, loss = 0.15209843\n",
      "Iteration 445, loss = 0.15305623\n",
      "Iteration 446, loss = 0.15191430\n",
      "Iteration 447, loss = 0.15116972\n",
      "Iteration 448, loss = 0.15157885\n",
      "Iteration 449, loss = 0.15078634\n",
      "Iteration 450, loss = 0.15090128\n",
      "Iteration 451, loss = 0.15067431\n",
      "Iteration 452, loss = 0.14983962\n",
      "Iteration 453, loss = 0.15003328\n",
      "Iteration 454, loss = 0.15009455\n",
      "Iteration 455, loss = 0.14998860\n",
      "Iteration 456, loss = 0.14932842\n",
      "Iteration 457, loss = 0.14928888\n",
      "Iteration 458, loss = 0.14875555\n",
      "Iteration 459, loss = 0.14882075\n",
      "Iteration 460, loss = 0.14820591\n",
      "Iteration 461, loss = 0.14833765\n",
      "Iteration 462, loss = 0.14776920\n",
      "Iteration 463, loss = 0.14727242\n",
      "Iteration 464, loss = 0.14740656\n",
      "Iteration 465, loss = 0.14728659\n",
      "Iteration 466, loss = 0.14682281\n",
      "Iteration 467, loss = 0.14654161\n",
      "Iteration 468, loss = 0.14640281\n",
      "Iteration 469, loss = 0.14640238\n",
      "Iteration 470, loss = 0.14606329\n",
      "Iteration 471, loss = 0.14566958\n",
      "Iteration 472, loss = 0.14576959\n",
      "Iteration 473, loss = 0.14566012\n",
      "Iteration 474, loss = 0.14516990\n",
      "Iteration 475, loss = 0.14456094\n",
      "Iteration 476, loss = 0.14460938\n",
      "Iteration 477, loss = 0.14440870\n",
      "Iteration 478, loss = 0.14385943\n",
      "Iteration 479, loss = 0.14408128\n",
      "Iteration 480, loss = 0.14367647\n",
      "Iteration 481, loss = 0.14396274\n",
      "Iteration 482, loss = 0.14386142\n",
      "Iteration 483, loss = 0.14314934\n",
      "Iteration 484, loss = 0.14307051\n",
      "Iteration 485, loss = 0.14214853\n",
      "Iteration 486, loss = 0.14271263\n",
      "Iteration 487, loss = 0.14271146\n",
      "Iteration 488, loss = 0.14210182\n",
      "Iteration 489, loss = 0.14162110\n",
      "Iteration 490, loss = 0.14130929\n",
      "Iteration 491, loss = 0.14091386\n",
      "Iteration 492, loss = 0.14087733\n",
      "Iteration 493, loss = 0.14075071\n",
      "Iteration 494, loss = 0.14085841\n",
      "Iteration 495, loss = 0.14043644\n",
      "Iteration 496, loss = 0.13983621\n",
      "Iteration 497, loss = 0.14035193\n",
      "Iteration 498, loss = 0.13930344\n",
      "Iteration 499, loss = 0.13928322\n",
      "Iteration 500, loss = 0.13975626\n",
      "Iteration 501, loss = 0.13927155\n",
      "Iteration 502, loss = 0.13840954\n",
      "Iteration 503, loss = 0.13823771\n",
      "Iteration 504, loss = 0.13922642\n",
      "Iteration 505, loss = 0.13869922\n",
      "Iteration 506, loss = 0.13802034\n",
      "Iteration 507, loss = 0.13841732\n",
      "Iteration 508, loss = 0.13737580\n",
      "Iteration 509, loss = 0.13730105\n",
      "Iteration 510, loss = 0.13726120\n",
      "Iteration 511, loss = 0.13758607\n",
      "Iteration 512, loss = 0.13689078\n",
      "Iteration 513, loss = 0.13703651\n",
      "Iteration 514, loss = 0.13594348\n",
      "Iteration 515, loss = 0.13615224\n",
      "Iteration 516, loss = 0.13550419\n",
      "Iteration 517, loss = 0.13583241\n",
      "Iteration 518, loss = 0.13600514\n",
      "Iteration 519, loss = 0.13568788\n",
      "Iteration 520, loss = 0.13560960\n",
      "Iteration 521, loss = 0.13502228\n",
      "Iteration 522, loss = 0.13491293\n",
      "Iteration 523, loss = 0.13490092\n",
      "Iteration 524, loss = 0.13397160\n",
      "Iteration 525, loss = 0.13454470\n",
      "Iteration 526, loss = 0.13456894\n",
      "Iteration 527, loss = 0.13423451\n",
      "Iteration 528, loss = 0.13360399\n",
      "Iteration 529, loss = 0.13384600\n",
      "Iteration 530, loss = 0.13320959\n",
      "Iteration 531, loss = 0.13311845\n",
      "Iteration 532, loss = 0.13317087\n",
      "Iteration 533, loss = 0.13293465\n",
      "Iteration 534, loss = 0.13255852\n",
      "Iteration 535, loss = 0.13233947\n",
      "Iteration 536, loss = 0.13229993\n",
      "Iteration 537, loss = 0.13176035\n",
      "Iteration 538, loss = 0.13214723\n",
      "Iteration 539, loss = 0.13169300\n",
      "Iteration 540, loss = 0.13192006\n",
      "Iteration 541, loss = 0.13078615\n",
      "Iteration 542, loss = 0.13075216\n",
      "Iteration 543, loss = 0.13083778\n",
      "Iteration 544, loss = 0.13182604\n",
      "Iteration 545, loss = 0.13103833\n",
      "Iteration 546, loss = 0.13077624\n",
      "Iteration 547, loss = 0.12979233\n",
      "Iteration 548, loss = 0.13018946\n",
      "Iteration 549, loss = 0.12953751\n",
      "Iteration 550, loss = 0.13039269\n",
      "Iteration 551, loss = 0.12921468\n",
      "Iteration 552, loss = 0.12949783\n",
      "Iteration 553, loss = 0.12889197\n",
      "Iteration 554, loss = 0.12911048\n",
      "Iteration 555, loss = 0.12893703\n",
      "Iteration 556, loss = 0.12869255\n",
      "Iteration 557, loss = 0.12887213\n",
      "Iteration 558, loss = 0.12830153\n",
      "Iteration 559, loss = 0.12795755\n",
      "Iteration 560, loss = 0.12885460\n",
      "Iteration 561, loss = 0.12779192\n",
      "Iteration 562, loss = 0.12744181\n",
      "Iteration 563, loss = 0.12731647\n",
      "Iteration 564, loss = 0.12701809\n",
      "Iteration 565, loss = 0.12662919\n",
      "Iteration 566, loss = 0.12733635\n",
      "Iteration 567, loss = 0.12669018\n",
      "Iteration 568, loss = 0.12616757\n",
      "Iteration 569, loss = 0.12609773\n",
      "Iteration 570, loss = 0.12638475\n",
      "Iteration 571, loss = 0.12634494\n",
      "Iteration 572, loss = 0.12550487\n",
      "Iteration 573, loss = 0.12544996\n",
      "Iteration 574, loss = 0.12534442\n",
      "Iteration 575, loss = 0.12518075\n",
      "Iteration 576, loss = 0.12509925\n",
      "Iteration 577, loss = 0.12541978\n",
      "Iteration 578, loss = 0.12466344\n",
      "Iteration 579, loss = 0.12443787\n",
      "Iteration 580, loss = 0.12489616\n",
      "Iteration 581, loss = 0.12414873\n",
      "Iteration 582, loss = 0.12384145\n",
      "Iteration 583, loss = 0.12400309\n",
      "Iteration 584, loss = 0.12419019\n",
      "Iteration 585, loss = 0.12328874\n",
      "Iteration 586, loss = 0.12390729\n",
      "Iteration 587, loss = 0.12338429\n",
      "Iteration 588, loss = 0.12348085\n",
      "Iteration 589, loss = 0.12313941\n",
      "Iteration 590, loss = 0.12319354\n",
      "Iteration 591, loss = 0.12331409\n",
      "Iteration 592, loss = 0.12263245\n",
      "Iteration 593, loss = 0.12300327\n",
      "Iteration 594, loss = 0.12169298\n",
      "Iteration 595, loss = 0.12200624\n",
      "Iteration 596, loss = 0.12188875\n",
      "Iteration 597, loss = 0.12205738\n",
      "Iteration 598, loss = 0.12200036\n",
      "Iteration 599, loss = 0.12088436\n",
      "Iteration 600, loss = 0.12149616\n",
      "Iteration 601, loss = 0.12158126\n",
      "Iteration 602, loss = 0.12161940\n",
      "Iteration 603, loss = 0.12100009\n",
      "Iteration 604, loss = 0.12053022\n",
      "Iteration 605, loss = 0.12046998\n",
      "Iteration 606, loss = 0.12068864\n",
      "Iteration 607, loss = 0.12001435\n",
      "Iteration 608, loss = 0.11981462\n",
      "Iteration 609, loss = 0.12045334\n",
      "Iteration 610, loss = 0.11938461\n",
      "Iteration 611, loss = 0.11989235\n",
      "Iteration 612, loss = 0.11992662\n",
      "Iteration 613, loss = 0.11878227\n",
      "Iteration 614, loss = 0.11977639\n",
      "Iteration 615, loss = 0.11950922\n",
      "Iteration 616, loss = 0.11885425\n",
      "Iteration 617, loss = 0.11877396\n",
      "Iteration 618, loss = 0.11903566\n",
      "Iteration 619, loss = 0.11841496\n",
      "Iteration 620, loss = 0.11819275\n",
      "Iteration 621, loss = 0.11798916\n",
      "Iteration 622, loss = 0.11883706\n",
      "Iteration 623, loss = 0.11769897\n",
      "Iteration 624, loss = 0.11798921\n",
      "Iteration 625, loss = 0.11691468\n",
      "Iteration 626, loss = 0.11815089\n",
      "Iteration 627, loss = 0.11706595\n",
      "Iteration 628, loss = 0.11705760\n",
      "Iteration 629, loss = 0.11721482\n",
      "Iteration 630, loss = 0.11663566\n",
      "Iteration 631, loss = 0.11703603\n",
      "Iteration 632, loss = 0.11638181\n",
      "Iteration 633, loss = 0.11708976\n",
      "Iteration 634, loss = 0.11668052\n",
      "Iteration 635, loss = 0.11637259\n",
      "Iteration 636, loss = 0.11548509\n",
      "Iteration 637, loss = 0.11627267\n",
      "Iteration 638, loss = 0.11658096\n",
      "Iteration 639, loss = 0.11542436\n",
      "Iteration 640, loss = 0.11542223\n",
      "Iteration 641, loss = 0.11531474\n",
      "Iteration 642, loss = 0.11536378\n",
      "Iteration 643, loss = 0.11552980\n",
      "Iteration 644, loss = 0.11513993\n",
      "Iteration 645, loss = 0.11519053\n",
      "Iteration 646, loss = 0.11452913\n",
      "Iteration 647, loss = 0.11453819\n",
      "Iteration 648, loss = 0.11409586\n",
      "Iteration 649, loss = 0.11426017\n",
      "Iteration 650, loss = 0.11438403\n",
      "Iteration 651, loss = 0.11375745\n",
      "Iteration 652, loss = 0.11506554\n",
      "Iteration 653, loss = 0.11416966\n",
      "Iteration 654, loss = 0.11294493\n",
      "Iteration 655, loss = 0.11343614\n",
      "Iteration 656, loss = 0.11389014\n",
      "Iteration 657, loss = 0.11241121\n",
      "Iteration 658, loss = 0.11292208\n",
      "Iteration 659, loss = 0.11199250\n",
      "Iteration 660, loss = 0.11278681\n",
      "Iteration 661, loss = 0.11258486\n",
      "Iteration 662, loss = 0.11246618\n",
      "Iteration 663, loss = 0.11184345\n",
      "Iteration 664, loss = 0.11271483\n",
      "Iteration 665, loss = 0.11216399\n",
      "Iteration 666, loss = 0.11158505\n",
      "Iteration 667, loss = 0.11147701\n",
      "Iteration 668, loss = 0.11196394\n",
      "Iteration 669, loss = 0.11112593\n",
      "Iteration 670, loss = 0.11104324\n",
      "Iteration 671, loss = 0.11104082\n",
      "Iteration 672, loss = 0.11130585\n",
      "Iteration 673, loss = 0.11073681\n",
      "Iteration 674, loss = 0.11062825\n",
      "Iteration 675, loss = 0.11135367\n",
      "Iteration 676, loss = 0.11077004\n",
      "Iteration 677, loss = 0.11100539\n",
      "Iteration 678, loss = 0.11051564\n",
      "Iteration 679, loss = 0.11054798\n",
      "Iteration 680, loss = 0.11151085\n",
      "Iteration 681, loss = 0.11020762\n",
      "Iteration 682, loss = 0.10949084\n",
      "Iteration 683, loss = 0.10983295\n",
      "Iteration 684, loss = 0.10917653\n",
      "Iteration 685, loss = 0.10916032\n",
      "Iteration 686, loss = 0.10936465\n",
      "Iteration 687, loss = 0.10899863\n",
      "Iteration 688, loss = 0.10995230\n",
      "Iteration 689, loss = 0.10845879\n",
      "Iteration 690, loss = 0.10901224\n",
      "Iteration 691, loss = 0.10906049\n",
      "Iteration 692, loss = 0.10857662\n",
      "Iteration 693, loss = 0.10848859\n",
      "Iteration 694, loss = 0.10801118\n",
      "Iteration 695, loss = 0.10868269\n",
      "Iteration 696, loss = 0.10887063\n",
      "Iteration 697, loss = 0.10831178\n",
      "Iteration 698, loss = 0.10816305\n",
      "Iteration 699, loss = 0.10756940\n",
      "Iteration 700, loss = 0.10807976\n",
      "Iteration 701, loss = 0.10747999\n",
      "Iteration 702, loss = 0.10903240\n",
      "Iteration 703, loss = 0.10729796\n",
      "Iteration 704, loss = 0.10775081\n",
      "Iteration 705, loss = 0.10742905\n",
      "Iteration 706, loss = 0.10733149\n",
      "Iteration 707, loss = 0.10707221\n",
      "Iteration 708, loss = 0.10650928\n",
      "Iteration 709, loss = 0.10721180\n",
      "Iteration 710, loss = 0.10676981\n",
      "Iteration 711, loss = 0.10656193\n",
      "Iteration 712, loss = 0.10645604\n",
      "Iteration 713, loss = 0.10647030\n",
      "Iteration 714, loss = 0.10646278\n",
      "Iteration 715, loss = 0.10679925\n",
      "Iteration 716, loss = 0.10603468\n",
      "Iteration 717, loss = 0.10567252\n",
      "Iteration 718, loss = 0.10620797\n",
      "Iteration 719, loss = 0.10623825\n",
      "Iteration 720, loss = 0.10474653\n",
      "Iteration 721, loss = 0.10621649\n",
      "Iteration 722, loss = 0.10520091\n",
      "Iteration 723, loss = 0.10538037\n",
      "Iteration 724, loss = 0.10540824\n",
      "Iteration 725, loss = 0.10507718\n",
      "Iteration 726, loss = 0.10517711\n",
      "Iteration 727, loss = 0.10565874\n",
      "Iteration 728, loss = 0.10479043\n",
      "Iteration 729, loss = 0.10493412\n",
      "Iteration 730, loss = 0.10499523\n",
      "Iteration 731, loss = 0.10477644\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68208820\n",
      "Iteration 2, loss = 0.66438699\n",
      "Iteration 3, loss = 0.65917104\n",
      "Iteration 4, loss = 0.65685369\n",
      "Iteration 5, loss = 0.65614460\n",
      "Iteration 6, loss = 0.65576837\n",
      "Iteration 7, loss = 0.65527312\n",
      "Iteration 8, loss = 0.65469171\n",
      "Iteration 9, loss = 0.65415101\n",
      "Iteration 10, loss = 0.65408531\n",
      "Iteration 11, loss = 0.65363976\n",
      "Iteration 12, loss = 0.65364259\n",
      "Iteration 13, loss = 0.65335505\n",
      "Iteration 14, loss = 0.65332693\n",
      "Iteration 15, loss = 0.65318890\n",
      "Iteration 16, loss = 0.65297521\n",
      "Iteration 17, loss = 0.65306811\n",
      "Iteration 18, loss = 0.65273596\n",
      "Iteration 19, loss = 0.65274507\n",
      "Iteration 20, loss = 0.65261251\n",
      "Iteration 21, loss = 0.65261825\n",
      "Iteration 22, loss = 0.65219666\n",
      "Iteration 23, loss = 0.65207813\n",
      "Iteration 24, loss = 0.65215582\n",
      "Iteration 25, loss = 0.65232330\n",
      "Iteration 26, loss = 0.65211644\n",
      "Iteration 27, loss = 0.65188275\n",
      "Iteration 28, loss = 0.65168217\n",
      "Iteration 29, loss = 0.65222851\n",
      "Iteration 30, loss = 0.65199270\n",
      "Iteration 31, loss = 0.65170316\n",
      "Iteration 32, loss = 0.65194733\n",
      "Iteration 33, loss = 0.65183386\n",
      "Iteration 34, loss = 0.65167163\n",
      "Iteration 35, loss = 0.65163851\n",
      "Iteration 36, loss = 0.65184330\n",
      "Iteration 37, loss = 0.65163509\n",
      "Iteration 38, loss = 0.65147878\n",
      "Iteration 39, loss = 0.65116941\n",
      "Iteration 40, loss = 0.65212934\n",
      "Iteration 41, loss = 0.65172812\n",
      "Iteration 42, loss = 0.65178280\n",
      "Iteration 43, loss = 0.65097652\n",
      "Iteration 44, loss = 0.65178972\n",
      "Iteration 45, loss = 0.65098685\n",
      "Iteration 46, loss = 0.65097092\n",
      "Iteration 47, loss = 0.65109411\n",
      "Iteration 48, loss = 0.65069851\n",
      "Iteration 49, loss = 0.65055692\n",
      "Iteration 50, loss = 0.65063827\n",
      "Iteration 51, loss = 0.65023397\n",
      "Iteration 52, loss = 0.65015654\n",
      "Iteration 53, loss = 0.65024495\n",
      "Iteration 54, loss = 0.65019624\n",
      "Iteration 55, loss = 0.64983137\n",
      "Iteration 56, loss = 0.64989024\n",
      "Iteration 57, loss = 0.64953962\n",
      "Iteration 58, loss = 0.64923236\n",
      "Iteration 59, loss = 0.64931353\n",
      "Iteration 60, loss = 0.64883078\n",
      "Iteration 61, loss = 0.64861112\n",
      "Iteration 62, loss = 0.64878082\n",
      "Iteration 63, loss = 0.64868577\n",
      "Iteration 64, loss = 0.64811021\n",
      "Iteration 65, loss = 0.64796803\n",
      "Iteration 66, loss = 0.64747578\n",
      "Iteration 67, loss = 0.64761140\n",
      "Iteration 68, loss = 0.64758235\n",
      "Iteration 69, loss = 0.64689784\n",
      "Iteration 70, loss = 0.64684333\n",
      "Iteration 71, loss = 0.64670454\n",
      "Iteration 72, loss = 0.64644654\n",
      "Iteration 73, loss = 0.64608233\n",
      "Iteration 74, loss = 0.64603274\n",
      "Iteration 75, loss = 0.64557149\n",
      "Iteration 76, loss = 0.64524410\n",
      "Iteration 77, loss = 0.64514196\n",
      "Iteration 78, loss = 0.64498910\n",
      "Iteration 79, loss = 0.64449349\n",
      "Iteration 80, loss = 0.64431445\n",
      "Iteration 81, loss = 0.64385764\n",
      "Iteration 82, loss = 0.64374571\n",
      "Iteration 83, loss = 0.64412529\n",
      "Iteration 84, loss = 0.64308913\n",
      "Iteration 85, loss = 0.64301850\n",
      "Iteration 86, loss = 0.64277025\n",
      "Iteration 87, loss = 0.64248124\n",
      "Iteration 88, loss = 0.64200488\n",
      "Iteration 89, loss = 0.64151324\n",
      "Iteration 90, loss = 0.64171758\n",
      "Iteration 91, loss = 0.64120136\n",
      "Iteration 92, loss = 0.64088512\n",
      "Iteration 93, loss = 0.64063753\n",
      "Iteration 94, loss = 0.64028177\n",
      "Iteration 95, loss = 0.63971831\n",
      "Iteration 96, loss = 0.63937850\n",
      "Iteration 97, loss = 0.63914867\n",
      "Iteration 98, loss = 0.63896017\n",
      "Iteration 99, loss = 0.63857281\n",
      "Iteration 100, loss = 0.63817143\n",
      "Iteration 101, loss = 0.63789231\n",
      "Iteration 102, loss = 0.63758791\n",
      "Iteration 103, loss = 0.63737137\n",
      "Iteration 104, loss = 0.63672026\n",
      "Iteration 105, loss = 0.63639485\n",
      "Iteration 106, loss = 0.63645149\n",
      "Iteration 107, loss = 0.63575493\n",
      "Iteration 108, loss = 0.63507765\n",
      "Iteration 109, loss = 0.63474691\n",
      "Iteration 110, loss = 0.63427193\n",
      "Iteration 111, loss = 0.63417668\n",
      "Iteration 112, loss = 0.63379742\n",
      "Iteration 113, loss = 0.63299442\n",
      "Iteration 114, loss = 0.63347464\n",
      "Iteration 115, loss = 0.63254848\n",
      "Iteration 116, loss = 0.63188558\n",
      "Iteration 117, loss = 0.63140660\n",
      "Iteration 118, loss = 0.63125434\n",
      "Iteration 119, loss = 0.63086337\n",
      "Iteration 120, loss = 0.63047470\n",
      "Iteration 121, loss = 0.63020916\n",
      "Iteration 122, loss = 0.62966535\n",
      "Iteration 123, loss = 0.62931768\n",
      "Iteration 124, loss = 0.62885817\n",
      "Iteration 125, loss = 0.62840336\n",
      "Iteration 126, loss = 0.62785389\n",
      "Iteration 127, loss = 0.62756582\n",
      "Iteration 128, loss = 0.62707051\n",
      "Iteration 129, loss = 0.62678916\n",
      "Iteration 130, loss = 0.62645718\n",
      "Iteration 131, loss = 0.62615318\n",
      "Iteration 132, loss = 0.62554859\n",
      "Iteration 133, loss = 0.62549720\n",
      "Iteration 134, loss = 0.62513934\n",
      "Iteration 135, loss = 0.62436013\n",
      "Iteration 136, loss = 0.62396024\n",
      "Iteration 137, loss = 0.62367765\n",
      "Iteration 138, loss = 0.62350016\n",
      "Iteration 139, loss = 0.62291818\n",
      "Iteration 140, loss = 0.62282305\n",
      "Iteration 141, loss = 0.62187928\n",
      "Iteration 142, loss = 0.62161970\n",
      "Iteration 143, loss = 0.62156688\n",
      "Iteration 144, loss = 0.62113929\n",
      "Iteration 145, loss = 0.62036701\n",
      "Iteration 146, loss = 0.61989756\n",
      "Iteration 147, loss = 0.61990684\n",
      "Iteration 148, loss = 0.61968859\n",
      "Iteration 149, loss = 0.61926387\n",
      "Iteration 150, loss = 0.61866074\n",
      "Iteration 151, loss = 0.61842935\n",
      "Iteration 152, loss = 0.61785071\n",
      "Iteration 153, loss = 0.61767658\n",
      "Iteration 154, loss = 0.61746441\n",
      "Iteration 155, loss = 0.61677202\n",
      "Iteration 156, loss = 0.61682961\n",
      "Iteration 157, loss = 0.61592432\n",
      "Iteration 158, loss = 0.61528748\n",
      "Iteration 159, loss = 0.61531446\n",
      "Iteration 160, loss = 0.61478434\n",
      "Iteration 161, loss = 0.61528112\n",
      "Iteration 162, loss = 0.61393950\n",
      "Iteration 163, loss = 0.61350443\n",
      "Iteration 164, loss = 0.61356063\n",
      "Iteration 165, loss = 0.61280319\n",
      "Iteration 166, loss = 0.61249084\n",
      "Iteration 167, loss = 0.61186817\n",
      "Iteration 168, loss = 0.61187421\n",
      "Iteration 169, loss = 0.61164093\n",
      "Iteration 170, loss = 0.61132254\n",
      "Iteration 171, loss = 0.61046432\n",
      "Iteration 172, loss = 0.60997841\n",
      "Iteration 173, loss = 0.60946460\n",
      "Iteration 174, loss = 0.60952259\n",
      "Iteration 175, loss = 0.60933592\n",
      "Iteration 176, loss = 0.60856625\n",
      "Iteration 177, loss = 0.60823063\n",
      "Iteration 178, loss = 0.60756247\n",
      "Iteration 179, loss = 0.60707237\n",
      "Iteration 180, loss = 0.60673420\n",
      "Iteration 181, loss = 0.60639433\n",
      "Iteration 182, loss = 0.60629112\n",
      "Iteration 183, loss = 0.60557428\n",
      "Iteration 184, loss = 0.60512976\n",
      "Iteration 185, loss = 0.60477966\n",
      "Iteration 186, loss = 0.60466352\n",
      "Iteration 187, loss = 0.60348571\n",
      "Iteration 188, loss = 0.60355881\n",
      "Iteration 189, loss = 0.60295459\n",
      "Iteration 190, loss = 0.60223250\n",
      "Iteration 191, loss = 0.60208440\n",
      "Iteration 192, loss = 0.60168540\n",
      "Iteration 193, loss = 0.60063910\n",
      "Iteration 194, loss = 0.60031686\n",
      "Iteration 195, loss = 0.60012742\n",
      "Iteration 196, loss = 0.59911062\n",
      "Iteration 197, loss = 0.59918047\n",
      "Iteration 198, loss = 0.59848561\n",
      "Iteration 199, loss = 0.59845826\n",
      "Iteration 200, loss = 0.59783259\n",
      "Iteration 201, loss = 0.59774511\n",
      "Iteration 202, loss = 0.59684502\n",
      "Iteration 203, loss = 0.59671607\n",
      "Iteration 204, loss = 0.59584250\n",
      "Iteration 205, loss = 0.59574613\n",
      "Iteration 206, loss = 0.59483539\n",
      "Iteration 207, loss = 0.59476892\n",
      "Iteration 208, loss = 0.59369732\n",
      "Iteration 209, loss = 0.59446709\n",
      "Iteration 210, loss = 0.59287364\n",
      "Iteration 211, loss = 0.59315222\n",
      "Iteration 212, loss = 0.59234175\n",
      "Iteration 213, loss = 0.59171744\n",
      "Iteration 214, loss = 0.59081360\n",
      "Iteration 215, loss = 0.59054661\n",
      "Iteration 216, loss = 0.58998788\n",
      "Iteration 217, loss = 0.59045855\n",
      "Iteration 218, loss = 0.58905564\n",
      "Iteration 219, loss = 0.58942158\n",
      "Iteration 220, loss = 0.58860330\n",
      "Iteration 221, loss = 0.58734306\n",
      "Iteration 222, loss = 0.58768013\n",
      "Iteration 223, loss = 0.58696780\n",
      "Iteration 224, loss = 0.58635882\n",
      "Iteration 225, loss = 0.58589239\n",
      "Iteration 226, loss = 0.58515546\n",
      "Iteration 227, loss = 0.58486407\n",
      "Iteration 228, loss = 0.58457091\n",
      "Iteration 229, loss = 0.58348954\n",
      "Iteration 230, loss = 0.58318689\n",
      "Iteration 231, loss = 0.58240305\n",
      "Iteration 232, loss = 0.58163594\n",
      "Iteration 233, loss = 0.58233857\n",
      "Iteration 234, loss = 0.58055012\n",
      "Iteration 235, loss = 0.58068873\n",
      "Iteration 236, loss = 0.57985163\n",
      "Iteration 237, loss = 0.57917198\n",
      "Iteration 238, loss = 0.57889123\n",
      "Iteration 239, loss = 0.57818759\n",
      "Iteration 240, loss = 0.57839100\n",
      "Iteration 241, loss = 0.57763095\n",
      "Iteration 242, loss = 0.57680316\n",
      "Iteration 243, loss = 0.57655684\n",
      "Iteration 244, loss = 0.57604379\n",
      "Iteration 245, loss = 0.57566799\n",
      "Iteration 246, loss = 0.57469352\n",
      "Iteration 247, loss = 0.57408104\n",
      "Iteration 248, loss = 0.57390944\n",
      "Iteration 249, loss = 0.57311537\n",
      "Iteration 250, loss = 0.57234167\n",
      "Iteration 251, loss = 0.57279585\n",
      "Iteration 252, loss = 0.57194881\n",
      "Iteration 253, loss = 0.57076062\n",
      "Iteration 254, loss = 0.57129317\n",
      "Iteration 255, loss = 0.57083338\n",
      "Iteration 256, loss = 0.57004844\n",
      "Iteration 257, loss = 0.56911834\n",
      "Iteration 258, loss = 0.56787844\n",
      "Iteration 259, loss = 0.56785534\n",
      "Iteration 260, loss = 0.56742414\n",
      "Iteration 261, loss = 0.56726185\n",
      "Iteration 262, loss = 0.56618833\n",
      "Iteration 263, loss = 0.56581919\n",
      "Iteration 264, loss = 0.56514422\n",
      "Iteration 265, loss = 0.56526886\n",
      "Iteration 266, loss = 0.56436574\n",
      "Iteration 267, loss = 0.56397721\n",
      "Iteration 268, loss = 0.56418666\n",
      "Iteration 269, loss = 0.56326611\n",
      "Iteration 270, loss = 0.56249999\n",
      "Iteration 271, loss = 0.56204812\n",
      "Iteration 272, loss = 0.56051991\n",
      "Iteration 273, loss = 0.56073558\n",
      "Iteration 274, loss = 0.56040085\n",
      "Iteration 275, loss = 0.56002410\n",
      "Iteration 276, loss = 0.55923349\n",
      "Iteration 277, loss = 0.55927439\n",
      "Iteration 278, loss = 0.55771622\n",
      "Iteration 279, loss = 0.55700110\n",
      "Iteration 280, loss = 0.55778073\n",
      "Iteration 281, loss = 0.55706363\n",
      "Iteration 282, loss = 0.55575563\n",
      "Iteration 283, loss = 0.55572408\n",
      "Iteration 284, loss = 0.55559748\n",
      "Iteration 285, loss = 0.55524878\n",
      "Iteration 286, loss = 0.55439276\n",
      "Iteration 287, loss = 0.55354868\n",
      "Iteration 288, loss = 0.55405600\n",
      "Iteration 289, loss = 0.55221463\n",
      "Iteration 290, loss = 0.55245106\n",
      "Iteration 291, loss = 0.55245614\n",
      "Iteration 292, loss = 0.55103446\n",
      "Iteration 293, loss = 0.55088822\n",
      "Iteration 294, loss = 0.54967623\n",
      "Iteration 295, loss = 0.54969729\n",
      "Iteration 296, loss = 0.55004464\n",
      "Iteration 297, loss = 0.54860431\n",
      "Iteration 298, loss = 0.54893707\n",
      "Iteration 299, loss = 0.54755219\n",
      "Iteration 300, loss = 0.54718624\n",
      "Iteration 301, loss = 0.54690526\n",
      "Iteration 302, loss = 0.54626015\n",
      "Iteration 303, loss = 0.54555225\n",
      "Iteration 304, loss = 0.54470789\n",
      "Iteration 305, loss = 0.54481000\n",
      "Iteration 306, loss = 0.54472757\n",
      "Iteration 307, loss = 0.54382151\n",
      "Iteration 308, loss = 0.54335883\n",
      "Iteration 309, loss = 0.54325629\n",
      "Iteration 310, loss = 0.54146581\n",
      "Iteration 311, loss = 0.54175206\n",
      "Iteration 312, loss = 0.54152941\n",
      "Iteration 313, loss = 0.54056400\n",
      "Iteration 314, loss = 0.54078045\n",
      "Iteration 315, loss = 0.54008098\n",
      "Iteration 316, loss = 0.53902177\n",
      "Iteration 317, loss = 0.53955978\n",
      "Iteration 318, loss = 0.53825142\n",
      "Iteration 319, loss = 0.53814457\n",
      "Iteration 320, loss = 0.53750893\n",
      "Iteration 321, loss = 0.53686548\n",
      "Iteration 322, loss = 0.53731911\n",
      "Iteration 323, loss = 0.53669228\n",
      "Iteration 324, loss = 0.53687094\n",
      "Iteration 325, loss = 0.53616616\n",
      "Iteration 326, loss = 0.53566708\n",
      "Iteration 327, loss = 0.53478203\n",
      "Iteration 328, loss = 0.53426344\n",
      "Iteration 329, loss = 0.53346374\n",
      "Iteration 330, loss = 0.53321436\n",
      "Iteration 331, loss = 0.53317560\n",
      "Iteration 332, loss = 0.53299321\n",
      "Iteration 333, loss = 0.53241072\n",
      "Iteration 334, loss = 0.53200922\n",
      "Iteration 335, loss = 0.53137728\n",
      "Iteration 336, loss = 0.53078166\n",
      "Iteration 337, loss = 0.53016734\n",
      "Iteration 338, loss = 0.52968668\n",
      "Iteration 339, loss = 0.53061137\n",
      "Iteration 340, loss = 0.52917432\n",
      "Iteration 341, loss = 0.52888987\n",
      "Iteration 342, loss = 0.52756311\n",
      "Iteration 343, loss = 0.52862191\n",
      "Iteration 344, loss = 0.52788011\n",
      "Iteration 345, loss = 0.52672457\n",
      "Iteration 346, loss = 0.52765752\n",
      "Iteration 347, loss = 0.52557168\n",
      "Iteration 348, loss = 0.52555733\n",
      "Iteration 349, loss = 0.52551920\n",
      "Iteration 350, loss = 0.52594515\n",
      "Iteration 351, loss = 0.52392530\n",
      "Iteration 352, loss = 0.52444356\n",
      "Iteration 353, loss = 0.52430037\n",
      "Iteration 354, loss = 0.52314936\n",
      "Iteration 355, loss = 0.52354075\n",
      "Iteration 356, loss = 0.52244605\n",
      "Iteration 357, loss = 0.52204411\n",
      "Iteration 358, loss = 0.52259409\n",
      "Iteration 359, loss = 0.52115990\n",
      "Iteration 360, loss = 0.52143911\n",
      "Iteration 361, loss = 0.52167115\n",
      "Iteration 362, loss = 0.52026511\n",
      "Iteration 363, loss = 0.52029090\n",
      "Iteration 364, loss = 0.52020877\n",
      "Iteration 365, loss = 0.51909237\n",
      "Iteration 366, loss = 0.51856476\n",
      "Iteration 367, loss = 0.51806495\n",
      "Iteration 368, loss = 0.51833128\n",
      "Iteration 369, loss = 0.51786898\n",
      "Iteration 370, loss = 0.51793290\n",
      "Iteration 371, loss = 0.51682057\n",
      "Iteration 372, loss = 0.51666222\n",
      "Iteration 373, loss = 0.51598362\n",
      "Iteration 374, loss = 0.51589352\n",
      "Iteration 375, loss = 0.51676657\n",
      "Iteration 376, loss = 0.51536828\n",
      "Iteration 377, loss = 0.51531759\n",
      "Iteration 378, loss = 0.51412591\n",
      "Iteration 379, loss = 0.51399572\n",
      "Iteration 380, loss = 0.51400755\n",
      "Iteration 381, loss = 0.51318745\n",
      "Iteration 382, loss = 0.51244512\n",
      "Iteration 383, loss = 0.51333864\n",
      "Iteration 384, loss = 0.51244745\n",
      "Iteration 385, loss = 0.51244116\n",
      "Iteration 386, loss = 0.51118204\n",
      "Iteration 387, loss = 0.51124361\n",
      "Iteration 388, loss = 0.51052614\n",
      "Iteration 389, loss = 0.51106107\n",
      "Iteration 390, loss = 0.51029772\n",
      "Iteration 391, loss = 0.50945193\n",
      "Iteration 392, loss = 0.51031665\n",
      "Iteration 393, loss = 0.50978202\n",
      "Iteration 394, loss = 0.50871243\n",
      "Iteration 395, loss = 0.50867567\n",
      "Iteration 396, loss = 0.50880149\n",
      "Iteration 397, loss = 0.50827056\n",
      "Iteration 398, loss = 0.50775978\n",
      "Iteration 399, loss = 0.50729599\n",
      "Iteration 400, loss = 0.50697461\n",
      "Iteration 401, loss = 0.50692852\n",
      "Iteration 402, loss = 0.50649948\n",
      "Iteration 403, loss = 0.50613725\n",
      "Iteration 404, loss = 0.50578206\n",
      "Iteration 405, loss = 0.50500304\n",
      "Iteration 406, loss = 0.50583712\n",
      "Iteration 407, loss = 0.50460640\n",
      "Iteration 408, loss = 0.50500333\n",
      "Iteration 409, loss = 0.50417345\n",
      "Iteration 410, loss = 0.50464049\n",
      "Iteration 411, loss = 0.50409336\n",
      "Iteration 412, loss = 0.50353728\n",
      "Iteration 413, loss = 0.50259210\n",
      "Iteration 414, loss = 0.50273239\n",
      "Iteration 415, loss = 0.50224530\n",
      "Iteration 416, loss = 0.50213594\n",
      "Iteration 417, loss = 0.50178888\n",
      "Iteration 418, loss = 0.50134131\n",
      "Iteration 419, loss = 0.50194064\n",
      "Iteration 420, loss = 0.50125734\n",
      "Iteration 421, loss = 0.50047757\n",
      "Iteration 422, loss = 0.50069720\n",
      "Iteration 423, loss = 0.50092333\n",
      "Iteration 424, loss = 0.49998104\n",
      "Iteration 425, loss = 0.49942273\n",
      "Iteration 426, loss = 0.49919036\n",
      "Iteration 427, loss = 0.49889869\n",
      "Iteration 428, loss = 0.49842101\n",
      "Iteration 429, loss = 0.49848972\n",
      "Iteration 430, loss = 0.49814935\n",
      "Iteration 431, loss = 0.49783523\n",
      "Iteration 432, loss = 0.49706553\n",
      "Iteration 433, loss = 0.49763559\n",
      "Iteration 434, loss = 0.49645227\n",
      "Iteration 435, loss = 0.49698819\n",
      "Iteration 436, loss = 0.49654782\n",
      "Iteration 437, loss = 0.49659764\n",
      "Iteration 438, loss = 0.49543013\n",
      "Iteration 439, loss = 0.49553550\n",
      "Iteration 440, loss = 0.49579198\n",
      "Iteration 441, loss = 0.49435318\n",
      "Iteration 442, loss = 0.49598547\n",
      "Iteration 443, loss = 0.49483822\n",
      "Iteration 444, loss = 0.49479097\n",
      "Iteration 445, loss = 0.49372095\n",
      "Iteration 446, loss = 0.49359333\n",
      "Iteration 447, loss = 0.49361052\n",
      "Iteration 448, loss = 0.49283525\n",
      "Iteration 449, loss = 0.49259329\n",
      "Iteration 450, loss = 0.49293009\n",
      "Iteration 451, loss = 0.49192574\n",
      "Iteration 452, loss = 0.49165776\n",
      "Iteration 453, loss = 0.49107607\n",
      "Iteration 454, loss = 0.49207137\n",
      "Iteration 455, loss = 0.49055095\n",
      "Iteration 456, loss = 0.49061625\n",
      "Iteration 457, loss = 0.49090826\n",
      "Iteration 458, loss = 0.49038450\n",
      "Iteration 459, loss = 0.49070066\n",
      "Iteration 460, loss = 0.48977043\n",
      "Iteration 461, loss = 0.49037990\n",
      "Iteration 462, loss = 0.48940992\n",
      "Iteration 463, loss = 0.48939117\n",
      "Iteration 464, loss = 0.48973033\n",
      "Iteration 465, loss = 0.48954590\n",
      "Iteration 466, loss = 0.48842058\n",
      "Iteration 467, loss = 0.48856862\n",
      "Iteration 468, loss = 0.48725860\n",
      "Iteration 469, loss = 0.48763085\n",
      "Iteration 470, loss = 0.48850893\n",
      "Iteration 471, loss = 0.48688878\n",
      "Iteration 472, loss = 0.48731727\n",
      "Iteration 473, loss = 0.48630896\n",
      "Iteration 474, loss = 0.48590150\n",
      "Iteration 475, loss = 0.48609258\n",
      "Iteration 476, loss = 0.48566172\n",
      "Iteration 477, loss = 0.48638612\n",
      "Iteration 478, loss = 0.48522523\n",
      "Iteration 479, loss = 0.48534822\n",
      "Iteration 480, loss = 0.48500257\n",
      "Iteration 481, loss = 0.48422797\n",
      "Iteration 482, loss = 0.48510374\n",
      "Iteration 483, loss = 0.48393519\n",
      "Iteration 484, loss = 0.48443732\n",
      "Iteration 485, loss = 0.48494247\n",
      "Iteration 486, loss = 0.48349876\n",
      "Iteration 487, loss = 0.48294472\n",
      "Iteration 488, loss = 0.48326396\n",
      "Iteration 489, loss = 0.48354745\n",
      "Iteration 490, loss = 0.48257789\n",
      "Iteration 491, loss = 0.48253713\n",
      "Iteration 492, loss = 0.48169359\n",
      "Iteration 493, loss = 0.48223938\n",
      "Iteration 494, loss = 0.48180185\n",
      "Iteration 495, loss = 0.48124542\n",
      "Iteration 496, loss = 0.48152340\n",
      "Iteration 497, loss = 0.48131993\n",
      "Iteration 498, loss = 0.48033106\n",
      "Iteration 499, loss = 0.48037074\n",
      "Iteration 500, loss = 0.48010771\n",
      "Iteration 501, loss = 0.47988805\n",
      "Iteration 502, loss = 0.48035148\n",
      "Iteration 503, loss = 0.47959780\n",
      "Iteration 504, loss = 0.47852841\n",
      "Iteration 505, loss = 0.47879311\n",
      "Iteration 506, loss = 0.47906009\n",
      "Iteration 507, loss = 0.47897280\n",
      "Iteration 508, loss = 0.47814173\n",
      "Iteration 509, loss = 0.47741818\n",
      "Iteration 510, loss = 0.47903750\n",
      "Iteration 511, loss = 0.47787524\n",
      "Iteration 512, loss = 0.47694655\n",
      "Iteration 513, loss = 0.47740227\n",
      "Iteration 514, loss = 0.47777436\n",
      "Iteration 515, loss = 0.47644926\n",
      "Iteration 516, loss = 0.47645210\n",
      "Iteration 517, loss = 0.47659793\n",
      "Iteration 518, loss = 0.47668914\n",
      "Iteration 519, loss = 0.47540042\n",
      "Iteration 520, loss = 0.47572903\n",
      "Iteration 521, loss = 0.47593025\n",
      "Iteration 522, loss = 0.47462504\n",
      "Iteration 523, loss = 0.47444645\n",
      "Iteration 524, loss = 0.47519245\n",
      "Iteration 525, loss = 0.47447933\n",
      "Iteration 526, loss = 0.47518811\n",
      "Iteration 527, loss = 0.47504306\n",
      "Iteration 528, loss = 0.47343199\n",
      "Iteration 529, loss = 0.47309357\n",
      "Iteration 530, loss = 0.47418448\n",
      "Iteration 531, loss = 0.47335392\n",
      "Iteration 532, loss = 0.47314558\n",
      "Iteration 533, loss = 0.47319077\n",
      "Iteration 534, loss = 0.47386271\n",
      "Iteration 535, loss = 0.47183998\n",
      "Iteration 536, loss = 0.47162252\n",
      "Iteration 537, loss = 0.47247358\n",
      "Iteration 538, loss = 0.47194327\n",
      "Iteration 539, loss = 0.47138339\n",
      "Iteration 540, loss = 0.47112781\n",
      "Iteration 541, loss = 0.47116606\n",
      "Iteration 542, loss = 0.47084491\n",
      "Iteration 543, loss = 0.47102592\n",
      "Iteration 544, loss = 0.46974245\n",
      "Iteration 545, loss = 0.47084514\n",
      "Iteration 546, loss = 0.47034746\n",
      "Iteration 547, loss = 0.46908109\n",
      "Iteration 548, loss = 0.46921608\n",
      "Iteration 549, loss = 0.46998735\n",
      "Iteration 550, loss = 0.46887678\n",
      "Iteration 551, loss = 0.46816023\n",
      "Iteration 552, loss = 0.46784143\n",
      "Iteration 553, loss = 0.46760967\n",
      "Iteration 554, loss = 0.46831833\n",
      "Iteration 555, loss = 0.46784529\n",
      "Iteration 556, loss = 0.46831148\n",
      "Iteration 557, loss = 0.46832162\n",
      "Iteration 558, loss = 0.46757549\n",
      "Iteration 559, loss = 0.46738424\n",
      "Iteration 560, loss = 0.46720719\n",
      "Iteration 561, loss = 0.46642570\n",
      "Iteration 562, loss = 0.46688686\n",
      "Iteration 563, loss = 0.46540873\n",
      "Iteration 564, loss = 0.46677500\n",
      "Iteration 565, loss = 0.46598169\n",
      "Iteration 566, loss = 0.46654381\n",
      "Iteration 567, loss = 0.46549044\n",
      "Iteration 568, loss = 0.46510012\n",
      "Iteration 569, loss = 0.46475290\n",
      "Iteration 570, loss = 0.46424957\n",
      "Iteration 571, loss = 0.46561591\n",
      "Iteration 572, loss = 0.46487876\n",
      "Iteration 573, loss = 0.46472215\n",
      "Iteration 574, loss = 0.46388701\n",
      "Iteration 575, loss = 0.46381635\n",
      "Iteration 576, loss = 0.46375704\n",
      "Iteration 577, loss = 0.46270134\n",
      "Iteration 578, loss = 0.46291481\n",
      "Iteration 579, loss = 0.46359091\n",
      "Iteration 580, loss = 0.46293102\n",
      "Iteration 581, loss = 0.46265394\n",
      "Iteration 582, loss = 0.46245677\n",
      "Iteration 583, loss = 0.46175896\n",
      "Iteration 584, loss = 0.46210086\n",
      "Iteration 585, loss = 0.46217612\n",
      "Iteration 586, loss = 0.46223854\n",
      "Iteration 587, loss = 0.46193101\n",
      "Iteration 588, loss = 0.46124816\n",
      "Iteration 589, loss = 0.46097050\n",
      "Iteration 590, loss = 0.46062521\n",
      "Iteration 591, loss = 0.46211686\n",
      "Iteration 592, loss = 0.46079894\n",
      "Iteration 593, loss = 0.46007901\n",
      "Iteration 594, loss = 0.46074948\n",
      "Iteration 595, loss = 0.45949054\n",
      "Iteration 596, loss = 0.45936311\n",
      "Iteration 597, loss = 0.45946981\n",
      "Iteration 598, loss = 0.45949591\n",
      "Iteration 599, loss = 0.45830332\n",
      "Iteration 600, loss = 0.45898367\n",
      "Iteration 601, loss = 0.45938128\n",
      "Iteration 602, loss = 0.45838832\n",
      "Iteration 603, loss = 0.45747165\n",
      "Iteration 604, loss = 0.45783899\n",
      "Iteration 605, loss = 0.45844090\n",
      "Iteration 606, loss = 0.45812174\n",
      "Iteration 607, loss = 0.45748727\n",
      "Iteration 608, loss = 0.45739704\n",
      "Iteration 609, loss = 0.45679680\n",
      "Iteration 610, loss = 0.45701418\n",
      "Iteration 611, loss = 0.45682816\n",
      "Iteration 612, loss = 0.45718700\n",
      "Iteration 613, loss = 0.45644073\n",
      "Iteration 614, loss = 0.45645716\n",
      "Iteration 615, loss = 0.45603579\n",
      "Iteration 616, loss = 0.45606945\n",
      "Iteration 617, loss = 0.45602985\n",
      "Iteration 618, loss = 0.45591262\n",
      "Iteration 619, loss = 0.45684191\n",
      "Iteration 620, loss = 0.45540726\n",
      "Iteration 621, loss = 0.45503493\n",
      "Iteration 622, loss = 0.45488502\n",
      "Iteration 623, loss = 0.45409522\n",
      "Iteration 624, loss = 0.45414789\n",
      "Iteration 625, loss = 0.45424885\n",
      "Iteration 626, loss = 0.45380375\n",
      "Iteration 627, loss = 0.45398192\n",
      "Iteration 628, loss = 0.45470673\n",
      "Iteration 629, loss = 0.45294552\n",
      "Iteration 630, loss = 0.45339837\n",
      "Iteration 631, loss = 0.45207064\n",
      "Iteration 632, loss = 0.45331463\n",
      "Iteration 633, loss = 0.45333163\n",
      "Iteration 634, loss = 0.45217363\n",
      "Iteration 635, loss = 0.45247237\n",
      "Iteration 636, loss = 0.45149940\n",
      "Iteration 637, loss = 0.45177218\n",
      "Iteration 638, loss = 0.45102028\n",
      "Iteration 639, loss = 0.45199671\n",
      "Iteration 640, loss = 0.45186766\n",
      "Iteration 641, loss = 0.45102548\n",
      "Iteration 642, loss = 0.45083644\n",
      "Iteration 643, loss = 0.45031198\n",
      "Iteration 644, loss = 0.45071545\n",
      "Iteration 645, loss = 0.45107027\n",
      "Iteration 646, loss = 0.45090862\n",
      "Iteration 647, loss = 0.44998004\n",
      "Iteration 648, loss = 0.44950718\n",
      "Iteration 649, loss = 0.44928148\n",
      "Iteration 650, loss = 0.44917209\n",
      "Iteration 651, loss = 0.44877338\n",
      "Iteration 652, loss = 0.44934661\n",
      "Iteration 653, loss = 0.44899273\n",
      "Iteration 654, loss = 0.44858728\n",
      "Iteration 655, loss = 0.44836237\n",
      "Iteration 656, loss = 0.44931948\n",
      "Iteration 657, loss = 0.44787559\n",
      "Iteration 658, loss = 0.44867476\n",
      "Iteration 659, loss = 0.44777334\n",
      "Iteration 660, loss = 0.44725918\n",
      "Iteration 661, loss = 0.44682435\n",
      "Iteration 662, loss = 0.44658194\n",
      "Iteration 663, loss = 0.44693736\n",
      "Iteration 664, loss = 0.44720156\n",
      "Iteration 665, loss = 0.44711391\n",
      "Iteration 666, loss = 0.44592341\n",
      "Iteration 667, loss = 0.44580217\n",
      "Iteration 668, loss = 0.44539736\n",
      "Iteration 669, loss = 0.44526101\n",
      "Iteration 670, loss = 0.44588216\n",
      "Iteration 671, loss = 0.44607996\n",
      "Iteration 672, loss = 0.44545896\n",
      "Iteration 673, loss = 0.44464379\n",
      "Iteration 674, loss = 0.44521134\n",
      "Iteration 675, loss = 0.44432298\n",
      "Iteration 676, loss = 0.44439922\n",
      "Iteration 677, loss = 0.44425333\n",
      "Iteration 678, loss = 0.44441002\n",
      "Iteration 679, loss = 0.44395491\n",
      "Iteration 680, loss = 0.44335724\n",
      "Iteration 681, loss = 0.44302322\n",
      "Iteration 682, loss = 0.44353577\n",
      "Iteration 683, loss = 0.44294900\n",
      "Iteration 684, loss = 0.44248482\n",
      "Iteration 685, loss = 0.44235642\n",
      "Iteration 686, loss = 0.44252570\n",
      "Iteration 687, loss = 0.44249330\n",
      "Iteration 688, loss = 0.44165563\n",
      "Iteration 689, loss = 0.44213030\n",
      "Iteration 690, loss = 0.44165208\n",
      "Iteration 691, loss = 0.44129576\n",
      "Iteration 692, loss = 0.44121215\n",
      "Iteration 693, loss = 0.44078920\n",
      "Iteration 694, loss = 0.44137114\n",
      "Iteration 695, loss = 0.44091801\n",
      "Iteration 696, loss = 0.44008452\n",
      "Iteration 697, loss = 0.43980282\n",
      "Iteration 698, loss = 0.43983179\n",
      "Iteration 699, loss = 0.44067613\n",
      "Iteration 700, loss = 0.44064315\n",
      "Iteration 701, loss = 0.43898480\n",
      "Iteration 702, loss = 0.43889167\n",
      "Iteration 703, loss = 0.43864606\n",
      "Iteration 704, loss = 0.43869349\n",
      "Iteration 705, loss = 0.43892743\n",
      "Iteration 706, loss = 0.43844068\n",
      "Iteration 707, loss = 0.43817775\n",
      "Iteration 708, loss = 0.43795200\n",
      "Iteration 709, loss = 0.43819414\n",
      "Iteration 710, loss = 0.43721720\n",
      "Iteration 711, loss = 0.43782874\n",
      "Iteration 712, loss = 0.43828603\n",
      "Iteration 713, loss = 0.43767751\n",
      "Iteration 714, loss = 0.43737791\n",
      "Iteration 715, loss = 0.43670931\n",
      "Iteration 716, loss = 0.43665538\n",
      "Iteration 717, loss = 0.43655189\n",
      "Iteration 718, loss = 0.43680629\n",
      "Iteration 719, loss = 0.43700826\n",
      "Iteration 720, loss = 0.43646019\n",
      "Iteration 721, loss = 0.43525122\n",
      "Iteration 722, loss = 0.43449670\n",
      "Iteration 723, loss = 0.43453686\n",
      "Iteration 724, loss = 0.43489651\n",
      "Iteration 725, loss = 0.43510618\n",
      "Iteration 726, loss = 0.43427591\n",
      "Iteration 727, loss = 0.43359354\n",
      "Iteration 728, loss = 0.43468958\n",
      "Iteration 729, loss = 0.43445448\n",
      "Iteration 730, loss = 0.43420966\n",
      "Iteration 731, loss = 0.43345368\n",
      "Iteration 732, loss = 0.43435760\n",
      "Iteration 733, loss = 0.43297341\n",
      "Iteration 734, loss = 0.43329514\n",
      "Iteration 735, loss = 0.43315014\n",
      "Iteration 736, loss = 0.43260347\n",
      "Iteration 737, loss = 0.43311424\n",
      "Iteration 738, loss = 0.43162996\n",
      "Iteration 739, loss = 0.43217715\n",
      "Iteration 740, loss = 0.43248762\n",
      "Iteration 741, loss = 0.43172515\n",
      "Iteration 742, loss = 0.43103217\n",
      "Iteration 743, loss = 0.43136243\n",
      "Iteration 744, loss = 0.43093317\n",
      "Iteration 745, loss = 0.43114379\n",
      "Iteration 746, loss = 0.43133276\n",
      "Iteration 747, loss = 0.43039939\n",
      "Iteration 748, loss = 0.43021411\n",
      "Iteration 749, loss = 0.43036921\n",
      "Iteration 750, loss = 0.43013349\n",
      "Iteration 751, loss = 0.42948742\n",
      "Iteration 752, loss = 0.42958613\n",
      "Iteration 753, loss = 0.42984052\n",
      "Iteration 754, loss = 0.42845437\n",
      "Iteration 755, loss = 0.42927689\n",
      "Iteration 756, loss = 0.42946745\n",
      "Iteration 757, loss = 0.42840159\n",
      "Iteration 758, loss = 0.42834649\n",
      "Iteration 759, loss = 0.42850319\n",
      "Iteration 760, loss = 0.42807569\n",
      "Iteration 761, loss = 0.42794932\n",
      "Iteration 762, loss = 0.42730360\n",
      "Iteration 763, loss = 0.42741413\n",
      "Iteration 764, loss = 0.42787773\n",
      "Iteration 765, loss = 0.42684699\n",
      "Iteration 766, loss = 0.42644689\n",
      "Iteration 767, loss = 0.42554424\n",
      "Iteration 768, loss = 0.42625152\n",
      "Iteration 769, loss = 0.42634460\n",
      "Iteration 770, loss = 0.42597561\n",
      "Iteration 771, loss = 0.42629519\n",
      "Iteration 772, loss = 0.42482984\n",
      "Iteration 773, loss = 0.42669193\n",
      "Iteration 774, loss = 0.42544167\n",
      "Iteration 775, loss = 0.42448276\n",
      "Iteration 776, loss = 0.42432889\n",
      "Iteration 777, loss = 0.42495923\n",
      "Iteration 778, loss = 0.42435609\n",
      "Iteration 779, loss = 0.42415360\n",
      "Iteration 780, loss = 0.42363117\n",
      "Iteration 781, loss = 0.42348120\n",
      "Iteration 782, loss = 0.42377054\n",
      "Iteration 783, loss = 0.42339791\n",
      "Iteration 784, loss = 0.42366123\n",
      "Iteration 785, loss = 0.42237146\n",
      "Iteration 786, loss = 0.42213379\n",
      "Iteration 787, loss = 0.42253706\n",
      "Iteration 788, loss = 0.42188026\n",
      "Iteration 789, loss = 0.42245056\n",
      "Iteration 790, loss = 0.42148385\n",
      "Iteration 791, loss = 0.42169099\n",
      "Iteration 792, loss = 0.42174965\n",
      "Iteration 793, loss = 0.42049795\n",
      "Iteration 794, loss = 0.42062433\n",
      "Iteration 795, loss = 0.42203608\n",
      "Iteration 796, loss = 0.42065511\n",
      "Iteration 797, loss = 0.41926925\n",
      "Iteration 798, loss = 0.42033265\n",
      "Iteration 799, loss = 0.41861866\n",
      "Iteration 800, loss = 0.41977327\n",
      "Iteration 801, loss = 0.42119063\n",
      "Iteration 802, loss = 0.41893506\n",
      "Iteration 803, loss = 0.41981649\n",
      "Iteration 804, loss = 0.41816263\n",
      "Iteration 805, loss = 0.41859878\n",
      "Iteration 806, loss = 0.41883417\n",
      "Iteration 807, loss = 0.41889498\n",
      "Iteration 808, loss = 0.41763329\n",
      "Iteration 809, loss = 0.41752133\n",
      "Iteration 810, loss = 0.41721412\n",
      "Iteration 811, loss = 0.41808002\n",
      "Iteration 812, loss = 0.41706484\n",
      "Iteration 813, loss = 0.41672300\n",
      "Iteration 814, loss = 0.41752600\n",
      "Iteration 815, loss = 0.41620478\n",
      "Iteration 816, loss = 0.41667730\n",
      "Iteration 817, loss = 0.41551210\n",
      "Iteration 818, loss = 0.41595798\n",
      "Iteration 819, loss = 0.41573727\n",
      "Iteration 820, loss = 0.41547501\n",
      "Iteration 821, loss = 0.41572441\n",
      "Iteration 822, loss = 0.41451169\n",
      "Iteration 823, loss = 0.41557840\n",
      "Iteration 824, loss = 0.41527788\n",
      "Iteration 825, loss = 0.41354111\n",
      "Iteration 826, loss = 0.41354670\n",
      "Iteration 827, loss = 0.41393011\n",
      "Iteration 828, loss = 0.41297529\n",
      "Iteration 829, loss = 0.41329919\n",
      "Iteration 830, loss = 0.41444424\n",
      "Iteration 831, loss = 0.41255229\n",
      "Iteration 832, loss = 0.41214208\n",
      "Iteration 833, loss = 0.41283946\n",
      "Iteration 834, loss = 0.41255655\n",
      "Iteration 835, loss = 0.41264616\n",
      "Iteration 836, loss = 0.41191009\n",
      "Iteration 837, loss = 0.41118214\n",
      "Iteration 838, loss = 0.41230309\n",
      "Iteration 839, loss = 0.41153014\n",
      "Iteration 840, loss = 0.41087748\n",
      "Iteration 841, loss = 0.41091181\n",
      "Iteration 842, loss = 0.41083703\n",
      "Iteration 843, loss = 0.41125686\n",
      "Iteration 844, loss = 0.40998192\n",
      "Iteration 845, loss = 0.40995339\n",
      "Iteration 846, loss = 0.41097160\n",
      "Iteration 847, loss = 0.40854225\n",
      "Iteration 848, loss = 0.40868723\n",
      "Iteration 849, loss = 0.40860389\n",
      "Iteration 850, loss = 0.40850298\n",
      "Iteration 851, loss = 0.40827406\n",
      "Iteration 852, loss = 0.40802559\n",
      "Iteration 853, loss = 0.40780200\n",
      "Iteration 854, loss = 0.40720189\n",
      "Iteration 855, loss = 0.40818814\n",
      "Iteration 856, loss = 0.40791420\n",
      "Iteration 857, loss = 0.40772768\n",
      "Iteration 858, loss = 0.40750650\n",
      "Iteration 859, loss = 0.40663708\n",
      "Iteration 860, loss = 0.40700203\n",
      "Iteration 861, loss = 0.40615553\n",
      "Iteration 862, loss = 0.40671104\n",
      "Iteration 863, loss = 0.40572313\n",
      "Iteration 864, loss = 0.40618066\n",
      "Iteration 865, loss = 0.40514073\n",
      "Iteration 866, loss = 0.40491574\n",
      "Iteration 867, loss = 0.40466950\n",
      "Iteration 868, loss = 0.40521237\n",
      "Iteration 869, loss = 0.40514368\n",
      "Iteration 870, loss = 0.40409778\n",
      "Iteration 871, loss = 0.40343985\n",
      "Iteration 872, loss = 0.40367702\n",
      "Iteration 873, loss = 0.40487850\n",
      "Iteration 874, loss = 0.40422977\n",
      "Iteration 875, loss = 0.40306639\n",
      "Iteration 876, loss = 0.40239981\n",
      "Iteration 877, loss = 0.40282902\n",
      "Iteration 878, loss = 0.40266821\n",
      "Iteration 879, loss = 0.40274807\n",
      "Iteration 880, loss = 0.40142516\n",
      "Iteration 881, loss = 0.40196504\n",
      "Iteration 882, loss = 0.40193027\n",
      "Iteration 883, loss = 0.40171130\n",
      "Iteration 884, loss = 0.40105420\n",
      "Iteration 885, loss = 0.40092260\n",
      "Iteration 886, loss = 0.40022362\n",
      "Iteration 887, loss = 0.39996260\n",
      "Iteration 888, loss = 0.39990929\n",
      "Iteration 889, loss = 0.39985229\n",
      "Iteration 890, loss = 0.39977130\n",
      "Iteration 891, loss = 0.39963518\n",
      "Iteration 892, loss = 0.39956917\n",
      "Iteration 893, loss = 0.39946778\n",
      "Iteration 894, loss = 0.39835872\n",
      "Iteration 895, loss = 0.39899006\n",
      "Iteration 896, loss = 0.39824217\n",
      "Iteration 897, loss = 0.39782906\n",
      "Iteration 898, loss = 0.39830421\n",
      "Iteration 899, loss = 0.39768068\n",
      "Iteration 900, loss = 0.39708497\n",
      "Iteration 901, loss = 0.39678261\n",
      "Iteration 902, loss = 0.39746300\n",
      "Iteration 903, loss = 0.39801628\n",
      "Iteration 904, loss = 0.39549090\n",
      "Iteration 905, loss = 0.39629334\n",
      "Iteration 906, loss = 0.39655322\n",
      "Iteration 907, loss = 0.39532487\n",
      "Iteration 908, loss = 0.39543822\n",
      "Iteration 909, loss = 0.39507356\n",
      "Iteration 910, loss = 0.39485698\n",
      "Iteration 911, loss = 0.39429941\n",
      "Iteration 912, loss = 0.39390656\n",
      "Iteration 913, loss = 0.39475359\n",
      "Iteration 914, loss = 0.39461421\n",
      "Iteration 915, loss = 0.39345180\n",
      "Iteration 916, loss = 0.39447838\n",
      "Iteration 917, loss = 0.39389440\n",
      "Iteration 918, loss = 0.39298356\n",
      "Iteration 919, loss = 0.39294014\n",
      "Iteration 920, loss = 0.39299239\n",
      "Iteration 921, loss = 0.39257896\n",
      "Iteration 922, loss = 0.39327276\n",
      "Iteration 923, loss = 0.39115274\n",
      "Iteration 924, loss = 0.39194758\n",
      "Iteration 925, loss = 0.39120347\n",
      "Iteration 926, loss = 0.39115685\n",
      "Iteration 927, loss = 0.39087372\n",
      "Iteration 928, loss = 0.39028625\n",
      "Iteration 929, loss = 0.39053962\n",
      "Iteration 930, loss = 0.38997074\n",
      "Iteration 931, loss = 0.39038582\n",
      "Iteration 932, loss = 0.39030619\n",
      "Iteration 933, loss = 0.38939033\n",
      "Iteration 934, loss = 0.38896487\n",
      "Iteration 935, loss = 0.38835970\n",
      "Iteration 936, loss = 0.38905255\n",
      "Iteration 937, loss = 0.38905381\n",
      "Iteration 938, loss = 0.38846645\n",
      "Iteration 939, loss = 0.38745284\n",
      "Iteration 940, loss = 0.38842282\n",
      "Iteration 941, loss = 0.38766831\n",
      "Iteration 942, loss = 0.38663951\n",
      "Iteration 943, loss = 0.38726578\n",
      "Iteration 944, loss = 0.38682595\n",
      "Iteration 945, loss = 0.38770139\n",
      "Iteration 946, loss = 0.38626740\n",
      "Iteration 947, loss = 0.38656417\n",
      "Iteration 948, loss = 0.38694194\n",
      "Iteration 949, loss = 0.38654601\n",
      "Iteration 950, loss = 0.38501699\n",
      "Iteration 951, loss = 0.38539155\n",
      "Iteration 952, loss = 0.38511817\n",
      "Iteration 953, loss = 0.38582764\n",
      "Iteration 954, loss = 0.38388479\n",
      "Iteration 955, loss = 0.38385717\n",
      "Iteration 956, loss = 0.38455625\n",
      "Iteration 957, loss = 0.38381507\n",
      "Iteration 958, loss = 0.38402875\n",
      "Iteration 959, loss = 0.38333085\n",
      "Iteration 960, loss = 0.38312645\n",
      "Iteration 961, loss = 0.38270188\n",
      "Iteration 962, loss = 0.38301462\n",
      "Iteration 963, loss = 0.38294046\n",
      "Iteration 964, loss = 0.38206219\n",
      "Iteration 965, loss = 0.38170188\n",
      "Iteration 966, loss = 0.38108459\n",
      "Iteration 967, loss = 0.38115966\n",
      "Iteration 968, loss = 0.38059958\n",
      "Iteration 969, loss = 0.38074771\n",
      "Iteration 970, loss = 0.38114812\n",
      "Iteration 971, loss = 0.38066338\n",
      "Iteration 972, loss = 0.37949802\n",
      "Iteration 973, loss = 0.38073174\n",
      "Iteration 974, loss = 0.37946491\n",
      "Iteration 975, loss = 0.37999513\n",
      "Iteration 976, loss = 0.37901047\n",
      "Iteration 977, loss = 0.37921525\n",
      "Iteration 978, loss = 0.37818624\n",
      "Iteration 979, loss = 0.37820116\n",
      "Iteration 980, loss = 0.37865217\n",
      "Iteration 981, loss = 0.37818914\n",
      "Iteration 982, loss = 0.37811121\n",
      "Iteration 983, loss = 0.37829653\n",
      "Iteration 984, loss = 0.37737762\n",
      "Iteration 985, loss = 0.37704009\n",
      "Iteration 986, loss = 0.37686088\n",
      "Iteration 987, loss = 0.37706045\n",
      "Iteration 988, loss = 0.37663029\n",
      "Iteration 989, loss = 0.37578242\n",
      "Iteration 990, loss = 0.37495277\n",
      "Iteration 991, loss = 0.37631613\n",
      "Iteration 992, loss = 0.37596642\n",
      "Iteration 993, loss = 0.37476020\n",
      "Iteration 994, loss = 0.37460442\n",
      "Iteration 995, loss = 0.37429266\n",
      "Iteration 996, loss = 0.37418068\n",
      "Iteration 997, loss = 0.37442814\n",
      "Iteration 998, loss = 0.37380085\n",
      "Iteration 999, loss = 0.37387582\n",
      "Iteration 1000, loss = 0.37367618\n",
      "Iteration 1001, loss = 0.37396301\n",
      "Iteration 1002, loss = 0.37329625\n",
      "Iteration 1003, loss = 0.37266127\n",
      "Iteration 1004, loss = 0.37249987\n",
      "Iteration 1005, loss = 0.37185176\n",
      "Iteration 1006, loss = 0.37187004\n",
      "Iteration 1007, loss = 0.37256399\n",
      "Iteration 1008, loss = 0.37250762\n",
      "Iteration 1009, loss = 0.37137731\n",
      "Iteration 1010, loss = 0.37089918\n",
      "Iteration 1011, loss = 0.37092296\n",
      "Iteration 1012, loss = 0.37054045\n",
      "Iteration 1013, loss = 0.37038514\n",
      "Iteration 1014, loss = 0.37089610\n",
      "Iteration 1015, loss = 0.36991503\n",
      "Iteration 1016, loss = 0.37040771\n",
      "Iteration 1017, loss = 0.36943844\n",
      "Iteration 1018, loss = 0.36943658\n",
      "Iteration 1019, loss = 0.36871625\n",
      "Iteration 1020, loss = 0.36885232\n",
      "Iteration 1021, loss = 0.36950207\n",
      "Iteration 1022, loss = 0.36812830\n",
      "Iteration 1023, loss = 0.36844247\n",
      "Iteration 1024, loss = 0.36812572\n",
      "Iteration 1025, loss = 0.36841053\n",
      "Iteration 1026, loss = 0.36745422\n",
      "Iteration 1027, loss = 0.36686116\n",
      "Iteration 1028, loss = 0.36783690\n",
      "Iteration 1029, loss = 0.36660516\n",
      "Iteration 1030, loss = 0.36719898\n",
      "Iteration 1031, loss = 0.36638017\n",
      "Iteration 1032, loss = 0.36726441\n",
      "Iteration 1033, loss = 0.36616856\n",
      "Iteration 1034, loss = 0.36487340\n",
      "Iteration 1035, loss = 0.36582537\n",
      "Iteration 1036, loss = 0.36568259\n",
      "Iteration 1037, loss = 0.36538159\n",
      "Iteration 1038, loss = 0.36465981\n",
      "Iteration 1039, loss = 0.36474484\n",
      "Iteration 1040, loss = 0.36441609\n",
      "Iteration 1041, loss = 0.36506352\n",
      "Iteration 1042, loss = 0.36346720\n",
      "Iteration 1043, loss = 0.36408219\n",
      "Iteration 1044, loss = 0.36320146\n",
      "Iteration 1045, loss = 0.36239714\n",
      "Iteration 1046, loss = 0.36415465\n",
      "Iteration 1047, loss = 0.36218213\n",
      "Iteration 1048, loss = 0.36216608\n",
      "Iteration 1049, loss = 0.36214116\n",
      "Iteration 1050, loss = 0.36239288\n",
      "Iteration 1051, loss = 0.36200040\n",
      "Iteration 1052, loss = 0.36085751\n",
      "Iteration 1053, loss = 0.36222801\n",
      "Iteration 1054, loss = 0.36181582\n",
      "Iteration 1055, loss = 0.36071501\n",
      "Iteration 1056, loss = 0.36107242\n",
      "Iteration 1057, loss = 0.36070121\n",
      "Iteration 1058, loss = 0.36007027\n",
      "Iteration 1059, loss = 0.36011772\n",
      "Iteration 1060, loss = 0.35963279\n",
      "Iteration 1061, loss = 0.35963775\n",
      "Iteration 1062, loss = 0.35980539\n",
      "Iteration 1063, loss = 0.36013878\n",
      "Iteration 1064, loss = 0.35877746\n",
      "Iteration 1065, loss = 0.36001875\n",
      "Iteration 1066, loss = 0.35875438\n",
      "Iteration 1067, loss = 0.35759995\n",
      "Iteration 1068, loss = 0.35771495\n",
      "Iteration 1069, loss = 0.35784551\n",
      "Iteration 1070, loss = 0.35748567\n",
      "Iteration 1071, loss = 0.35779017\n",
      "Iteration 1072, loss = 0.35695209\n",
      "Iteration 1073, loss = 0.35713679\n",
      "Iteration 1074, loss = 0.35669573\n",
      "Iteration 1075, loss = 0.35635644\n",
      "Iteration 1076, loss = 0.35682478\n",
      "Iteration 1077, loss = 0.35580554\n",
      "Iteration 1078, loss = 0.35727142\n",
      "Iteration 1079, loss = 0.35610245\n",
      "Iteration 1080, loss = 0.35614591\n",
      "Iteration 1081, loss = 0.35581764\n",
      "Iteration 1082, loss = 0.35483716\n",
      "Iteration 1083, loss = 0.35430753\n",
      "Iteration 1084, loss = 0.35462654\n",
      "Iteration 1085, loss = 0.35420625\n",
      "Iteration 1086, loss = 0.35434335\n",
      "Iteration 1087, loss = 0.35437464\n",
      "Iteration 1088, loss = 0.35447453\n",
      "Iteration 1089, loss = 0.35405112\n",
      "Iteration 1090, loss = 0.35398416\n",
      "Iteration 1091, loss = 0.35350832\n",
      "Iteration 1092, loss = 0.35275194\n",
      "Iteration 1093, loss = 0.35375118\n",
      "Iteration 1094, loss = 0.35227236\n",
      "Iteration 1095, loss = 0.35244886\n",
      "Iteration 1096, loss = 0.35344403\n",
      "Iteration 1097, loss = 0.35248860\n",
      "Iteration 1098, loss = 0.35214975\n",
      "Iteration 1099, loss = 0.35125094\n",
      "Iteration 1100, loss = 0.35226946\n",
      "Iteration 1101, loss = 0.35128712\n",
      "Iteration 1102, loss = 0.35097989\n",
      "Iteration 1103, loss = 0.35106067\n",
      "Iteration 1104, loss = 0.35023541\n",
      "Iteration 1105, loss = 0.35050666\n",
      "Iteration 1106, loss = 0.35026524\n",
      "Iteration 1107, loss = 0.34909053\n",
      "Iteration 1108, loss = 0.34957676\n",
      "Iteration 1109, loss = 0.34975685\n",
      "Iteration 1110, loss = 0.34936172\n",
      "Iteration 1111, loss = 0.34914058\n",
      "Iteration 1112, loss = 0.34867269\n",
      "Iteration 1113, loss = 0.34857448\n",
      "Iteration 1114, loss = 0.34802391\n",
      "Iteration 1115, loss = 0.34899010\n",
      "Iteration 1116, loss = 0.34786681\n",
      "Iteration 1117, loss = 0.34811605\n",
      "Iteration 1118, loss = 0.34778050\n",
      "Iteration 1119, loss = 0.34739530\n",
      "Iteration 1120, loss = 0.34735785\n",
      "Iteration 1121, loss = 0.34660364\n",
      "Iteration 1122, loss = 0.34695402\n",
      "Iteration 1123, loss = 0.34629113\n",
      "Iteration 1124, loss = 0.34625580\n",
      "Iteration 1125, loss = 0.34673426\n",
      "Iteration 1126, loss = 0.34623064\n",
      "Iteration 1127, loss = 0.34626043\n",
      "Iteration 1128, loss = 0.34556760\n",
      "Iteration 1129, loss = 0.34490034\n",
      "Iteration 1130, loss = 0.34615088\n",
      "Iteration 1131, loss = 0.34496358\n",
      "Iteration 1132, loss = 0.34524272\n",
      "Iteration 1133, loss = 0.34631896\n",
      "Iteration 1134, loss = 0.34434829\n",
      "Iteration 1135, loss = 0.34435143\n",
      "Iteration 1136, loss = 0.34413078\n",
      "Iteration 1137, loss = 0.34491638\n",
      "Iteration 1138, loss = 0.34366056\n",
      "Iteration 1139, loss = 0.34359190\n",
      "Iteration 1140, loss = 0.34310655\n",
      "Iteration 1141, loss = 0.34351326\n",
      "Iteration 1142, loss = 0.34335112\n",
      "Iteration 1143, loss = 0.34297590\n",
      "Iteration 1144, loss = 0.34306661\n",
      "Iteration 1145, loss = 0.34181790\n",
      "Iteration 1146, loss = 0.34276340\n",
      "Iteration 1147, loss = 0.34209097\n",
      "Iteration 1148, loss = 0.34183152\n",
      "Iteration 1149, loss = 0.34152009\n",
      "Iteration 1150, loss = 0.34191984\n",
      "Iteration 1151, loss = 0.34047982\n",
      "Iteration 1152, loss = 0.34071625\n",
      "Iteration 1153, loss = 0.34097513\n",
      "Iteration 1154, loss = 0.34038950\n",
      "Iteration 1155, loss = 0.34115438\n",
      "Iteration 1156, loss = 0.34051020\n",
      "Iteration 1157, loss = 0.34059498\n",
      "Iteration 1158, loss = 0.33994765\n",
      "Iteration 1159, loss = 0.33931626\n",
      "Iteration 1160, loss = 0.33921641\n",
      "Iteration 1161, loss = 0.33955057\n",
      "Iteration 1162, loss = 0.33909174\n",
      "Iteration 1163, loss = 0.33939279\n",
      "Iteration 1164, loss = 0.33892102\n",
      "Iteration 1165, loss = 0.33855626\n",
      "Iteration 1166, loss = 0.33841136\n",
      "Iteration 1167, loss = 0.33764186\n",
      "Iteration 1168, loss = 0.33800093\n",
      "Iteration 1169, loss = 0.33816652\n",
      "Iteration 1170, loss = 0.33757199\n",
      "Iteration 1171, loss = 0.33747207\n",
      "Iteration 1172, loss = 0.33667584\n",
      "Iteration 1173, loss = 0.33713627\n",
      "Iteration 1174, loss = 0.33679755\n",
      "Iteration 1175, loss = 0.33671002\n",
      "Iteration 1176, loss = 0.33714645\n",
      "Iteration 1177, loss = 0.33616937\n",
      "Iteration 1178, loss = 0.33615719\n",
      "Iteration 1179, loss = 0.33666712\n",
      "Iteration 1180, loss = 0.33536416\n",
      "Iteration 1181, loss = 0.33532424\n",
      "Iteration 1182, loss = 0.33512778\n",
      "Iteration 1183, loss = 0.33482296\n",
      "Iteration 1184, loss = 0.33560233\n",
      "Iteration 1185, loss = 0.33447966\n",
      "Iteration 1186, loss = 0.33457498\n",
      "Iteration 1187, loss = 0.33438329\n",
      "Iteration 1188, loss = 0.33372557\n",
      "Iteration 1189, loss = 0.33395489\n",
      "Iteration 1190, loss = 0.33403766\n",
      "Iteration 1191, loss = 0.33327817\n",
      "Iteration 1192, loss = 0.33417444\n",
      "Iteration 1193, loss = 0.33297548\n",
      "Iteration 1194, loss = 0.33256117\n",
      "Iteration 1195, loss = 0.33264673\n",
      "Iteration 1196, loss = 0.33198666\n",
      "Iteration 1197, loss = 0.33200518\n",
      "Iteration 1198, loss = 0.33376106\n",
      "Iteration 1199, loss = 0.33268501\n",
      "Iteration 1200, loss = 0.33182468\n",
      "Iteration 1201, loss = 0.33087275\n",
      "Iteration 1202, loss = 0.33198124\n",
      "Iteration 1203, loss = 0.33142650\n",
      "Iteration 1204, loss = 0.33177419\n",
      "Iteration 1205, loss = 0.33122635\n",
      "Iteration 1206, loss = 0.33043599\n",
      "Iteration 1207, loss = 0.33074122\n",
      "Iteration 1208, loss = 0.33041193\n",
      "Iteration 1209, loss = 0.33045912\n",
      "Iteration 1210, loss = 0.32944797\n",
      "Iteration 1211, loss = 0.32941703\n",
      "Iteration 1212, loss = 0.33048024\n",
      "Iteration 1213, loss = 0.32926631\n",
      "Iteration 1214, loss = 0.32905191\n",
      "Iteration 1215, loss = 0.32871336\n",
      "Iteration 1216, loss = 0.32921166\n",
      "Iteration 1217, loss = 0.32838949\n",
      "Iteration 1218, loss = 0.32812297\n",
      "Iteration 1219, loss = 0.32808923\n",
      "Iteration 1220, loss = 0.32862951\n",
      "Iteration 1221, loss = 0.32778656\n",
      "Iteration 1222, loss = 0.32689918\n",
      "Iteration 1223, loss = 0.32743698\n",
      "Iteration 1224, loss = 0.32749258\n",
      "Iteration 1225, loss = 0.32711999\n",
      "Iteration 1226, loss = 0.32679584\n",
      "Iteration 1227, loss = 0.32794237\n",
      "Iteration 1228, loss = 0.32692277\n",
      "Iteration 1229, loss = 0.32659010\n",
      "Iteration 1230, loss = 0.32643792\n",
      "Iteration 1231, loss = 0.32617004\n",
      "Iteration 1232, loss = 0.32560755\n",
      "Iteration 1233, loss = 0.32565197\n",
      "Iteration 1234, loss = 0.32601853\n",
      "Iteration 1235, loss = 0.32537856\n",
      "Iteration 1236, loss = 0.32492008\n",
      "Iteration 1237, loss = 0.32455662\n",
      "Iteration 1238, loss = 0.32484307\n",
      "Iteration 1239, loss = 0.32470540\n",
      "Iteration 1240, loss = 0.32435248\n",
      "Iteration 1241, loss = 0.32482925\n",
      "Iteration 1242, loss = 0.32450279\n",
      "Iteration 1243, loss = 0.32413343\n",
      "Iteration 1244, loss = 0.32446146\n",
      "Iteration 1245, loss = 0.32322065\n",
      "Iteration 1246, loss = 0.32332030\n",
      "Iteration 1247, loss = 0.32288915\n",
      "Iteration 1248, loss = 0.32247717\n",
      "Iteration 1249, loss = 0.32277243\n",
      "Iteration 1250, loss = 0.32256383\n",
      "Iteration 1251, loss = 0.32257603\n",
      "Iteration 1252, loss = 0.32176147\n",
      "Iteration 1253, loss = 0.32194625\n",
      "Iteration 1254, loss = 0.32202642\n",
      "Iteration 1255, loss = 0.32248402\n",
      "Iteration 1256, loss = 0.32165739\n",
      "Iteration 1257, loss = 0.32132310\n",
      "Iteration 1258, loss = 0.32195711\n",
      "Iteration 1259, loss = 0.32143545\n",
      "Iteration 1260, loss = 0.32071117\n",
      "Iteration 1261, loss = 0.32112810\n",
      "Iteration 1262, loss = 0.32105611\n",
      "Iteration 1263, loss = 0.32052713\n",
      "Iteration 1264, loss = 0.32021359\n",
      "Iteration 1265, loss = 0.32010146\n",
      "Iteration 1266, loss = 0.31982398\n",
      "Iteration 1267, loss = 0.32026569\n",
      "Iteration 1268, loss = 0.31958197\n",
      "Iteration 1269, loss = 0.31975507\n",
      "Iteration 1270, loss = 0.31902453\n",
      "Iteration 1271, loss = 0.31872846\n",
      "Iteration 1272, loss = 0.31913691\n",
      "Iteration 1273, loss = 0.31865888\n",
      "Iteration 1274, loss = 0.31941940\n",
      "Iteration 1275, loss = 0.31803132\n",
      "Iteration 1276, loss = 0.31778725\n",
      "Iteration 1277, loss = 0.31838813\n",
      "Iteration 1278, loss = 0.31805145\n",
      "Iteration 1279, loss = 0.31723903\n",
      "Iteration 1280, loss = 0.31782834\n",
      "Iteration 1281, loss = 0.31755890\n",
      "Iteration 1282, loss = 0.31711456\n",
      "Iteration 1283, loss = 0.31717201\n",
      "Iteration 1284, loss = 0.31680545\n",
      "Iteration 1285, loss = 0.31653404\n",
      "Iteration 1286, loss = 0.31638698\n",
      "Iteration 1287, loss = 0.31637837\n",
      "Iteration 1288, loss = 0.31568527\n",
      "Iteration 1289, loss = 0.31536605\n",
      "Iteration 1290, loss = 0.31597350\n",
      "Iteration 1291, loss = 0.31455383\n",
      "Iteration 1292, loss = 0.31570481\n",
      "Iteration 1293, loss = 0.31484166\n",
      "Iteration 1294, loss = 0.31597984\n",
      "Iteration 1295, loss = 0.31505503\n",
      "Iteration 1296, loss = 0.31439691\n",
      "Iteration 1297, loss = 0.31434373\n",
      "Iteration 1298, loss = 0.31344342\n",
      "Iteration 1299, loss = 0.31446963\n",
      "Iteration 1300, loss = 0.31426210\n",
      "Iteration 1301, loss = 0.31361630\n",
      "Iteration 1302, loss = 0.31412433\n",
      "Iteration 1303, loss = 0.31384420\n",
      "Iteration 1304, loss = 0.31367675\n",
      "Iteration 1305, loss = 0.31273100\n",
      "Iteration 1306, loss = 0.31361485\n",
      "Iteration 1307, loss = 0.31251416\n",
      "Iteration 1308, loss = 0.31304824\n",
      "Iteration 1309, loss = 0.31234764\n",
      "Iteration 1310, loss = 0.31282260\n",
      "Iteration 1311, loss = 0.31167425\n",
      "Iteration 1312, loss = 0.31141530\n",
      "Iteration 1313, loss = 0.31194964\n",
      "Iteration 1314, loss = 0.31192749\n",
      "Iteration 1315, loss = 0.31089686\n",
      "Iteration 1316, loss = 0.31167934\n",
      "Iteration 1317, loss = 0.31116604\n",
      "Iteration 1318, loss = 0.31046113\n",
      "Iteration 1319, loss = 0.31032931\n",
      "Iteration 1320, loss = 0.31098384\n",
      "Iteration 1321, loss = 0.31045215\n",
      "Iteration 1322, loss = 0.31091868\n",
      "Iteration 1323, loss = 0.31101102\n",
      "Iteration 1324, loss = 0.31009320\n",
      "Iteration 1325, loss = 0.30960499\n",
      "Iteration 1326, loss = 0.30960790\n",
      "Iteration 1327, loss = 0.30912790\n",
      "Iteration 1328, loss = 0.30916820\n",
      "Iteration 1329, loss = 0.30948641\n",
      "Iteration 1330, loss = 0.30843629\n",
      "Iteration 1331, loss = 0.30879648\n",
      "Iteration 1332, loss = 0.30879714\n",
      "Iteration 1333, loss = 0.30817069\n",
      "Iteration 1334, loss = 0.30828224\n",
      "Iteration 1335, loss = 0.30847432\n",
      "Iteration 1336, loss = 0.30790530\n",
      "Iteration 1337, loss = 0.30753881\n",
      "Iteration 1338, loss = 0.30749055\n",
      "Iteration 1339, loss = 0.30775102\n",
      "Iteration 1340, loss = 0.30754924\n",
      "Iteration 1341, loss = 0.30683053\n",
      "Iteration 1342, loss = 0.30734266\n",
      "Iteration 1343, loss = 0.30711740\n",
      "Iteration 1344, loss = 0.30653305\n",
      "Iteration 1345, loss = 0.30620541\n",
      "Iteration 1346, loss = 0.30577476\n",
      "Iteration 1347, loss = 0.30619065\n",
      "Iteration 1348, loss = 0.30553529\n",
      "Iteration 1349, loss = 0.30547273\n",
      "Iteration 1350, loss = 0.30533184\n",
      "Iteration 1351, loss = 0.30584240\n",
      "Iteration 1352, loss = 0.30612352\n",
      "Iteration 1353, loss = 0.30466530\n",
      "Iteration 1354, loss = 0.30511273\n",
      "Iteration 1355, loss = 0.30519350\n",
      "Iteration 1356, loss = 0.30461789\n",
      "Iteration 1357, loss = 0.30441647\n",
      "Iteration 1358, loss = 0.30414953\n",
      "Iteration 1359, loss = 0.30409191\n",
      "Iteration 1360, loss = 0.30466468\n",
      "Iteration 1361, loss = 0.30390176\n",
      "Iteration 1362, loss = 0.30329776\n",
      "Iteration 1363, loss = 0.30368327\n",
      "Iteration 1364, loss = 0.30368064\n",
      "Iteration 1365, loss = 0.30356758\n",
      "Iteration 1366, loss = 0.30285454\n",
      "Iteration 1367, loss = 0.30313764\n",
      "Iteration 1368, loss = 0.30249706\n",
      "Iteration 1369, loss = 0.30245733\n",
      "Iteration 1370, loss = 0.30223719\n",
      "Iteration 1371, loss = 0.30211964\n",
      "Iteration 1372, loss = 0.30184084\n",
      "Iteration 1373, loss = 0.30199617\n",
      "Iteration 1374, loss = 0.30153583\n",
      "Iteration 1375, loss = 0.30088867\n",
      "Iteration 1376, loss = 0.30130237\n",
      "Iteration 1377, loss = 0.30254471\n",
      "Iteration 1378, loss = 0.30104221\n",
      "Iteration 1379, loss = 0.30143858\n",
      "Iteration 1380, loss = 0.30086633\n",
      "Iteration 1381, loss = 0.30048122\n",
      "Iteration 1382, loss = 0.30005713\n",
      "Iteration 1383, loss = 0.30048832\n",
      "Iteration 1384, loss = 0.30000863\n",
      "Iteration 1385, loss = 0.30030517\n",
      "Iteration 1386, loss = 0.30027814\n",
      "Iteration 1387, loss = 0.29977541\n",
      "Iteration 1388, loss = 0.29960916\n",
      "Iteration 1389, loss = 0.29986286\n",
      "Iteration 1390, loss = 0.29925994\n",
      "Iteration 1391, loss = 0.29884138\n",
      "Iteration 1392, loss = 0.29889462\n",
      "Iteration 1393, loss = 0.29884661\n",
      "Iteration 1394, loss = 0.29904088\n",
      "Iteration 1395, loss = 0.29872255\n",
      "Iteration 1396, loss = 0.29792075\n",
      "Iteration 1397, loss = 0.29786917\n",
      "Iteration 1398, loss = 0.29816680\n",
      "Iteration 1399, loss = 0.29790150\n",
      "Iteration 1400, loss = 0.29822236\n",
      "Iteration 1401, loss = 0.29763902\n",
      "Iteration 1402, loss = 0.29761909\n",
      "Iteration 1403, loss = 0.29707226\n",
      "Iteration 1404, loss = 0.29704109\n",
      "Iteration 1405, loss = 0.29807778\n",
      "Iteration 1406, loss = 0.29697626\n",
      "Iteration 1407, loss = 0.29631373\n",
      "Iteration 1408, loss = 0.29634331\n",
      "Iteration 1409, loss = 0.29651185\n",
      "Iteration 1410, loss = 0.29609389\n",
      "Iteration 1411, loss = 0.29600288\n",
      "Iteration 1412, loss = 0.29610832\n",
      "Iteration 1413, loss = 0.29566680\n",
      "Iteration 1414, loss = 0.29574237\n",
      "Iteration 1415, loss = 0.29552668\n",
      "Iteration 1416, loss = 0.29514935\n",
      "Iteration 1417, loss = 0.29515107\n",
      "Iteration 1418, loss = 0.29485646\n",
      "Iteration 1419, loss = 0.29531740\n",
      "Iteration 1420, loss = 0.29444643\n",
      "Iteration 1421, loss = 0.29472019\n",
      "Iteration 1422, loss = 0.29453228\n",
      "Iteration 1423, loss = 0.29411896\n",
      "Iteration 1424, loss = 0.29412640\n",
      "Iteration 1425, loss = 0.29386252\n",
      "Iteration 1426, loss = 0.29344635\n",
      "Iteration 1427, loss = 0.29358679\n",
      "Iteration 1428, loss = 0.29407577\n",
      "Iteration 1429, loss = 0.29353472\n",
      "Iteration 1430, loss = 0.29284125\n",
      "Iteration 1431, loss = 0.29329944\n",
      "Iteration 1432, loss = 0.29253750\n",
      "Iteration 1433, loss = 0.29322311\n",
      "Iteration 1434, loss = 0.29248342\n",
      "Iteration 1435, loss = 0.29302688\n",
      "Iteration 1436, loss = 0.29217568\n",
      "Iteration 1437, loss = 0.29210131\n",
      "Iteration 1438, loss = 0.29185049\n",
      "Iteration 1439, loss = 0.29217276\n",
      "Iteration 1440, loss = 0.29220986\n",
      "Iteration 1441, loss = 0.29153937\n",
      "Iteration 1442, loss = 0.29117140\n",
      "Iteration 1443, loss = 0.29170760\n",
      "Iteration 1444, loss = 0.29091940\n",
      "Iteration 1445, loss = 0.29077738\n",
      "Iteration 1446, loss = 0.29160365\n",
      "Iteration 1447, loss = 0.29106124\n",
      "Iteration 1448, loss = 0.29081209\n",
      "Iteration 1449, loss = 0.28982980\n",
      "Iteration 1450, loss = 0.29000226\n",
      "Iteration 1451, loss = 0.29032205\n",
      "Iteration 1452, loss = 0.29069179\n",
      "Iteration 1453, loss = 0.29040673\n",
      "Iteration 1454, loss = 0.29004311\n",
      "Iteration 1455, loss = 0.28936235\n",
      "Iteration 1456, loss = 0.28883602\n",
      "Iteration 1457, loss = 0.28968203\n",
      "Iteration 1458, loss = 0.28889701\n",
      "Iteration 1459, loss = 0.28913881\n",
      "Iteration 1460, loss = 0.28919871\n",
      "Iteration 1461, loss = 0.28892239\n",
      "Iteration 1462, loss = 0.28938431\n",
      "Iteration 1463, loss = 0.28904694\n",
      "Iteration 1464, loss = 0.28832952\n",
      "Iteration 1465, loss = 0.28776854\n",
      "Iteration 1466, loss = 0.28841602\n",
      "Iteration 1467, loss = 0.28791961\n",
      "Iteration 1468, loss = 0.28776417\n",
      "Iteration 1469, loss = 0.28791032\n",
      "Iteration 1470, loss = 0.28760790\n",
      "Iteration 1471, loss = 0.28716011\n",
      "Iteration 1472, loss = 0.28767834\n",
      "Iteration 1473, loss = 0.28695125\n",
      "Iteration 1474, loss = 0.28705501\n",
      "Iteration 1475, loss = 0.28666415\n",
      "Iteration 1476, loss = 0.28755087\n",
      "Iteration 1477, loss = 0.28726427\n",
      "Iteration 1478, loss = 0.28587747\n",
      "Iteration 1479, loss = 0.28678226\n",
      "Iteration 1480, loss = 0.28661248\n",
      "Iteration 1481, loss = 0.28593911\n",
      "Iteration 1482, loss = 0.28592986\n",
      "Iteration 1483, loss = 0.28622571\n",
      "Iteration 1484, loss = 0.28591052\n",
      "Iteration 1485, loss = 0.28583458\n",
      "Iteration 1486, loss = 0.28571361\n",
      "Iteration 1487, loss = 0.28590256\n",
      "Iteration 1488, loss = 0.28531338\n",
      "Iteration 1489, loss = 0.28462672\n",
      "Iteration 1490, loss = 0.28462967\n",
      "Iteration 1491, loss = 0.28530647\n",
      "Iteration 1492, loss = 0.28479720\n",
      "Iteration 1493, loss = 0.28445019\n",
      "Iteration 1494, loss = 0.28444951\n",
      "Iteration 1495, loss = 0.28452733\n",
      "Iteration 1496, loss = 0.28432908\n",
      "Iteration 1497, loss = 0.28469713\n",
      "Iteration 1498, loss = 0.28422095\n",
      "Iteration 1499, loss = 0.28431030\n",
      "Iteration 1500, loss = 0.28383347\n",
      "Iteration 1501, loss = 0.28341751\n",
      "Iteration 1502, loss = 0.28307055\n",
      "Iteration 1503, loss = 0.28287115\n",
      "Iteration 1504, loss = 0.28308598\n",
      "Iteration 1505, loss = 0.28318082\n",
      "Iteration 1506, loss = 0.28328326\n",
      "Iteration 1507, loss = 0.28226769\n",
      "Iteration 1508, loss = 0.28261921\n",
      "Iteration 1509, loss = 0.28304548\n",
      "Iteration 1510, loss = 0.28256871\n",
      "Iteration 1511, loss = 0.28302492\n",
      "Iteration 1512, loss = 0.28232897\n",
      "Iteration 1513, loss = 0.28190077\n",
      "Iteration 1514, loss = 0.28235633\n",
      "Iteration 1515, loss = 0.28281365\n",
      "Iteration 1516, loss = 0.28131736\n",
      "Iteration 1517, loss = 0.28146990\n",
      "Iteration 1518, loss = 0.28084652\n",
      "Iteration 1519, loss = 0.28197389\n",
      "Iteration 1520, loss = 0.28151201\n",
      "Iteration 1521, loss = 0.28070106\n",
      "Iteration 1522, loss = 0.28121947\n",
      "Iteration 1523, loss = 0.28081474\n",
      "Iteration 1524, loss = 0.28105972\n",
      "Iteration 1525, loss = 0.28113803\n",
      "Iteration 1526, loss = 0.28059946\n",
      "Iteration 1527, loss = 0.28021774\n",
      "Iteration 1528, loss = 0.27992120\n",
      "Iteration 1529, loss = 0.27993266\n",
      "Iteration 1530, loss = 0.28045951\n",
      "Iteration 1531, loss = 0.28020235\n",
      "Iteration 1532, loss = 0.27981679\n",
      "Iteration 1533, loss = 0.27952222\n",
      "Iteration 1534, loss = 0.28008833\n",
      "Iteration 1535, loss = 0.27908868\n",
      "Iteration 1536, loss = 0.27897404\n",
      "Iteration 1537, loss = 0.27941134\n",
      "Iteration 1538, loss = 0.27897183\n",
      "Iteration 1539, loss = 0.27918690\n",
      "Iteration 1540, loss = 0.27861102\n",
      "Iteration 1541, loss = 0.27923693\n",
      "Iteration 1542, loss = 0.27833535\n",
      "Iteration 1543, loss = 0.27887917\n",
      "Iteration 1544, loss = 0.27825264\n",
      "Iteration 1545, loss = 0.27828045\n",
      "Iteration 1546, loss = 0.27869591\n",
      "Iteration 1547, loss = 0.27789119\n",
      "Iteration 1548, loss = 0.27847791\n",
      "Iteration 1549, loss = 0.27840622\n",
      "Iteration 1550, loss = 0.27722100\n",
      "Iteration 1551, loss = 0.27764331\n",
      "Iteration 1552, loss = 0.27754500\n",
      "Iteration 1553, loss = 0.27750789\n",
      "Iteration 1554, loss = 0.27717445\n",
      "Iteration 1555, loss = 0.27732560\n",
      "Iteration 1556, loss = 0.27641093\n",
      "Iteration 1557, loss = 0.27716304\n",
      "Iteration 1558, loss = 0.27679052\n",
      "Iteration 1559, loss = 0.27618973\n",
      "Iteration 1560, loss = 0.27668737\n",
      "Iteration 1561, loss = 0.27647048\n",
      "Iteration 1562, loss = 0.27678685\n",
      "Iteration 1563, loss = 0.27620453\n",
      "Iteration 1564, loss = 0.27659651\n",
      "Iteration 1565, loss = 0.27614670\n",
      "Iteration 1566, loss = 0.27564735\n",
      "Iteration 1567, loss = 0.27572530\n",
      "Iteration 1568, loss = 0.27600262\n",
      "Iteration 1569, loss = 0.27532885\n",
      "Iteration 1570, loss = 0.27531500\n",
      "Iteration 1571, loss = 0.27608616\n",
      "Iteration 1572, loss = 0.27486301\n",
      "Iteration 1573, loss = 0.27494271\n",
      "Iteration 1574, loss = 0.27569061\n",
      "Iteration 1575, loss = 0.27465218\n",
      "Iteration 1576, loss = 0.27522093\n",
      "Iteration 1577, loss = 0.27389448\n",
      "Iteration 1578, loss = 0.27439776\n",
      "Iteration 1579, loss = 0.27482453\n",
      "Iteration 1580, loss = 0.27477265\n",
      "Iteration 1581, loss = 0.27470469\n",
      "Iteration 1582, loss = 0.27430515\n",
      "Iteration 1583, loss = 0.27356016\n",
      "Iteration 1584, loss = 0.27413807\n",
      "Iteration 1585, loss = 0.27352998\n",
      "Iteration 1586, loss = 0.27382014\n",
      "Iteration 1587, loss = 0.27345736\n",
      "Iteration 1588, loss = 0.27303628\n",
      "Iteration 1589, loss = 0.27374034\n",
      "Iteration 1590, loss = 0.27289376\n",
      "Iteration 1591, loss = 0.27379955\n",
      "Iteration 1592, loss = 0.27291374\n",
      "Iteration 1593, loss = 0.27276314\n",
      "Iteration 1594, loss = 0.27285595\n",
      "Iteration 1595, loss = 0.27250293\n",
      "Iteration 1596, loss = 0.27225737\n",
      "Iteration 1597, loss = 0.27351851\n",
      "Iteration 1598, loss = 0.27250100\n",
      "Iteration 1599, loss = 0.27198767\n",
      "Iteration 1600, loss = 0.27166316\n",
      "Iteration 1601, loss = 0.27158824\n",
      "Iteration 1602, loss = 0.27244668\n",
      "Iteration 1603, loss = 0.27149733\n",
      "Iteration 1604, loss = 0.27168575\n",
      "Iteration 1605, loss = 0.27173416\n",
      "Iteration 1606, loss = 0.27177030\n",
      "Iteration 1607, loss = 0.27141724\n",
      "Iteration 1608, loss = 0.27106458\n",
      "Iteration 1609, loss = 0.27108527\n",
      "Iteration 1610, loss = 0.27099800\n",
      "Iteration 1611, loss = 0.27130611\n",
      "Iteration 1612, loss = 0.27111416\n",
      "Iteration 1613, loss = 0.27069972\n",
      "Iteration 1614, loss = 0.27062069\n",
      "Iteration 1615, loss = 0.27039680\n",
      "Iteration 1616, loss = 0.26997572\n",
      "Iteration 1617, loss = 0.27045346\n",
      "Iteration 1618, loss = 0.27039170\n",
      "Iteration 1619, loss = 0.27049644\n",
      "Iteration 1620, loss = 0.27022831\n",
      "Iteration 1621, loss = 0.26983676\n",
      "Iteration 1622, loss = 0.26936175\n",
      "Iteration 1623, loss = 0.26974507\n",
      "Iteration 1624, loss = 0.26940598\n",
      "Iteration 1625, loss = 0.27038958\n",
      "Iteration 1626, loss = 0.26940284\n",
      "Iteration 1627, loss = 0.26880915\n",
      "Iteration 1628, loss = 0.26922827\n",
      "Iteration 1629, loss = 0.26953139\n",
      "Iteration 1630, loss = 0.26951757\n",
      "Iteration 1631, loss = 0.26865392\n",
      "Iteration 1632, loss = 0.26869673\n",
      "Iteration 1633, loss = 0.26887935\n",
      "Iteration 1634, loss = 0.26873913\n",
      "Iteration 1635, loss = 0.26880478\n",
      "Iteration 1636, loss = 0.26819036\n",
      "Iteration 1637, loss = 0.26794988\n",
      "Iteration 1638, loss = 0.26827622\n",
      "Iteration 1639, loss = 0.26819502\n",
      "Iteration 1640, loss = 0.26810192\n",
      "Iteration 1641, loss = 0.26806918\n",
      "Iteration 1642, loss = 0.26740589\n",
      "Iteration 1643, loss = 0.26713155\n",
      "Iteration 1644, loss = 0.26862643\n",
      "Iteration 1645, loss = 0.26763104\n",
      "Iteration 1646, loss = 0.26705696\n",
      "Iteration 1647, loss = 0.26678108\n",
      "Iteration 1648, loss = 0.26634874\n",
      "Iteration 1649, loss = 0.26713178\n",
      "Iteration 1650, loss = 0.26677577\n",
      "Iteration 1651, loss = 0.26661026\n",
      "Iteration 1652, loss = 0.26713285\n",
      "Iteration 1653, loss = 0.26605277\n",
      "Iteration 1654, loss = 0.26617608\n",
      "Iteration 1655, loss = 0.26585039\n",
      "Iteration 1656, loss = 0.26614250\n",
      "Iteration 1657, loss = 0.26634731\n",
      "Iteration 1658, loss = 0.26577139\n",
      "Iteration 1659, loss = 0.26604683\n",
      "Iteration 1660, loss = 0.26583063\n",
      "Iteration 1661, loss = 0.26548387\n",
      "Iteration 1662, loss = 0.26543068\n",
      "Iteration 1663, loss = 0.26569250\n",
      "Iteration 1664, loss = 0.26478601\n",
      "Iteration 1665, loss = 0.26507908\n",
      "Iteration 1666, loss = 0.26525678\n",
      "Iteration 1667, loss = 0.26478360\n",
      "Iteration 1668, loss = 0.26497113\n",
      "Iteration 1669, loss = 0.26503348\n",
      "Iteration 1670, loss = 0.26497947\n",
      "Iteration 1671, loss = 0.26419528\n",
      "Iteration 1672, loss = 0.26487759\n",
      "Iteration 1673, loss = 0.26437781\n",
      "Iteration 1674, loss = 0.26420439\n",
      "Iteration 1675, loss = 0.26446566\n",
      "Iteration 1676, loss = 0.26460454\n",
      "Iteration 1677, loss = 0.26467005\n",
      "Iteration 1678, loss = 0.26430800\n",
      "Iteration 1679, loss = 0.26348136\n",
      "Iteration 1680, loss = 0.26370636\n",
      "Iteration 1681, loss = 0.26440745\n",
      "Iteration 1682, loss = 0.26373667\n",
      "Iteration 1683, loss = 0.26313921\n",
      "Iteration 1684, loss = 0.26282849\n",
      "Iteration 1685, loss = 0.26311503\n",
      "Iteration 1686, loss = 0.26292792\n",
      "Iteration 1687, loss = 0.26355371\n",
      "Iteration 1688, loss = 0.26285346\n",
      "Iteration 1689, loss = 0.26259910\n",
      "Iteration 1690, loss = 0.26283958\n",
      "Iteration 1691, loss = 0.26260919\n",
      "Iteration 1692, loss = 0.26239257\n",
      "Iteration 1693, loss = 0.26281570\n",
      "Iteration 1694, loss = 0.26241523\n",
      "Iteration 1695, loss = 0.26186321\n",
      "Iteration 1696, loss = 0.26201678\n",
      "Iteration 1697, loss = 0.26225912\n",
      "Iteration 1698, loss = 0.26161952\n",
      "Iteration 1699, loss = 0.26205539\n",
      "Iteration 1700, loss = 0.26150857\n",
      "Iteration 1701, loss = 0.26175633\n",
      "Iteration 1702, loss = 0.26110386\n",
      "Iteration 1703, loss = 0.26131881\n",
      "Iteration 1704, loss = 0.26161806\n",
      "Iteration 1705, loss = 0.26148957\n",
      "Iteration 1706, loss = 0.26130030\n",
      "Iteration 1707, loss = 0.26121672\n",
      "Iteration 1708, loss = 0.26078119\n",
      "Iteration 1709, loss = 0.26145721\n",
      "Iteration 1710, loss = 0.26042717\n",
      "Iteration 1711, loss = 0.26085185\n",
      "Iteration 1712, loss = 0.26045640\n",
      "Iteration 1713, loss = 0.26018351\n",
      "Iteration 1714, loss = 0.26087835\n",
      "Iteration 1715, loss = 0.25973374\n",
      "Iteration 1716, loss = 0.26016219\n",
      "Iteration 1717, loss = 0.26023800\n",
      "Iteration 1718, loss = 0.26032493\n",
      "Iteration 1719, loss = 0.25979689\n",
      "Iteration 1720, loss = 0.25931570\n",
      "Iteration 1721, loss = 0.25940027\n",
      "Iteration 1722, loss = 0.26004429\n",
      "Iteration 1723, loss = 0.25917167\n",
      "Iteration 1724, loss = 0.25944508\n",
      "Iteration 1725, loss = 0.25934077\n",
      "Iteration 1726, loss = 0.25955211\n",
      "Iteration 1727, loss = 0.25891783\n",
      "Iteration 1728, loss = 0.25918804\n",
      "Iteration 1729, loss = 0.25862090\n",
      "Iteration 1730, loss = 0.25868129\n",
      "Iteration 1731, loss = 0.25847553\n",
      "Iteration 1732, loss = 0.25875593\n",
      "Iteration 1733, loss = 0.25911398\n",
      "Iteration 1734, loss = 0.25840363\n",
      "Iteration 1735, loss = 0.25789447\n",
      "Iteration 1736, loss = 0.25838679\n",
      "Iteration 1737, loss = 0.25777691\n",
      "Iteration 1738, loss = 0.25842789\n",
      "Iteration 1739, loss = 0.25800353\n",
      "Iteration 1740, loss = 0.25802971\n",
      "Iteration 1741, loss = 0.25784732\n",
      "Iteration 1742, loss = 0.25806985\n",
      "Iteration 1743, loss = 0.25810572\n",
      "Iteration 1744, loss = 0.25743150\n",
      "Iteration 1745, loss = 0.25746008\n",
      "Iteration 1746, loss = 0.25689771\n",
      "Iteration 1747, loss = 0.25699830\n",
      "Iteration 1748, loss = 0.25747775\n",
      "Iteration 1749, loss = 0.25696995\n",
      "Iteration 1750, loss = 0.25763571\n",
      "Iteration 1751, loss = 0.25649117\n",
      "Iteration 1752, loss = 0.25721560\n",
      "Iteration 1753, loss = 0.25688909\n",
      "Iteration 1754, loss = 0.25645036\n",
      "Iteration 1755, loss = 0.25672125\n",
      "Iteration 1756, loss = 0.25649695\n",
      "Iteration 1757, loss = 0.25690213\n",
      "Iteration 1758, loss = 0.25612972\n",
      "Iteration 1759, loss = 0.25613979\n",
      "Iteration 1760, loss = 0.25653050\n",
      "Iteration 1761, loss = 0.25608920\n",
      "Iteration 1762, loss = 0.25559665\n",
      "Iteration 1763, loss = 0.25631248\n",
      "Iteration 1764, loss = 0.25557033\n",
      "Iteration 1765, loss = 0.25563112\n",
      "Iteration 1766, loss = 0.25535386\n",
      "Iteration 1767, loss = 0.25566297\n",
      "Iteration 1768, loss = 0.25552304\n",
      "Iteration 1769, loss = 0.25529027\n",
      "Iteration 1770, loss = 0.25506065\n",
      "Iteration 1771, loss = 0.25555852\n",
      "Iteration 1772, loss = 0.25486548\n",
      "Iteration 1773, loss = 0.25503954\n",
      "Iteration 1774, loss = 0.25476758\n",
      "Iteration 1775, loss = 0.25452550\n",
      "Iteration 1776, loss = 0.25478619\n",
      "Iteration 1777, loss = 0.25460765\n",
      "Iteration 1778, loss = 0.25502018\n",
      "Iteration 1779, loss = 0.25458862\n",
      "Iteration 1780, loss = 0.25443945\n",
      "Iteration 1781, loss = 0.25448319\n",
      "Iteration 1782, loss = 0.25483225\n",
      "Iteration 1783, loss = 0.25495395\n",
      "Iteration 1784, loss = 0.25392860\n",
      "Iteration 1785, loss = 0.25378524\n",
      "Iteration 1786, loss = 0.25410421\n",
      "Iteration 1787, loss = 0.25376181\n",
      "Iteration 1788, loss = 0.25368396\n",
      "Iteration 1789, loss = 0.25403697\n",
      "Iteration 1790, loss = 0.25327249\n",
      "Iteration 1791, loss = 0.25381585\n",
      "Iteration 1792, loss = 0.25365405\n",
      "Iteration 1793, loss = 0.25317488\n",
      "Iteration 1794, loss = 0.25283918\n",
      "Iteration 1795, loss = 0.25312049\n",
      "Iteration 1796, loss = 0.25297161\n",
      "Iteration 1797, loss = 0.25356178\n",
      "Iteration 1798, loss = 0.25282696\n",
      "Iteration 1799, loss = 0.25312028\n",
      "Iteration 1800, loss = 0.25298973\n",
      "Iteration 1801, loss = 0.25281862\n",
      "Iteration 1802, loss = 0.25305623\n",
      "Iteration 1803, loss = 0.25353890\n",
      "Iteration 1804, loss = 0.25214993\n",
      "Iteration 1805, loss = 0.25242523\n",
      "Iteration 1806, loss = 0.25186160\n",
      "Iteration 1807, loss = 0.25193218\n",
      "Iteration 1808, loss = 0.25238243\n",
      "Iteration 1809, loss = 0.25197636\n",
      "Iteration 1810, loss = 0.25173300\n",
      "Iteration 1811, loss = 0.25200718\n",
      "Iteration 1812, loss = 0.25163238\n",
      "Iteration 1813, loss = 0.25200413\n",
      "Iteration 1814, loss = 0.25175372\n",
      "Iteration 1815, loss = 0.25133681\n",
      "Iteration 1816, loss = 0.25115638\n",
      "Iteration 1817, loss = 0.25158025\n",
      "Iteration 1818, loss = 0.25179611\n",
      "Iteration 1819, loss = 0.25145703\n",
      "Iteration 1820, loss = 0.25145816\n",
      "Iteration 1821, loss = 0.25061504\n",
      "Iteration 1822, loss = 0.25106595\n",
      "Iteration 1823, loss = 0.25080984\n",
      "Iteration 1824, loss = 0.25142589\n",
      "Iteration 1825, loss = 0.25062517\n",
      "Iteration 1826, loss = 0.25070257\n",
      "Iteration 1827, loss = 0.25009759\n",
      "Iteration 1828, loss = 0.25081449\n",
      "Iteration 1829, loss = 0.25054878\n",
      "Iteration 1830, loss = 0.25001710\n",
      "Iteration 1831, loss = 0.25061060\n",
      "Iteration 1832, loss = 0.25019909\n",
      "Iteration 1833, loss = 0.25058713\n",
      "Iteration 1834, loss = 0.24990169\n",
      "Iteration 1835, loss = 0.24994414\n",
      "Iteration 1836, loss = 0.24991593\n",
      "Iteration 1837, loss = 0.24964615\n",
      "Iteration 1838, loss = 0.24969873\n",
      "Iteration 1839, loss = 0.24959672\n",
      "Iteration 1840, loss = 0.24932195\n",
      "Iteration 1841, loss = 0.24932420\n",
      "Iteration 1842, loss = 0.24959250\n",
      "Iteration 1843, loss = 0.24880672\n",
      "Iteration 1844, loss = 0.24899781\n",
      "Iteration 1845, loss = 0.24948855\n",
      "Iteration 1846, loss = 0.24884337\n",
      "Iteration 1847, loss = 0.24950489\n",
      "Iteration 1848, loss = 0.24892605\n",
      "Iteration 1849, loss = 0.24886613\n",
      "Iteration 1850, loss = 0.24887951\n",
      "Iteration 1851, loss = 0.24833377\n",
      "Iteration 1852, loss = 0.24922542\n",
      "Iteration 1853, loss = 0.24918131\n",
      "Iteration 1854, loss = 0.24852682\n",
      "Iteration 1855, loss = 0.24833963\n",
      "Iteration 1856, loss = 0.24871721\n",
      "Iteration 1857, loss = 0.24771831\n",
      "Iteration 1858, loss = 0.24861819\n",
      "Iteration 1859, loss = 0.24881639\n",
      "Iteration 1860, loss = 0.24742342\n",
      "Iteration 1861, loss = 0.24746686\n",
      "Iteration 1862, loss = 0.24784261\n",
      "Iteration 1863, loss = 0.24835853\n",
      "Iteration 1864, loss = 0.24733876\n",
      "Iteration 1865, loss = 0.24720630\n",
      "Iteration 1866, loss = 0.24737669\n",
      "Iteration 1867, loss = 0.24775037\n",
      "Iteration 1868, loss = 0.24695287\n",
      "Iteration 1869, loss = 0.24706259\n",
      "Iteration 1870, loss = 0.24756314\n",
      "Iteration 1871, loss = 0.24667835\n",
      "Iteration 1872, loss = 0.24679410\n",
      "Iteration 1873, loss = 0.24726760\n",
      "Iteration 1874, loss = 0.24741496\n",
      "Iteration 1875, loss = 0.24719916\n",
      "Iteration 1876, loss = 0.24649178\n",
      "Iteration 1877, loss = 0.24636189\n",
      "Iteration 1878, loss = 0.24662439\n",
      "Iteration 1879, loss = 0.24701364\n",
      "Iteration 1880, loss = 0.24687785\n",
      "Iteration 1881, loss = 0.24673536\n",
      "Iteration 1882, loss = 0.24608698\n",
      "Iteration 1883, loss = 0.24616452\n",
      "Iteration 1884, loss = 0.24610411\n",
      "Iteration 1885, loss = 0.24624966\n",
      "Iteration 1886, loss = 0.24654878\n",
      "Iteration 1887, loss = 0.24583247\n",
      "Iteration 1888, loss = 0.24561823\n",
      "Iteration 1889, loss = 0.24586325\n",
      "Iteration 1890, loss = 0.24528552\n",
      "Iteration 1891, loss = 0.24634629\n",
      "Iteration 1892, loss = 0.24509133\n",
      "Iteration 1893, loss = 0.24502277\n",
      "Iteration 1894, loss = 0.24541039\n",
      "Iteration 1895, loss = 0.24505777\n",
      "Iteration 1896, loss = 0.24556461\n",
      "Iteration 1897, loss = 0.24522722\n",
      "Iteration 1898, loss = 0.24521668\n",
      "Iteration 1899, loss = 0.24508076\n",
      "Iteration 1900, loss = 0.24510267\n",
      "Iteration 1901, loss = 0.24491126\n",
      "Iteration 1902, loss = 0.24501581\n",
      "Iteration 1903, loss = 0.24449095\n",
      "Iteration 1904, loss = 0.24471296\n",
      "Iteration 1905, loss = 0.24439060\n",
      "Iteration 1906, loss = 0.24431675\n",
      "Iteration 1907, loss = 0.24490571\n",
      "Iteration 1908, loss = 0.24458888\n",
      "Iteration 1909, loss = 0.24467127\n",
      "Iteration 1910, loss = 0.24488934\n",
      "Iteration 1911, loss = 0.24427725\n",
      "Iteration 1912, loss = 0.24449013\n",
      "Iteration 1913, loss = 0.24378461\n",
      "Iteration 1914, loss = 0.24374859\n",
      "Iteration 1915, loss = 0.24372205\n",
      "Iteration 1916, loss = 0.24387846\n",
      "Iteration 1917, loss = 0.24380011\n",
      "Iteration 1918, loss = 0.24349749\n",
      "Iteration 1919, loss = 0.24304970\n",
      "Iteration 1920, loss = 0.24313316\n",
      "Iteration 1921, loss = 0.24324643\n",
      "Iteration 1922, loss = 0.24414608\n",
      "Iteration 1923, loss = 0.24332430\n",
      "Iteration 1924, loss = 0.24325004\n",
      "Iteration 1925, loss = 0.24326952\n",
      "Iteration 1926, loss = 0.24339239\n",
      "Iteration 1927, loss = 0.24304722\n",
      "Iteration 1928, loss = 0.24337865\n",
      "Iteration 1929, loss = 0.24283705\n",
      "Iteration 1930, loss = 0.24236453\n",
      "Iteration 1931, loss = 0.24247078\n",
      "Iteration 1932, loss = 0.24264570\n",
      "Iteration 1933, loss = 0.24312467\n",
      "Iteration 1934, loss = 0.24204837\n",
      "Iteration 1935, loss = 0.24280416\n",
      "Iteration 1936, loss = 0.24259700\n",
      "Iteration 1937, loss = 0.24223291\n",
      "Iteration 1938, loss = 0.24244704\n",
      "Iteration 1939, loss = 0.24178417\n",
      "Iteration 1940, loss = 0.24202614\n",
      "Iteration 1941, loss = 0.24223576\n",
      "Iteration 1942, loss = 0.24212551\n",
      "Iteration 1943, loss = 0.24205042\n",
      "Iteration 1944, loss = 0.24236786\n",
      "Iteration 1945, loss = 0.24189876\n",
      "Iteration 1946, loss = 0.24156725\n",
      "Iteration 1947, loss = 0.24141562\n",
      "Iteration 1948, loss = 0.24148720\n",
      "Iteration 1949, loss = 0.24182541\n",
      "Iteration 1950, loss = 0.24170382\n",
      "Iteration 1951, loss = 0.24106590\n",
      "Iteration 1952, loss = 0.24179687\n",
      "Iteration 1953, loss = 0.24140775\n",
      "Iteration 1954, loss = 0.24081591\n",
      "Iteration 1955, loss = 0.24080349\n",
      "Iteration 1956, loss = 0.24111261\n",
      "Iteration 1957, loss = 0.24109033\n",
      "Iteration 1958, loss = 0.24118303\n",
      "Iteration 1959, loss = 0.24080885\n",
      "Iteration 1960, loss = 0.24111105\n",
      "Iteration 1961, loss = 0.24084903\n",
      "Iteration 1962, loss = 0.24054496\n",
      "Iteration 1963, loss = 0.24093053\n",
      "Iteration 1964, loss = 0.24016050\n",
      "Iteration 1965, loss = 0.24025430\n",
      "Iteration 1966, loss = 0.23999179\n",
      "Iteration 1967, loss = 0.24005866\n",
      "Iteration 1968, loss = 0.24023528\n",
      "Iteration 1969, loss = 0.24055742\n",
      "Iteration 1970, loss = 0.24060949\n",
      "Iteration 1971, loss = 0.23987909\n",
      "Iteration 1972, loss = 0.24046722\n",
      "Iteration 1973, loss = 0.23954055\n",
      "Iteration 1974, loss = 0.24001980\n",
      "Iteration 1975, loss = 0.24023244\n",
      "Iteration 1976, loss = 0.23961945\n",
      "Iteration 1977, loss = 0.24030779\n",
      "Iteration 1978, loss = 0.23944609\n",
      "Iteration 1979, loss = 0.23990627\n",
      "Iteration 1980, loss = 0.23951854\n",
      "Iteration 1981, loss = 0.23923055\n",
      "Iteration 1982, loss = 0.23908705\n",
      "Iteration 1983, loss = 0.23946345\n",
      "Iteration 1984, loss = 0.23927252\n",
      "Iteration 1985, loss = 0.23894607\n",
      "Iteration 1986, loss = 0.23945065\n",
      "Iteration 1987, loss = 0.23861628\n",
      "Iteration 1988, loss = 0.23868238\n",
      "Iteration 1989, loss = 0.23885453\n",
      "Iteration 1990, loss = 0.23925978\n",
      "Iteration 1991, loss = 0.23859912\n",
      "Iteration 1992, loss = 0.23836903\n",
      "Iteration 1993, loss = 0.23855861\n",
      "Iteration 1994, loss = 0.23853473\n",
      "Iteration 1995, loss = 0.23842049\n",
      "Iteration 1996, loss = 0.23826765\n",
      "Iteration 1997, loss = 0.23814546\n",
      "Iteration 1998, loss = 0.23791639\n",
      "Iteration 1999, loss = 0.23845203\n",
      "Iteration 2000, loss = 0.23796937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66285843\n",
      "Iteration 2, loss = 0.65615823\n",
      "Iteration 3, loss = 0.65390034\n",
      "Iteration 4, loss = 0.65175088\n",
      "Iteration 5, loss = 0.65031050\n",
      "Iteration 6, loss = 0.64879121\n",
      "Iteration 7, loss = 0.64685075\n",
      "Iteration 8, loss = 0.64494455\n",
      "Iteration 9, loss = 0.64261051\n",
      "Iteration 10, loss = 0.64085118\n",
      "Iteration 11, loss = 0.63823820\n",
      "Iteration 12, loss = 0.63602363\n",
      "Iteration 13, loss = 0.63366986\n",
      "Iteration 14, loss = 0.63185062\n",
      "Iteration 15, loss = 0.62933951\n",
      "Iteration 16, loss = 0.62640821\n",
      "Iteration 17, loss = 0.62398209\n",
      "Iteration 18, loss = 0.62097473\n",
      "Iteration 19, loss = 0.61764012\n",
      "Iteration 20, loss = 0.61414679\n",
      "Iteration 21, loss = 0.61125227\n",
      "Iteration 22, loss = 0.60731496\n",
      "Iteration 23, loss = 0.60388211\n",
      "Iteration 24, loss = 0.60053707\n",
      "Iteration 25, loss = 0.59654378\n",
      "Iteration 26, loss = 0.59280581\n",
      "Iteration 27, loss = 0.58915254\n",
      "Iteration 28, loss = 0.58524247\n",
      "Iteration 29, loss = 0.58188998\n",
      "Iteration 30, loss = 0.57838711\n",
      "Iteration 31, loss = 0.57451524\n",
      "Iteration 32, loss = 0.57123806\n",
      "Iteration 33, loss = 0.56707982\n",
      "Iteration 34, loss = 0.56314205\n",
      "Iteration 35, loss = 0.56006846\n",
      "Iteration 36, loss = 0.55556031\n",
      "Iteration 37, loss = 0.55189642\n",
      "Iteration 38, loss = 0.54855019\n",
      "Iteration 39, loss = 0.54459870\n",
      "Iteration 40, loss = 0.54190148\n",
      "Iteration 41, loss = 0.53778544\n",
      "Iteration 42, loss = 0.53427670\n",
      "Iteration 43, loss = 0.53038489\n",
      "Iteration 44, loss = 0.52755110\n",
      "Iteration 45, loss = 0.52351192\n",
      "Iteration 46, loss = 0.52033630\n",
      "Iteration 47, loss = 0.51744895\n",
      "Iteration 48, loss = 0.51370374\n",
      "Iteration 49, loss = 0.51065902\n",
      "Iteration 50, loss = 0.50742028\n",
      "Iteration 51, loss = 0.50440600\n",
      "Iteration 52, loss = 0.50088703\n",
      "Iteration 53, loss = 0.49833338\n",
      "Iteration 54, loss = 0.49496522\n",
      "Iteration 55, loss = 0.49231525\n",
      "Iteration 56, loss = 0.48935922\n",
      "Iteration 57, loss = 0.48688697\n",
      "Iteration 58, loss = 0.48353633\n",
      "Iteration 59, loss = 0.48090758\n",
      "Iteration 60, loss = 0.47791938\n",
      "Iteration 61, loss = 0.47470193\n",
      "Iteration 62, loss = 0.47208099\n",
      "Iteration 63, loss = 0.46969732\n",
      "Iteration 64, loss = 0.46643672\n",
      "Iteration 65, loss = 0.46405104\n",
      "Iteration 66, loss = 0.46106596\n",
      "Iteration 67, loss = 0.45855625\n",
      "Iteration 68, loss = 0.45658742\n",
      "Iteration 69, loss = 0.45331532\n",
      "Iteration 70, loss = 0.45115728\n",
      "Iteration 71, loss = 0.44834266\n",
      "Iteration 72, loss = 0.44623045\n",
      "Iteration 73, loss = 0.44347398\n",
      "Iteration 74, loss = 0.44155408\n",
      "Iteration 75, loss = 0.43890945\n",
      "Iteration 76, loss = 0.43609468\n",
      "Iteration 77, loss = 0.43367214\n",
      "Iteration 78, loss = 0.43190823\n",
      "Iteration 79, loss = 0.42913632\n",
      "Iteration 80, loss = 0.42733886\n",
      "Iteration 81, loss = 0.42464930\n",
      "Iteration 82, loss = 0.42279888\n",
      "Iteration 83, loss = 0.42072190\n",
      "Iteration 84, loss = 0.41808386\n",
      "Iteration 85, loss = 0.41641430\n",
      "Iteration 86, loss = 0.41443833\n",
      "Iteration 87, loss = 0.41129556\n",
      "Iteration 88, loss = 0.40966246\n",
      "Iteration 89, loss = 0.40723698\n",
      "Iteration 90, loss = 0.40600347\n",
      "Iteration 91, loss = 0.40439411\n",
      "Iteration 92, loss = 0.40172696\n",
      "Iteration 93, loss = 0.40014820\n",
      "Iteration 94, loss = 0.39819831\n",
      "Iteration 95, loss = 0.39644875\n",
      "Iteration 96, loss = 0.39364543\n",
      "Iteration 97, loss = 0.39250077\n",
      "Iteration 98, loss = 0.39082406\n",
      "Iteration 99, loss = 0.38877184\n",
      "Iteration 100, loss = 0.38653460\n",
      "Iteration 101, loss = 0.38547118\n",
      "Iteration 102, loss = 0.38401228\n",
      "Iteration 103, loss = 0.38190178\n",
      "Iteration 104, loss = 0.38010282\n",
      "Iteration 105, loss = 0.37863067\n",
      "Iteration 106, loss = 0.37729760\n",
      "Iteration 107, loss = 0.37490393\n",
      "Iteration 108, loss = 0.37273897\n",
      "Iteration 109, loss = 0.37153848\n",
      "Iteration 110, loss = 0.36935592\n",
      "Iteration 111, loss = 0.36820461\n",
      "Iteration 112, loss = 0.36587870\n",
      "Iteration 113, loss = 0.36470984\n",
      "Iteration 114, loss = 0.36320581\n",
      "Iteration 115, loss = 0.36198444\n",
      "Iteration 116, loss = 0.36003522\n",
      "Iteration 117, loss = 0.35853351\n",
      "Iteration 118, loss = 0.35658800\n",
      "Iteration 119, loss = 0.35577680\n",
      "Iteration 120, loss = 0.35327641\n",
      "Iteration 121, loss = 0.35278526\n",
      "Iteration 122, loss = 0.35060688\n",
      "Iteration 123, loss = 0.34887731\n",
      "Iteration 124, loss = 0.34724679\n",
      "Iteration 125, loss = 0.34664511\n",
      "Iteration 126, loss = 0.34491985\n",
      "Iteration 127, loss = 0.34306900\n",
      "Iteration 128, loss = 0.34161841\n",
      "Iteration 129, loss = 0.34024888\n",
      "Iteration 130, loss = 0.33911683\n",
      "Iteration 131, loss = 0.33768334\n",
      "Iteration 132, loss = 0.33562229\n",
      "Iteration 133, loss = 0.33513746\n",
      "Iteration 134, loss = 0.33358734\n",
      "Iteration 135, loss = 0.33214180\n",
      "Iteration 136, loss = 0.33061659\n",
      "Iteration 137, loss = 0.32937360\n",
      "Iteration 138, loss = 0.32863951\n",
      "Iteration 139, loss = 0.32686339\n",
      "Iteration 140, loss = 0.32593647\n",
      "Iteration 141, loss = 0.32449719\n",
      "Iteration 142, loss = 0.32315894\n",
      "Iteration 143, loss = 0.32196695\n",
      "Iteration 144, loss = 0.32040188\n",
      "Iteration 145, loss = 0.31986346\n",
      "Iteration 146, loss = 0.31845536\n",
      "Iteration 147, loss = 0.31724456\n",
      "Iteration 148, loss = 0.31628452\n",
      "Iteration 149, loss = 0.31486309\n",
      "Iteration 150, loss = 0.31366897\n",
      "Iteration 151, loss = 0.31269865\n",
      "Iteration 152, loss = 0.31139498\n",
      "Iteration 153, loss = 0.31017942\n",
      "Iteration 154, loss = 0.30986420\n",
      "Iteration 155, loss = 0.30794068\n",
      "Iteration 156, loss = 0.30738373\n",
      "Iteration 157, loss = 0.30521763\n",
      "Iteration 158, loss = 0.30435427\n",
      "Iteration 159, loss = 0.30331210\n",
      "Iteration 160, loss = 0.30226772\n",
      "Iteration 161, loss = 0.30196272\n",
      "Iteration 162, loss = 0.29988334\n",
      "Iteration 163, loss = 0.29933050\n",
      "Iteration 164, loss = 0.29792518\n",
      "Iteration 165, loss = 0.29682308\n",
      "Iteration 166, loss = 0.29641817\n",
      "Iteration 167, loss = 0.29480059\n",
      "Iteration 168, loss = 0.29380733\n",
      "Iteration 169, loss = 0.29331309\n",
      "Iteration 170, loss = 0.29204919\n",
      "Iteration 171, loss = 0.29083716\n",
      "Iteration 172, loss = 0.29030908\n",
      "Iteration 173, loss = 0.28896905\n",
      "Iteration 174, loss = 0.28822887\n",
      "Iteration 175, loss = 0.28723235\n",
      "Iteration 176, loss = 0.28638975\n",
      "Iteration 177, loss = 0.28447440\n",
      "Iteration 178, loss = 0.28440824\n",
      "Iteration 179, loss = 0.28340467\n",
      "Iteration 180, loss = 0.28240543\n",
      "Iteration 181, loss = 0.28213034\n",
      "Iteration 182, loss = 0.28065613\n",
      "Iteration 183, loss = 0.27990789\n",
      "Iteration 184, loss = 0.27829843\n",
      "Iteration 185, loss = 0.27725322\n",
      "Iteration 186, loss = 0.27742674\n",
      "Iteration 187, loss = 0.27606318\n",
      "Iteration 188, loss = 0.27522634\n",
      "Iteration 189, loss = 0.27391334\n",
      "Iteration 190, loss = 0.27304895\n",
      "Iteration 191, loss = 0.27303299\n",
      "Iteration 192, loss = 0.27167519\n",
      "Iteration 193, loss = 0.27099367\n",
      "Iteration 194, loss = 0.27015043\n",
      "Iteration 195, loss = 0.26910484\n",
      "Iteration 196, loss = 0.26801986\n",
      "Iteration 197, loss = 0.26760785\n",
      "Iteration 198, loss = 0.26614418\n",
      "Iteration 199, loss = 0.26577922\n",
      "Iteration 200, loss = 0.26543749\n",
      "Iteration 201, loss = 0.26461142\n",
      "Iteration 202, loss = 0.26339201\n",
      "Iteration 203, loss = 0.26352095\n",
      "Iteration 204, loss = 0.26156147\n",
      "Iteration 205, loss = 0.26126717\n",
      "Iteration 206, loss = 0.26009706\n",
      "Iteration 207, loss = 0.25960103\n",
      "Iteration 208, loss = 0.25936282\n",
      "Iteration 209, loss = 0.25864705\n",
      "Iteration 210, loss = 0.25776956\n",
      "Iteration 211, loss = 0.25674679\n",
      "Iteration 212, loss = 0.25689562\n",
      "Iteration 213, loss = 0.25474062\n",
      "Iteration 214, loss = 0.25436111\n",
      "Iteration 215, loss = 0.25405005\n",
      "Iteration 216, loss = 0.25280025\n",
      "Iteration 217, loss = 0.25278510\n",
      "Iteration 218, loss = 0.25158507\n",
      "Iteration 219, loss = 0.25125173\n",
      "Iteration 220, loss = 0.25039168\n",
      "Iteration 221, loss = 0.24895965\n",
      "Iteration 222, loss = 0.24916552\n",
      "Iteration 223, loss = 0.24801274\n",
      "Iteration 224, loss = 0.24694787\n",
      "Iteration 225, loss = 0.24660881\n",
      "Iteration 226, loss = 0.24571417\n",
      "Iteration 227, loss = 0.24457301\n",
      "Iteration 228, loss = 0.24485448\n",
      "Iteration 229, loss = 0.24375939\n",
      "Iteration 230, loss = 0.24338229\n",
      "Iteration 231, loss = 0.24255123\n",
      "Iteration 232, loss = 0.24206376\n",
      "Iteration 233, loss = 0.24150953\n",
      "Iteration 234, loss = 0.24031925\n",
      "Iteration 235, loss = 0.24032374\n",
      "Iteration 236, loss = 0.23894264\n",
      "Iteration 237, loss = 0.23816158\n",
      "Iteration 238, loss = 0.23829351\n",
      "Iteration 239, loss = 0.23697673\n",
      "Iteration 240, loss = 0.23663756\n",
      "Iteration 241, loss = 0.23547986\n",
      "Iteration 242, loss = 0.23496185\n",
      "Iteration 243, loss = 0.23490583\n",
      "Iteration 244, loss = 0.23435283\n",
      "Iteration 245, loss = 0.23366156\n",
      "Iteration 246, loss = 0.23328266\n",
      "Iteration 247, loss = 0.23213165\n",
      "Iteration 248, loss = 0.23149499\n",
      "Iteration 249, loss = 0.23117964\n",
      "Iteration 250, loss = 0.23068196\n",
      "Iteration 251, loss = 0.22948274\n",
      "Iteration 252, loss = 0.22895411\n",
      "Iteration 253, loss = 0.22865766\n",
      "Iteration 254, loss = 0.22774715\n",
      "Iteration 255, loss = 0.22726620\n",
      "Iteration 256, loss = 0.22697760\n",
      "Iteration 257, loss = 0.22555623\n",
      "Iteration 258, loss = 0.22595225\n",
      "Iteration 259, loss = 0.22493550\n",
      "Iteration 260, loss = 0.22359436\n",
      "Iteration 261, loss = 0.22380555\n",
      "Iteration 262, loss = 0.22311953\n",
      "Iteration 263, loss = 0.22188571\n",
      "Iteration 264, loss = 0.22231209\n",
      "Iteration 265, loss = 0.22131446\n",
      "Iteration 266, loss = 0.22063594\n",
      "Iteration 267, loss = 0.21993135\n",
      "Iteration 268, loss = 0.21966719\n",
      "Iteration 269, loss = 0.21870415\n",
      "Iteration 270, loss = 0.21907502\n",
      "Iteration 271, loss = 0.21754327\n",
      "Iteration 272, loss = 0.21758511\n",
      "Iteration 273, loss = 0.21737979\n",
      "Iteration 274, loss = 0.21590647\n",
      "Iteration 275, loss = 0.21560949\n",
      "Iteration 276, loss = 0.21531937\n",
      "Iteration 277, loss = 0.21448586\n",
      "Iteration 278, loss = 0.21418404\n",
      "Iteration 279, loss = 0.21302095\n",
      "Iteration 280, loss = 0.21260932\n",
      "Iteration 281, loss = 0.21219405\n",
      "Iteration 282, loss = 0.21133982\n",
      "Iteration 283, loss = 0.21114575\n",
      "Iteration 284, loss = 0.21062513\n",
      "Iteration 285, loss = 0.21002317\n",
      "Iteration 286, loss = 0.20966464\n",
      "Iteration 287, loss = 0.20912248\n",
      "Iteration 288, loss = 0.20899517\n",
      "Iteration 289, loss = 0.20800352\n",
      "Iteration 290, loss = 0.20743931\n",
      "Iteration 291, loss = 0.20683877\n",
      "Iteration 292, loss = 0.20631235\n",
      "Iteration 293, loss = 0.20574179\n",
      "Iteration 294, loss = 0.20504536\n",
      "Iteration 295, loss = 0.20536083\n",
      "Iteration 296, loss = 0.20447706\n",
      "Iteration 297, loss = 0.20432693\n",
      "Iteration 298, loss = 0.20412427\n",
      "Iteration 299, loss = 0.20198157\n",
      "Iteration 300, loss = 0.20226095\n",
      "Iteration 301, loss = 0.20229106\n",
      "Iteration 302, loss = 0.20135708\n",
      "Iteration 303, loss = 0.20099519\n",
      "Iteration 304, loss = 0.20035888\n",
      "Iteration 305, loss = 0.19971376\n",
      "Iteration 306, loss = 0.19928385\n",
      "Iteration 307, loss = 0.19902338\n",
      "Iteration 308, loss = 0.19818308\n",
      "Iteration 309, loss = 0.19851057\n",
      "Iteration 310, loss = 0.19771702\n",
      "Iteration 311, loss = 0.19731867\n",
      "Iteration 312, loss = 0.19666348\n",
      "Iteration 313, loss = 0.19678163\n",
      "Iteration 314, loss = 0.19588288\n",
      "Iteration 315, loss = 0.19584953\n",
      "Iteration 316, loss = 0.19416501\n",
      "Iteration 317, loss = 0.19471323\n",
      "Iteration 318, loss = 0.19433589\n",
      "Iteration 319, loss = 0.19363659\n",
      "Iteration 320, loss = 0.19322995\n",
      "Iteration 321, loss = 0.19358293\n",
      "Iteration 322, loss = 0.19281661\n",
      "Iteration 323, loss = 0.19233280\n",
      "Iteration 324, loss = 0.19180481\n",
      "Iteration 325, loss = 0.19161055\n",
      "Iteration 326, loss = 0.19037983\n",
      "Iteration 327, loss = 0.19046827\n",
      "Iteration 328, loss = 0.18928557\n",
      "Iteration 329, loss = 0.18996174\n",
      "Iteration 330, loss = 0.18998648\n",
      "Iteration 331, loss = 0.18856457\n",
      "Iteration 332, loss = 0.18781908\n",
      "Iteration 333, loss = 0.18873780\n",
      "Iteration 334, loss = 0.18729151\n",
      "Iteration 335, loss = 0.18697689\n",
      "Iteration 336, loss = 0.18682568\n",
      "Iteration 337, loss = 0.18651197\n",
      "Iteration 338, loss = 0.18606774\n",
      "Iteration 339, loss = 0.18578411\n",
      "Iteration 340, loss = 0.18496042\n",
      "Iteration 341, loss = 0.18490322\n",
      "Iteration 342, loss = 0.18526251\n",
      "Iteration 343, loss = 0.18365186\n",
      "Iteration 344, loss = 0.18357269\n",
      "Iteration 345, loss = 0.18358553\n",
      "Iteration 346, loss = 0.18282036\n",
      "Iteration 347, loss = 0.18241142\n",
      "Iteration 348, loss = 0.18231085\n",
      "Iteration 349, loss = 0.18247726\n",
      "Iteration 350, loss = 0.18114151\n",
      "Iteration 351, loss = 0.18106042\n",
      "Iteration 352, loss = 0.18075028\n",
      "Iteration 353, loss = 0.18043470\n",
      "Iteration 354, loss = 0.18009729\n",
      "Iteration 355, loss = 0.17967396\n",
      "Iteration 356, loss = 0.17886216\n",
      "Iteration 357, loss = 0.17893614\n",
      "Iteration 358, loss = 0.17882582\n",
      "Iteration 359, loss = 0.17814823\n",
      "Iteration 360, loss = 0.17810936\n",
      "Iteration 361, loss = 0.17776798\n",
      "Iteration 362, loss = 0.17720152\n",
      "Iteration 363, loss = 0.17697539\n",
      "Iteration 364, loss = 0.17652056\n",
      "Iteration 365, loss = 0.17608796\n",
      "Iteration 366, loss = 0.17561442\n",
      "Iteration 367, loss = 0.17531383\n",
      "Iteration 368, loss = 0.17506582\n",
      "Iteration 369, loss = 0.17492549\n",
      "Iteration 370, loss = 0.17455900\n",
      "Iteration 371, loss = 0.17397165\n",
      "Iteration 372, loss = 0.17382506\n",
      "Iteration 373, loss = 0.17340693\n",
      "Iteration 374, loss = 0.17294458\n",
      "Iteration 375, loss = 0.17281749\n",
      "Iteration 376, loss = 0.17229054\n",
      "Iteration 377, loss = 0.17251912\n",
      "Iteration 378, loss = 0.17142878\n",
      "Iteration 379, loss = 0.17168714\n",
      "Iteration 380, loss = 0.17108160\n",
      "Iteration 381, loss = 0.17070725\n",
      "Iteration 382, loss = 0.17028904\n",
      "Iteration 383, loss = 0.17047519\n",
      "Iteration 384, loss = 0.16985597\n",
      "Iteration 385, loss = 0.17018515\n",
      "Iteration 386, loss = 0.16915020\n",
      "Iteration 387, loss = 0.16872027\n",
      "Iteration 388, loss = 0.16872629\n",
      "Iteration 389, loss = 0.16914163\n",
      "Iteration 390, loss = 0.16763652\n",
      "Iteration 391, loss = 0.16753872\n",
      "Iteration 392, loss = 0.16790743\n",
      "Iteration 393, loss = 0.16669665\n",
      "Iteration 394, loss = 0.16664285\n",
      "Iteration 395, loss = 0.16617263\n",
      "Iteration 396, loss = 0.16595693\n",
      "Iteration 397, loss = 0.16591123\n",
      "Iteration 398, loss = 0.16560772\n",
      "Iteration 399, loss = 0.16547831\n",
      "Iteration 400, loss = 0.16510511\n",
      "Iteration 401, loss = 0.16459410\n",
      "Iteration 402, loss = 0.16432744\n",
      "Iteration 403, loss = 0.16423065\n",
      "Iteration 404, loss = 0.16394715\n",
      "Iteration 405, loss = 0.16369664\n",
      "Iteration 406, loss = 0.16376832\n",
      "Iteration 407, loss = 0.16339203\n",
      "Iteration 408, loss = 0.16256809\n",
      "Iteration 409, loss = 0.16192667\n",
      "Iteration 410, loss = 0.16183823\n",
      "Iteration 411, loss = 0.16203800\n",
      "Iteration 412, loss = 0.16103256\n",
      "Iteration 413, loss = 0.16089954\n",
      "Iteration 414, loss = 0.16079495\n",
      "Iteration 415, loss = 0.16087804\n",
      "Iteration 416, loss = 0.16022031\n",
      "Iteration 417, loss = 0.16026628\n",
      "Iteration 418, loss = 0.15958515\n",
      "Iteration 419, loss = 0.15868919\n",
      "Iteration 420, loss = 0.15999358\n",
      "Iteration 421, loss = 0.15868264\n",
      "Iteration 422, loss = 0.15899618\n",
      "Iteration 423, loss = 0.15808960\n",
      "Iteration 424, loss = 0.15776840\n",
      "Iteration 425, loss = 0.15810269\n",
      "Iteration 426, loss = 0.15695700\n",
      "Iteration 427, loss = 0.15711668\n",
      "Iteration 428, loss = 0.15709235\n",
      "Iteration 429, loss = 0.15765145\n",
      "Iteration 430, loss = 0.15621242\n",
      "Iteration 431, loss = 0.15599809\n",
      "Iteration 432, loss = 0.15536482\n",
      "Iteration 433, loss = 0.15560285\n",
      "Iteration 434, loss = 0.15525074\n",
      "Iteration 435, loss = 0.15513892\n",
      "Iteration 436, loss = 0.15414993\n",
      "Iteration 437, loss = 0.15453764\n",
      "Iteration 438, loss = 0.15349255\n",
      "Iteration 439, loss = 0.15358313\n",
      "Iteration 440, loss = 0.15396303\n",
      "Iteration 441, loss = 0.15297668\n",
      "Iteration 442, loss = 0.15326794\n",
      "Iteration 443, loss = 0.15289417\n",
      "Iteration 444, loss = 0.15209843\n",
      "Iteration 445, loss = 0.15305623\n",
      "Iteration 446, loss = 0.15191430\n",
      "Iteration 447, loss = 0.15116972\n",
      "Iteration 448, loss = 0.15157885\n",
      "Iteration 449, loss = 0.15078634\n",
      "Iteration 450, loss = 0.15090128\n",
      "Iteration 451, loss = 0.15067431\n",
      "Iteration 452, loss = 0.14983962\n",
      "Iteration 453, loss = 0.15003328\n",
      "Iteration 454, loss = 0.15009455\n",
      "Iteration 455, loss = 0.14998860\n",
      "Iteration 456, loss = 0.14932842\n",
      "Iteration 457, loss = 0.14928888\n",
      "Iteration 458, loss = 0.14875555\n",
      "Iteration 459, loss = 0.14882075\n",
      "Iteration 460, loss = 0.14820591\n",
      "Iteration 461, loss = 0.14833765\n",
      "Iteration 462, loss = 0.14776920\n",
      "Iteration 463, loss = 0.14727242\n",
      "Iteration 464, loss = 0.14740656\n",
      "Iteration 465, loss = 0.14728659\n",
      "Iteration 466, loss = 0.14682281\n",
      "Iteration 467, loss = 0.14654161\n",
      "Iteration 468, loss = 0.14640281\n",
      "Iteration 469, loss = 0.14640238\n",
      "Iteration 470, loss = 0.14606329\n",
      "Iteration 471, loss = 0.14566958\n",
      "Iteration 472, loss = 0.14576959\n",
      "Iteration 473, loss = 0.14566012\n",
      "Iteration 474, loss = 0.14516990\n",
      "Iteration 475, loss = 0.14456094\n",
      "Iteration 476, loss = 0.14460938\n",
      "Iteration 477, loss = 0.14440870\n",
      "Iteration 478, loss = 0.14385943\n",
      "Iteration 479, loss = 0.14408128\n",
      "Iteration 480, loss = 0.14367647\n",
      "Iteration 481, loss = 0.14396274\n",
      "Iteration 482, loss = 0.14386142\n",
      "Iteration 483, loss = 0.14314934\n",
      "Iteration 484, loss = 0.14307051\n",
      "Iteration 485, loss = 0.14214853\n",
      "Iteration 486, loss = 0.14271263\n",
      "Iteration 487, loss = 0.14271146\n",
      "Iteration 488, loss = 0.14210182\n",
      "Iteration 489, loss = 0.14162110\n",
      "Iteration 490, loss = 0.14130929\n",
      "Iteration 491, loss = 0.14091386\n",
      "Iteration 492, loss = 0.14087733\n",
      "Iteration 493, loss = 0.14075071\n",
      "Iteration 494, loss = 0.14085841\n",
      "Iteration 495, loss = 0.14043644\n",
      "Iteration 496, loss = 0.13983621\n",
      "Iteration 497, loss = 0.14035193\n",
      "Iteration 498, loss = 0.13930344\n",
      "Iteration 499, loss = 0.13928322\n",
      "Iteration 500, loss = 0.13975626\n",
      "Iteration 501, loss = 0.13927155\n",
      "Iteration 502, loss = 0.13840954\n",
      "Iteration 503, loss = 0.13823771\n",
      "Iteration 504, loss = 0.13922642\n",
      "Iteration 505, loss = 0.13869922\n",
      "Iteration 506, loss = 0.13802034\n",
      "Iteration 507, loss = 0.13841732\n",
      "Iteration 508, loss = 0.13737580\n",
      "Iteration 509, loss = 0.13730105\n",
      "Iteration 510, loss = 0.13726120\n",
      "Iteration 511, loss = 0.13758607\n",
      "Iteration 512, loss = 0.13689078\n",
      "Iteration 513, loss = 0.13703651\n",
      "Iteration 514, loss = 0.13594348\n",
      "Iteration 515, loss = 0.13615224\n",
      "Iteration 516, loss = 0.13550419\n",
      "Iteration 517, loss = 0.13583241\n",
      "Iteration 518, loss = 0.13600514\n",
      "Iteration 519, loss = 0.13568788\n",
      "Iteration 520, loss = 0.13560960\n",
      "Iteration 521, loss = 0.13502228\n",
      "Iteration 522, loss = 0.13491293\n",
      "Iteration 523, loss = 0.13490092\n",
      "Iteration 524, loss = 0.13397160\n",
      "Iteration 525, loss = 0.13454470\n",
      "Iteration 526, loss = 0.13456894\n",
      "Iteration 527, loss = 0.13423451\n",
      "Iteration 528, loss = 0.13360399\n",
      "Iteration 529, loss = 0.13384600\n",
      "Iteration 530, loss = 0.13320959\n",
      "Iteration 531, loss = 0.13311845\n",
      "Iteration 532, loss = 0.13317087\n",
      "Iteration 533, loss = 0.13293465\n",
      "Iteration 534, loss = 0.13255852\n",
      "Iteration 535, loss = 0.13233947\n",
      "Iteration 536, loss = 0.13229993\n",
      "Iteration 537, loss = 0.13176035\n",
      "Iteration 538, loss = 0.13214723\n",
      "Iteration 539, loss = 0.13169300\n",
      "Iteration 540, loss = 0.13192006\n",
      "Iteration 541, loss = 0.13078615\n",
      "Iteration 542, loss = 0.13075216\n",
      "Iteration 543, loss = 0.13083778\n",
      "Iteration 544, loss = 0.13182604\n",
      "Iteration 545, loss = 0.13103833\n",
      "Iteration 546, loss = 0.13077624\n",
      "Iteration 547, loss = 0.12979233\n",
      "Iteration 548, loss = 0.13018946\n",
      "Iteration 549, loss = 0.12953751\n",
      "Iteration 550, loss = 0.13039269\n",
      "Iteration 551, loss = 0.12921468\n",
      "Iteration 552, loss = 0.12949783\n",
      "Iteration 553, loss = 0.12889197\n",
      "Iteration 554, loss = 0.12911048\n",
      "Iteration 555, loss = 0.12893703\n",
      "Iteration 556, loss = 0.12869255\n",
      "Iteration 557, loss = 0.12887213\n",
      "Iteration 558, loss = 0.12830153\n",
      "Iteration 559, loss = 0.12795755\n",
      "Iteration 560, loss = 0.12885460\n",
      "Iteration 561, loss = 0.12779192\n",
      "Iteration 562, loss = 0.12744181\n",
      "Iteration 563, loss = 0.12731647\n",
      "Iteration 564, loss = 0.12701809\n",
      "Iteration 565, loss = 0.12662919\n",
      "Iteration 566, loss = 0.12733635\n",
      "Iteration 567, loss = 0.12669018\n",
      "Iteration 568, loss = 0.12616757\n",
      "Iteration 569, loss = 0.12609773\n",
      "Iteration 570, loss = 0.12638475\n",
      "Iteration 571, loss = 0.12634494\n",
      "Iteration 572, loss = 0.12550487\n",
      "Iteration 573, loss = 0.12544996\n",
      "Iteration 574, loss = 0.12534442\n",
      "Iteration 575, loss = 0.12518075\n",
      "Iteration 576, loss = 0.12509925\n",
      "Iteration 577, loss = 0.12541978\n",
      "Iteration 578, loss = 0.12466344\n",
      "Iteration 579, loss = 0.12443787\n",
      "Iteration 580, loss = 0.12489616\n",
      "Iteration 581, loss = 0.12414873\n",
      "Iteration 582, loss = 0.12384145\n",
      "Iteration 583, loss = 0.12400309\n",
      "Iteration 584, loss = 0.12419019\n",
      "Iteration 585, loss = 0.12328874\n",
      "Iteration 586, loss = 0.12390729\n",
      "Iteration 587, loss = 0.12338429\n",
      "Iteration 588, loss = 0.12348085\n",
      "Iteration 589, loss = 0.12313941\n",
      "Iteration 590, loss = 0.12319354\n",
      "Iteration 591, loss = 0.12331409\n",
      "Iteration 592, loss = 0.12263245\n",
      "Iteration 593, loss = 0.12300327\n",
      "Iteration 594, loss = 0.12169298\n",
      "Iteration 595, loss = 0.12200624\n",
      "Iteration 596, loss = 0.12188875\n",
      "Iteration 597, loss = 0.12205738\n",
      "Iteration 598, loss = 0.12200036\n",
      "Iteration 599, loss = 0.12088436\n",
      "Iteration 600, loss = 0.12149616\n",
      "Iteration 601, loss = 0.12158126\n",
      "Iteration 602, loss = 0.12161940\n",
      "Iteration 603, loss = 0.12100009\n",
      "Iteration 604, loss = 0.12053022\n",
      "Iteration 605, loss = 0.12046998\n",
      "Iteration 606, loss = 0.12068864\n",
      "Iteration 607, loss = 0.12001435\n",
      "Iteration 608, loss = 0.11981462\n",
      "Iteration 609, loss = 0.12045334\n",
      "Iteration 610, loss = 0.11938461\n",
      "Iteration 611, loss = 0.11989235\n",
      "Iteration 612, loss = 0.11992662\n",
      "Iteration 613, loss = 0.11878227\n",
      "Iteration 614, loss = 0.11977639\n",
      "Iteration 615, loss = 0.11950922\n",
      "Iteration 616, loss = 0.11885425\n",
      "Iteration 617, loss = 0.11877396\n",
      "Iteration 618, loss = 0.11903566\n",
      "Iteration 619, loss = 0.11841496\n",
      "Iteration 620, loss = 0.11819275\n",
      "Iteration 621, loss = 0.11798916\n",
      "Iteration 622, loss = 0.11883706\n",
      "Iteration 623, loss = 0.11769897\n",
      "Iteration 624, loss = 0.11798921\n",
      "Iteration 625, loss = 0.11691468\n",
      "Iteration 626, loss = 0.11815089\n",
      "Iteration 627, loss = 0.11706595\n",
      "Iteration 628, loss = 0.11705760\n",
      "Iteration 629, loss = 0.11721482\n",
      "Iteration 630, loss = 0.11663566\n",
      "Iteration 631, loss = 0.11703603\n",
      "Iteration 632, loss = 0.11638181\n",
      "Iteration 633, loss = 0.11708976\n",
      "Iteration 634, loss = 0.11668052\n",
      "Iteration 635, loss = 0.11637259\n",
      "Iteration 636, loss = 0.11548509\n",
      "Iteration 637, loss = 0.11627267\n",
      "Iteration 638, loss = 0.11658096\n",
      "Iteration 639, loss = 0.11542436\n",
      "Iteration 640, loss = 0.11542223\n",
      "Iteration 641, loss = 0.11531474\n",
      "Iteration 642, loss = 0.11536378\n",
      "Iteration 643, loss = 0.11552980\n",
      "Iteration 644, loss = 0.11513993\n",
      "Iteration 645, loss = 0.11519053\n",
      "Iteration 646, loss = 0.11452913\n",
      "Iteration 647, loss = 0.11453819\n",
      "Iteration 648, loss = 0.11409586\n",
      "Iteration 649, loss = 0.11426017\n",
      "Iteration 650, loss = 0.11438403\n",
      "Iteration 651, loss = 0.11375745\n",
      "Iteration 652, loss = 0.11506554\n",
      "Iteration 653, loss = 0.11416966\n",
      "Iteration 654, loss = 0.11294493\n",
      "Iteration 655, loss = 0.11343614\n",
      "Iteration 656, loss = 0.11389014\n",
      "Iteration 657, loss = 0.11241121\n",
      "Iteration 658, loss = 0.11292208\n",
      "Iteration 659, loss = 0.11199250\n",
      "Iteration 660, loss = 0.11278681\n",
      "Iteration 661, loss = 0.11258486\n",
      "Iteration 662, loss = 0.11246618\n",
      "Iteration 663, loss = 0.11184345\n",
      "Iteration 664, loss = 0.11271483\n",
      "Iteration 665, loss = 0.11216399\n",
      "Iteration 666, loss = 0.11158505\n",
      "Iteration 667, loss = 0.11147701\n",
      "Iteration 668, loss = 0.11196394\n",
      "Iteration 669, loss = 0.11112593\n",
      "Iteration 670, loss = 0.11104324\n",
      "Iteration 671, loss = 0.11104082\n",
      "Iteration 672, loss = 0.11130585\n",
      "Iteration 673, loss = 0.11073681\n",
      "Iteration 674, loss = 0.11062825\n",
      "Iteration 675, loss = 0.11135367\n",
      "Iteration 676, loss = 0.11077004\n",
      "Iteration 677, loss = 0.11100539\n",
      "Iteration 678, loss = 0.11051564\n",
      "Iteration 679, loss = 0.11054798\n",
      "Iteration 680, loss = 0.11151085\n",
      "Iteration 681, loss = 0.11020762\n",
      "Iteration 682, loss = 0.10949084\n",
      "Iteration 683, loss = 0.10983295\n",
      "Iteration 684, loss = 0.10917653\n",
      "Iteration 685, loss = 0.10916032\n",
      "Iteration 686, loss = 0.10936465\n",
      "Iteration 687, loss = 0.10899863\n",
      "Iteration 688, loss = 0.10995230\n",
      "Iteration 689, loss = 0.10845879\n",
      "Iteration 690, loss = 0.10901224\n",
      "Iteration 691, loss = 0.10906049\n",
      "Iteration 692, loss = 0.10857662\n",
      "Iteration 693, loss = 0.10848859\n",
      "Iteration 694, loss = 0.10801118\n",
      "Iteration 695, loss = 0.10868269\n",
      "Iteration 696, loss = 0.10887063\n",
      "Iteration 697, loss = 0.10831178\n",
      "Iteration 698, loss = 0.10816305\n",
      "Iteration 699, loss = 0.10756940\n",
      "Iteration 700, loss = 0.10807976\n",
      "Iteration 701, loss = 0.10747999\n",
      "Iteration 702, loss = 0.10903240\n",
      "Iteration 703, loss = 0.10729796\n",
      "Iteration 704, loss = 0.10775081\n",
      "Iteration 705, loss = 0.10742905\n",
      "Iteration 706, loss = 0.10733149\n",
      "Iteration 707, loss = 0.10707221\n",
      "Iteration 708, loss = 0.10650928\n",
      "Iteration 709, loss = 0.10721180\n",
      "Iteration 710, loss = 0.10676981\n",
      "Iteration 711, loss = 0.10656193\n",
      "Iteration 712, loss = 0.10645604\n",
      "Iteration 713, loss = 0.10647030\n",
      "Iteration 714, loss = 0.10646278\n",
      "Iteration 715, loss = 0.10679925\n",
      "Iteration 716, loss = 0.10603468\n",
      "Iteration 717, loss = 0.10567252\n",
      "Iteration 718, loss = 0.10620797\n",
      "Iteration 719, loss = 0.10623825\n",
      "Iteration 720, loss = 0.10474653\n",
      "Iteration 721, loss = 0.10621649\n",
      "Iteration 722, loss = 0.10520091\n",
      "Iteration 723, loss = 0.10538037\n",
      "Iteration 724, loss = 0.10540824\n",
      "Iteration 725, loss = 0.10507718\n",
      "Iteration 726, loss = 0.10517711\n",
      "Iteration 727, loss = 0.10565874\n",
      "Iteration 728, loss = 0.10479043\n",
      "Iteration 729, loss = 0.10493412\n",
      "Iteration 730, loss = 0.10499523\n",
      "Iteration 731, loss = 0.10477644\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66349895\n",
      "Iteration 2, loss = 0.64413634\n",
      "Iteration 3, loss = 0.63055927\n",
      "Iteration 4, loss = 0.61664296\n",
      "Iteration 5, loss = 0.60449628\n",
      "Iteration 6, loss = 0.59281306\n",
      "Iteration 7, loss = 0.58114391\n",
      "Iteration 8, loss = 0.57170591\n",
      "Iteration 9, loss = 0.56156637\n",
      "Iteration 10, loss = 0.55380329\n",
      "Iteration 11, loss = 0.54448705\n",
      "Iteration 12, loss = 0.53640711\n",
      "Iteration 13, loss = 0.52955620\n",
      "Iteration 14, loss = 0.52332781\n",
      "Iteration 15, loss = 0.51707017\n",
      "Iteration 16, loss = 0.51157318\n",
      "Iteration 17, loss = 0.50590265\n",
      "Iteration 18, loss = 0.50039205\n",
      "Iteration 19, loss = 0.49479465\n",
      "Iteration 20, loss = 0.49087558\n",
      "Iteration 21, loss = 0.48633080\n",
      "Iteration 22, loss = 0.48134641\n",
      "Iteration 23, loss = 0.47862170\n",
      "Iteration 24, loss = 0.47418807\n",
      "Iteration 25, loss = 0.47044985\n",
      "Iteration 26, loss = 0.46765056\n",
      "Iteration 27, loss = 0.46440780\n",
      "Iteration 28, loss = 0.46006425\n",
      "Iteration 29, loss = 0.45783850\n",
      "Iteration 30, loss = 0.45463324\n",
      "Iteration 31, loss = 0.45203176\n",
      "Iteration 32, loss = 0.44964692\n",
      "Iteration 33, loss = 0.44558646\n",
      "Iteration 34, loss = 0.44276257\n",
      "Iteration 35, loss = 0.44126644\n",
      "Iteration 36, loss = 0.43833628\n",
      "Iteration 37, loss = 0.43489302\n",
      "Iteration 38, loss = 0.43297451\n",
      "Iteration 39, loss = 0.43144924\n",
      "Iteration 40, loss = 0.42844857\n",
      "Iteration 41, loss = 0.42524862\n",
      "Iteration 42, loss = 0.42458073\n",
      "Iteration 43, loss = 0.42173241\n",
      "Iteration 44, loss = 0.42007179\n",
      "Iteration 45, loss = 0.41773634\n",
      "Iteration 46, loss = 0.41560722\n",
      "Iteration 47, loss = 0.41432169\n",
      "Iteration 48, loss = 0.41220788\n",
      "Iteration 49, loss = 0.40948488\n",
      "Iteration 50, loss = 0.40740710\n",
      "Iteration 51, loss = 0.40695585\n",
      "Iteration 52, loss = 0.40449299\n",
      "Iteration 53, loss = 0.40300803\n",
      "Iteration 54, loss = 0.40181013\n",
      "Iteration 55, loss = 0.39976559\n",
      "Iteration 56, loss = 0.39723324\n",
      "Iteration 57, loss = 0.39666276\n",
      "Iteration 58, loss = 0.39498991\n",
      "Iteration 59, loss = 0.39273321\n",
      "Iteration 60, loss = 0.39177415\n",
      "Iteration 61, loss = 0.38954675\n",
      "Iteration 62, loss = 0.38915839\n",
      "Iteration 63, loss = 0.38629365\n",
      "Iteration 64, loss = 0.38595049\n",
      "Iteration 65, loss = 0.38416943\n",
      "Iteration 66, loss = 0.38378039\n",
      "Iteration 67, loss = 0.38133483\n",
      "Iteration 68, loss = 0.38074709\n",
      "Iteration 69, loss = 0.37906799\n",
      "Iteration 70, loss = 0.37954356\n",
      "Iteration 71, loss = 0.37586043\n",
      "Iteration 72, loss = 0.37559717\n",
      "Iteration 73, loss = 0.37352726\n",
      "Iteration 74, loss = 0.37357774\n",
      "Iteration 75, loss = 0.37184618\n",
      "Iteration 76, loss = 0.36954325\n",
      "Iteration 77, loss = 0.36866052\n",
      "Iteration 78, loss = 0.36827359\n",
      "Iteration 79, loss = 0.36665792\n",
      "Iteration 80, loss = 0.36545970\n",
      "Iteration 81, loss = 0.36480689\n",
      "Iteration 82, loss = 0.36191857\n",
      "Iteration 83, loss = 0.36286896\n",
      "Iteration 84, loss = 0.36181278\n",
      "Iteration 85, loss = 0.36043566\n",
      "Iteration 86, loss = 0.35964279\n",
      "Iteration 87, loss = 0.35667856\n",
      "Iteration 88, loss = 0.35741732\n",
      "Iteration 89, loss = 0.35546219\n",
      "Iteration 90, loss = 0.35579560\n",
      "Iteration 91, loss = 0.35487839\n",
      "Iteration 92, loss = 0.35225704\n",
      "Iteration 93, loss = 0.35229166\n",
      "Iteration 94, loss = 0.35184041\n",
      "Iteration 95, loss = 0.35068953\n",
      "Iteration 96, loss = 0.34984327\n",
      "Iteration 97, loss = 0.34972930\n",
      "Iteration 98, loss = 0.34973424\n",
      "Iteration 99, loss = 0.34652929\n",
      "Iteration 100, loss = 0.34595755\n",
      "Iteration 101, loss = 0.34569242\n",
      "Iteration 102, loss = 0.34623078\n",
      "Iteration 103, loss = 0.34427232\n",
      "Iteration 104, loss = 0.34344265\n",
      "Iteration 105, loss = 0.34430429\n",
      "Iteration 106, loss = 0.34093081\n",
      "Iteration 107, loss = 0.34185815\n",
      "Iteration 108, loss = 0.34042546\n",
      "Iteration 109, loss = 0.33935649\n",
      "Iteration 110, loss = 0.33930946\n",
      "Iteration 111, loss = 0.33690986\n",
      "Iteration 112, loss = 0.33622949\n",
      "Iteration 113, loss = 0.33622280\n",
      "Iteration 114, loss = 0.33559830\n",
      "Iteration 115, loss = 0.33594054\n",
      "Iteration 116, loss = 0.33492933\n",
      "Iteration 117, loss = 0.33307648\n",
      "Iteration 118, loss = 0.33280540\n",
      "Iteration 119, loss = 0.33285187\n",
      "Iteration 120, loss = 0.33118073\n",
      "Iteration 121, loss = 0.33092904\n",
      "Iteration 122, loss = 0.32956041\n",
      "Iteration 123, loss = 0.33058665\n",
      "Iteration 124, loss = 0.32991727\n",
      "Iteration 125, loss = 0.32795351\n",
      "Iteration 126, loss = 0.32667049\n",
      "Iteration 127, loss = 0.32553321\n",
      "Iteration 128, loss = 0.32536194\n",
      "Iteration 129, loss = 0.32469694\n",
      "Iteration 130, loss = 0.32453601\n",
      "Iteration 131, loss = 0.32373560\n",
      "Iteration 132, loss = 0.32316059\n",
      "Iteration 133, loss = 0.32324288\n",
      "Iteration 134, loss = 0.32152544\n",
      "Iteration 135, loss = 0.32209337\n",
      "Iteration 136, loss = 0.32084421\n",
      "Iteration 137, loss = 0.32043471\n",
      "Iteration 138, loss = 0.32012277\n",
      "Iteration 139, loss = 0.31896962\n",
      "Iteration 140, loss = 0.31817268\n",
      "Iteration 141, loss = 0.31827265\n",
      "Iteration 142, loss = 0.31632507\n",
      "Iteration 143, loss = 0.31597153\n",
      "Iteration 144, loss = 0.31531153\n",
      "Iteration 145, loss = 0.31503586\n",
      "Iteration 146, loss = 0.31437751\n",
      "Iteration 147, loss = 0.31358247\n",
      "Iteration 148, loss = 0.31298758\n",
      "Iteration 149, loss = 0.31567613\n",
      "Iteration 150, loss = 0.31091007\n",
      "Iteration 151, loss = 0.31180261\n",
      "Iteration 152, loss = 0.31080640\n",
      "Iteration 153, loss = 0.31039289\n",
      "Iteration 154, loss = 0.30912407\n",
      "Iteration 155, loss = 0.30862163\n",
      "Iteration 156, loss = 0.30901442\n",
      "Iteration 157, loss = 0.30714984\n",
      "Iteration 158, loss = 0.30882620\n",
      "Iteration 159, loss = 0.30891164\n",
      "Iteration 160, loss = 0.30486557\n",
      "Iteration 161, loss = 0.30544575\n",
      "Iteration 162, loss = 0.30645192\n",
      "Iteration 163, loss = 0.30489069\n",
      "Iteration 164, loss = 0.30517102\n",
      "Iteration 165, loss = 0.30491152\n",
      "Iteration 166, loss = 0.30347643\n",
      "Iteration 167, loss = 0.30259404\n",
      "Iteration 168, loss = 0.30253227\n",
      "Iteration 169, loss = 0.30279104\n",
      "Iteration 170, loss = 0.30062222\n",
      "Iteration 171, loss = 0.30126292\n",
      "Iteration 172, loss = 0.30179862\n",
      "Iteration 173, loss = 0.29927915\n",
      "Iteration 174, loss = 0.29922051\n",
      "Iteration 175, loss = 0.29996772\n",
      "Iteration 176, loss = 0.29885291\n",
      "Iteration 177, loss = 0.29822551\n",
      "Iteration 178, loss = 0.29658232\n",
      "Iteration 179, loss = 0.29578515\n",
      "Iteration 180, loss = 0.29648098\n",
      "Iteration 181, loss = 0.29692736\n",
      "Iteration 182, loss = 0.29555252\n",
      "Iteration 183, loss = 0.29411853\n",
      "Iteration 184, loss = 0.29395156\n",
      "Iteration 185, loss = 0.29544288\n",
      "Iteration 186, loss = 0.29523034\n",
      "Iteration 187, loss = 0.29380084\n",
      "Iteration 188, loss = 0.29346019\n",
      "Iteration 189, loss = 0.29271593\n",
      "Iteration 190, loss = 0.29192325\n",
      "Iteration 191, loss = 0.29210671\n",
      "Iteration 192, loss = 0.29400712\n",
      "Iteration 193, loss = 0.29160201\n",
      "Iteration 194, loss = 0.29028313\n",
      "Iteration 195, loss = 0.29114418\n",
      "Iteration 196, loss = 0.29052253\n",
      "Iteration 197, loss = 0.29013710\n",
      "Iteration 198, loss = 0.28782174\n",
      "Iteration 199, loss = 0.28886585\n",
      "Iteration 200, loss = 0.28697436\n",
      "Iteration 201, loss = 0.28748294\n",
      "Iteration 202, loss = 0.28873324\n",
      "Iteration 203, loss = 0.28663183\n",
      "Iteration 204, loss = 0.28791699\n",
      "Iteration 205, loss = 0.28619305\n",
      "Iteration 206, loss = 0.28743327\n",
      "Iteration 207, loss = 0.28613617\n",
      "Iteration 208, loss = 0.28424313\n",
      "Iteration 209, loss = 0.28532565\n",
      "Iteration 210, loss = 0.28565404\n",
      "Iteration 211, loss = 0.28521215\n",
      "Iteration 212, loss = 0.28457232\n",
      "Iteration 213, loss = 0.28304677\n",
      "Iteration 214, loss = 0.28290114\n",
      "Iteration 215, loss = 0.28197541\n",
      "Iteration 216, loss = 0.28245409\n",
      "Iteration 217, loss = 0.28289915\n",
      "Iteration 218, loss = 0.28066840\n",
      "Iteration 219, loss = 0.28098280\n",
      "Iteration 220, loss = 0.28181043\n",
      "Iteration 221, loss = 0.28015183\n",
      "Iteration 222, loss = 0.28067263\n",
      "Iteration 223, loss = 0.28013007\n",
      "Iteration 224, loss = 0.27916196\n",
      "Iteration 225, loss = 0.27831059\n",
      "Iteration 226, loss = 0.27850045\n",
      "Iteration 227, loss = 0.27799556\n",
      "Iteration 228, loss = 0.27782294\n",
      "Iteration 229, loss = 0.27717927\n",
      "Iteration 230, loss = 0.27734173\n",
      "Iteration 231, loss = 0.27711381\n",
      "Iteration 232, loss = 0.27681314\n",
      "Iteration 233, loss = 0.27638286\n",
      "Iteration 234, loss = 0.27489520\n",
      "Iteration 235, loss = 0.27591565\n",
      "Iteration 236, loss = 0.27440334\n",
      "Iteration 237, loss = 0.27491008\n",
      "Iteration 238, loss = 0.27347671\n",
      "Iteration 239, loss = 0.27450679\n",
      "Iteration 240, loss = 0.27640190\n",
      "Iteration 241, loss = 0.27344813\n",
      "Iteration 242, loss = 0.27305804\n",
      "Iteration 243, loss = 0.27285591\n",
      "Iteration 244, loss = 0.27287252\n",
      "Iteration 245, loss = 0.27145365\n",
      "Iteration 246, loss = 0.27199147\n",
      "Iteration 247, loss = 0.27064136\n",
      "Iteration 248, loss = 0.27466186\n",
      "Iteration 249, loss = 0.27002099\n",
      "Iteration 250, loss = 0.27126922\n",
      "Iteration 251, loss = 0.26963526\n",
      "Iteration 252, loss = 0.26911422\n",
      "Iteration 253, loss = 0.26955766\n",
      "Iteration 254, loss = 0.26990415\n",
      "Iteration 255, loss = 0.26970035\n",
      "Iteration 256, loss = 0.26950619\n",
      "Iteration 257, loss = 0.26842555\n",
      "Iteration 258, loss = 0.26848153\n",
      "Iteration 259, loss = 0.26801527\n",
      "Iteration 260, loss = 0.26792111\n",
      "Iteration 261, loss = 0.26860641\n",
      "Iteration 262, loss = 0.26574907\n",
      "Iteration 263, loss = 0.26706709\n",
      "Iteration 264, loss = 0.26476691\n",
      "Iteration 265, loss = 0.26535938\n",
      "Iteration 266, loss = 0.26603613\n",
      "Iteration 267, loss = 0.26655247\n",
      "Iteration 268, loss = 0.26440917\n",
      "Iteration 269, loss = 0.26446950\n",
      "Iteration 270, loss = 0.26458632\n",
      "Iteration 271, loss = 0.26318884\n",
      "Iteration 272, loss = 0.26293236\n",
      "Iteration 273, loss = 0.26520765\n",
      "Iteration 274, loss = 0.26550239\n",
      "Iteration 275, loss = 0.26383897\n",
      "Iteration 276, loss = 0.26118792\n",
      "Iteration 277, loss = 0.26190724\n",
      "Iteration 278, loss = 0.26139587\n",
      "Iteration 279, loss = 0.26306122\n",
      "Iteration 280, loss = 0.26216012\n",
      "Iteration 281, loss = 0.26094613\n",
      "Iteration 282, loss = 0.26137464\n",
      "Iteration 283, loss = 0.26402322\n",
      "Iteration 284, loss = 0.25933600\n",
      "Iteration 285, loss = 0.25846079\n",
      "Iteration 286, loss = 0.25910107\n",
      "Iteration 287, loss = 0.25850785\n",
      "Iteration 288, loss = 0.26165957\n",
      "Iteration 289, loss = 0.25995999\n",
      "Iteration 290, loss = 0.25956622\n",
      "Iteration 291, loss = 0.25784344\n",
      "Iteration 292, loss = 0.25943448\n",
      "Iteration 293, loss = 0.25716358\n",
      "Iteration 294, loss = 0.25769099\n",
      "Iteration 295, loss = 0.25845666\n",
      "Iteration 296, loss = 0.25732964\n",
      "Iteration 297, loss = 0.25660726\n",
      "Iteration 298, loss = 0.25631503\n",
      "Iteration 299, loss = 0.25665890\n",
      "Iteration 300, loss = 0.25558307\n",
      "Iteration 301, loss = 0.25816363\n",
      "Iteration 302, loss = 0.25535292\n",
      "Iteration 303, loss = 0.25652485\n",
      "Iteration 304, loss = 0.25628067\n",
      "Iteration 305, loss = 0.25658320\n",
      "Iteration 306, loss = 0.25488270\n",
      "Iteration 307, loss = 0.25361094\n",
      "Iteration 308, loss = 0.25332423\n",
      "Iteration 309, loss = 0.25489427\n",
      "Iteration 310, loss = 0.25369680\n",
      "Iteration 311, loss = 0.25376559\n",
      "Iteration 312, loss = 0.25240473\n",
      "Iteration 313, loss = 0.25401653\n",
      "Iteration 314, loss = 0.25307410\n",
      "Iteration 315, loss = 0.25414790\n",
      "Iteration 316, loss = 0.25126863\n",
      "Iteration 317, loss = 0.25286675\n",
      "Iteration 318, loss = 0.25062430\n",
      "Iteration 319, loss = 0.25305410\n",
      "Iteration 320, loss = 0.25179000\n",
      "Iteration 321, loss = 0.25100971\n",
      "Iteration 322, loss = 0.25093306\n",
      "Iteration 323, loss = 0.24985670\n",
      "Iteration 324, loss = 0.25219664\n",
      "Iteration 325, loss = 0.25073602\n",
      "Iteration 326, loss = 0.24949901\n",
      "Iteration 327, loss = 0.25126801\n",
      "Iteration 328, loss = 0.24916814\n",
      "Iteration 329, loss = 0.25012415\n",
      "Iteration 330, loss = 0.24876060\n",
      "Iteration 331, loss = 0.24825117\n",
      "Iteration 332, loss = 0.24770033\n",
      "Iteration 333, loss = 0.24917632\n",
      "Iteration 334, loss = 0.24729307\n",
      "Iteration 335, loss = 0.25135559\n",
      "Iteration 336, loss = 0.24800330\n",
      "Iteration 337, loss = 0.24672538\n",
      "Iteration 338, loss = 0.24762850\n",
      "Iteration 339, loss = 0.24664591\n",
      "Iteration 340, loss = 0.24728517\n",
      "Iteration 341, loss = 0.24691792\n",
      "Iteration 342, loss = 0.24648823\n",
      "Iteration 343, loss = 0.24573801\n",
      "Iteration 344, loss = 0.24652554\n",
      "Iteration 345, loss = 0.24538851\n",
      "Iteration 346, loss = 0.24467065\n",
      "Iteration 347, loss = 0.24427851\n",
      "Iteration 348, loss = 0.24420786\n",
      "Iteration 349, loss = 0.24437620\n",
      "Iteration 350, loss = 0.24512602\n",
      "Iteration 351, loss = 0.24415664\n",
      "Iteration 352, loss = 0.24547732\n",
      "Iteration 353, loss = 0.24409672\n",
      "Iteration 354, loss = 0.24304018\n",
      "Iteration 355, loss = 0.24347921\n",
      "Iteration 356, loss = 0.24215433\n",
      "Iteration 357, loss = 0.24398538\n",
      "Iteration 358, loss = 0.24140110\n",
      "Iteration 359, loss = 0.24191847\n",
      "Iteration 360, loss = 0.24384407\n",
      "Iteration 361, loss = 0.24160670\n",
      "Iteration 362, loss = 0.24057411\n",
      "Iteration 363, loss = 0.24325130\n",
      "Iteration 364, loss = 0.24124811\n",
      "Iteration 365, loss = 0.24048497\n",
      "Iteration 366, loss = 0.24074970\n",
      "Iteration 367, loss = 0.24185970\n",
      "Iteration 368, loss = 0.24219910\n",
      "Iteration 369, loss = 0.24222479\n",
      "Iteration 370, loss = 0.24200726\n",
      "Iteration 371, loss = 0.23958066\n",
      "Iteration 372, loss = 0.24153905\n",
      "Iteration 373, loss = 0.23897869\n",
      "Iteration 374, loss = 0.23950225\n",
      "Iteration 375, loss = 0.24145338\n",
      "Iteration 376, loss = 0.24030324\n",
      "Iteration 377, loss = 0.23996541\n",
      "Iteration 378, loss = 0.23881356\n",
      "Iteration 379, loss = 0.23876796\n",
      "Iteration 380, loss = 0.23730382\n",
      "Iteration 381, loss = 0.23857668\n",
      "Iteration 382, loss = 0.23847736\n",
      "Iteration 383, loss = 0.23801887\n",
      "Iteration 384, loss = 0.23852307\n",
      "Iteration 385, loss = 0.23752690\n",
      "Iteration 386, loss = 0.23892146\n",
      "Iteration 387, loss = 0.23915624\n",
      "Iteration 388, loss = 0.23597410\n",
      "Iteration 389, loss = 0.23843083\n",
      "Iteration 390, loss = 0.23698537\n",
      "Iteration 391, loss = 0.23530973\n",
      "Iteration 392, loss = 0.23685902\n",
      "Iteration 393, loss = 0.23570312\n",
      "Iteration 394, loss = 0.23592483\n",
      "Iteration 395, loss = 0.23453663\n",
      "Iteration 396, loss = 0.23541094\n",
      "Iteration 397, loss = 0.23534416\n",
      "Iteration 398, loss = 0.23419868\n",
      "Iteration 399, loss = 0.23562405\n",
      "Iteration 400, loss = 0.23563295\n",
      "Iteration 401, loss = 0.23772559\n",
      "Iteration 402, loss = 0.23394812\n",
      "Iteration 403, loss = 0.23553413\n",
      "Iteration 404, loss = 0.23476865\n",
      "Iteration 405, loss = 0.23679927\n",
      "Iteration 406, loss = 0.23478836\n",
      "Iteration 407, loss = 0.23238529\n",
      "Iteration 408, loss = 0.23460582\n",
      "Iteration 409, loss = 0.23333514\n",
      "Iteration 410, loss = 0.23302063\n",
      "Iteration 411, loss = 0.23163914\n",
      "Iteration 412, loss = 0.23654890\n",
      "Iteration 413, loss = 0.23268938\n",
      "Iteration 414, loss = 0.23356316\n",
      "Iteration 415, loss = 0.23296167\n",
      "Iteration 416, loss = 0.23291352\n",
      "Iteration 417, loss = 0.23228542\n",
      "Iteration 418, loss = 0.23174680\n",
      "Iteration 419, loss = 0.23454261\n",
      "Iteration 420, loss = 0.23220864\n",
      "Iteration 421, loss = 0.23205685\n",
      "Iteration 422, loss = 0.23282073\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65163449\n",
      "Iteration 2, loss = 0.64568639\n",
      "Iteration 3, loss = 0.64178817\n",
      "Iteration 4, loss = 0.63798521\n",
      "Iteration 5, loss = 0.63390166\n",
      "Iteration 6, loss = 0.62908503\n",
      "Iteration 7, loss = 0.62423493\n",
      "Iteration 8, loss = 0.61893730\n",
      "Iteration 9, loss = 0.61409519\n",
      "Iteration 10, loss = 0.60826204\n",
      "Iteration 11, loss = 0.60233906\n",
      "Iteration 12, loss = 0.59627551\n",
      "Iteration 13, loss = 0.58983280\n",
      "Iteration 14, loss = 0.58338872\n",
      "Iteration 15, loss = 0.57658458\n",
      "Iteration 16, loss = 0.56886913\n",
      "Iteration 17, loss = 0.56171838\n",
      "Iteration 18, loss = 0.55428909\n",
      "Iteration 19, loss = 0.54677487\n",
      "Iteration 20, loss = 0.53867361\n",
      "Iteration 21, loss = 0.53174212\n",
      "Iteration 22, loss = 0.52369316\n",
      "Iteration 23, loss = 0.51688001\n",
      "Iteration 24, loss = 0.50978107\n",
      "Iteration 25, loss = 0.50182332\n",
      "Iteration 26, loss = 0.49502492\n",
      "Iteration 27, loss = 0.48777953\n",
      "Iteration 28, loss = 0.48099633\n",
      "Iteration 29, loss = 0.47466775\n",
      "Iteration 30, loss = 0.46876004\n",
      "Iteration 31, loss = 0.46081862\n",
      "Iteration 32, loss = 0.45548092\n",
      "Iteration 33, loss = 0.44807547\n",
      "Iteration 34, loss = 0.44189758\n",
      "Iteration 35, loss = 0.43659378\n",
      "Iteration 36, loss = 0.42979853\n",
      "Iteration 37, loss = 0.42420000\n",
      "Iteration 38, loss = 0.41846438\n",
      "Iteration 39, loss = 0.41253405\n",
      "Iteration 40, loss = 0.40711513\n",
      "Iteration 41, loss = 0.40180529\n",
      "Iteration 42, loss = 0.39592847\n",
      "Iteration 43, loss = 0.39107435\n",
      "Iteration 44, loss = 0.38566627\n",
      "Iteration 45, loss = 0.38022974\n",
      "Iteration 46, loss = 0.37639444\n",
      "Iteration 47, loss = 0.37113153\n",
      "Iteration 48, loss = 0.36583402\n",
      "Iteration 49, loss = 0.36133267\n",
      "Iteration 50, loss = 0.35733912\n",
      "Iteration 51, loss = 0.35222311\n",
      "Iteration 52, loss = 0.34793077\n",
      "Iteration 53, loss = 0.34403523\n",
      "Iteration 54, loss = 0.33983713\n",
      "Iteration 55, loss = 0.33602391\n",
      "Iteration 56, loss = 0.33173059\n",
      "Iteration 57, loss = 0.32820300\n",
      "Iteration 58, loss = 0.32392519\n",
      "Iteration 59, loss = 0.32038875\n",
      "Iteration 60, loss = 0.31660557\n",
      "Iteration 61, loss = 0.31348286\n",
      "Iteration 62, loss = 0.30986689\n",
      "Iteration 63, loss = 0.30664520\n",
      "Iteration 64, loss = 0.30207317\n",
      "Iteration 65, loss = 0.29957786\n",
      "Iteration 66, loss = 0.29631840\n",
      "Iteration 67, loss = 0.29297095\n",
      "Iteration 68, loss = 0.28954602\n",
      "Iteration 69, loss = 0.28670088\n",
      "Iteration 70, loss = 0.28310306\n",
      "Iteration 71, loss = 0.28056450\n",
      "Iteration 72, loss = 0.27705814\n",
      "Iteration 73, loss = 0.27385009\n",
      "Iteration 74, loss = 0.27113889\n",
      "Iteration 75, loss = 0.26753793\n",
      "Iteration 76, loss = 0.26534971\n",
      "Iteration 77, loss = 0.26232534\n",
      "Iteration 78, loss = 0.26003941\n",
      "Iteration 79, loss = 0.25671366\n",
      "Iteration 80, loss = 0.25396182\n",
      "Iteration 81, loss = 0.25158001\n",
      "Iteration 82, loss = 0.24900225\n",
      "Iteration 83, loss = 0.24669636\n",
      "Iteration 84, loss = 0.24393285\n",
      "Iteration 85, loss = 0.24154532\n",
      "Iteration 86, loss = 0.23883574\n",
      "Iteration 87, loss = 0.23671925\n",
      "Iteration 88, loss = 0.23386728\n",
      "Iteration 89, loss = 0.23193324\n",
      "Iteration 90, loss = 0.22961487\n",
      "Iteration 91, loss = 0.22773092\n",
      "Iteration 92, loss = 0.22499472\n",
      "Iteration 93, loss = 0.22309119\n",
      "Iteration 94, loss = 0.22104072\n",
      "Iteration 95, loss = 0.21848281\n",
      "Iteration 96, loss = 0.21648380\n",
      "Iteration 97, loss = 0.21426667\n",
      "Iteration 98, loss = 0.21260986\n",
      "Iteration 99, loss = 0.21010004\n",
      "Iteration 100, loss = 0.20816706\n",
      "Iteration 101, loss = 0.20656795\n",
      "Iteration 102, loss = 0.20434770\n",
      "Iteration 103, loss = 0.20246628\n",
      "Iteration 104, loss = 0.20087801\n",
      "Iteration 105, loss = 0.19930349\n",
      "Iteration 106, loss = 0.19735586\n",
      "Iteration 107, loss = 0.19537748\n",
      "Iteration 108, loss = 0.19386067\n",
      "Iteration 109, loss = 0.19163043\n",
      "Iteration 110, loss = 0.19012716\n",
      "Iteration 111, loss = 0.18848472\n",
      "Iteration 112, loss = 0.18648982\n",
      "Iteration 113, loss = 0.18469590\n",
      "Iteration 114, loss = 0.18336955\n",
      "Iteration 115, loss = 0.18162370\n",
      "Iteration 116, loss = 0.18011195\n",
      "Iteration 117, loss = 0.17866432\n",
      "Iteration 118, loss = 0.17687153\n",
      "Iteration 119, loss = 0.17536892\n",
      "Iteration 120, loss = 0.17361887\n",
      "Iteration 121, loss = 0.17281871\n",
      "Iteration 122, loss = 0.17055282\n",
      "Iteration 123, loss = 0.16924891\n",
      "Iteration 124, loss = 0.16788457\n",
      "Iteration 125, loss = 0.16596200\n",
      "Iteration 126, loss = 0.16497115\n",
      "Iteration 127, loss = 0.16311448\n",
      "Iteration 128, loss = 0.16176653\n",
      "Iteration 129, loss = 0.16054520\n",
      "Iteration 130, loss = 0.15908238\n",
      "Iteration 131, loss = 0.15777602\n",
      "Iteration 132, loss = 0.15656561\n",
      "Iteration 133, loss = 0.15517781\n",
      "Iteration 134, loss = 0.15345895\n",
      "Iteration 135, loss = 0.15196859\n",
      "Iteration 136, loss = 0.15065832\n",
      "Iteration 137, loss = 0.15012596\n",
      "Iteration 138, loss = 0.14823011\n",
      "Iteration 139, loss = 0.14664873\n",
      "Iteration 140, loss = 0.14600372\n",
      "Iteration 141, loss = 0.14493564\n",
      "Iteration 142, loss = 0.14335774\n",
      "Iteration 143, loss = 0.14192837\n",
      "Iteration 144, loss = 0.14101488\n",
      "Iteration 145, loss = 0.13973353\n",
      "Iteration 146, loss = 0.13827172\n",
      "Iteration 147, loss = 0.13756248\n",
      "Iteration 148, loss = 0.13617107\n",
      "Iteration 149, loss = 0.13470482\n",
      "Iteration 150, loss = 0.13398374\n",
      "Iteration 151, loss = 0.13220397\n",
      "Iteration 152, loss = 0.13129770\n",
      "Iteration 153, loss = 0.13078766\n",
      "Iteration 154, loss = 0.12961339\n",
      "Iteration 155, loss = 0.12832421\n",
      "Iteration 156, loss = 0.12743980\n",
      "Iteration 157, loss = 0.12610771\n",
      "Iteration 158, loss = 0.12526129\n",
      "Iteration 159, loss = 0.12434675\n",
      "Iteration 160, loss = 0.12303309\n",
      "Iteration 161, loss = 0.12197889\n",
      "Iteration 162, loss = 0.12065088\n",
      "Iteration 163, loss = 0.11979773\n",
      "Iteration 164, loss = 0.11897523\n",
      "Iteration 165, loss = 0.11820733\n",
      "Iteration 166, loss = 0.11729170\n",
      "Iteration 167, loss = 0.11653787\n",
      "Iteration 168, loss = 0.11556212\n",
      "Iteration 169, loss = 0.11384284\n",
      "Iteration 170, loss = 0.11333269\n",
      "Iteration 171, loss = 0.11269601\n",
      "Iteration 172, loss = 0.11141252\n",
      "Iteration 173, loss = 0.11050067\n",
      "Iteration 174, loss = 0.10999164\n",
      "Iteration 175, loss = 0.10907547\n",
      "Iteration 176, loss = 0.10785037\n",
      "Iteration 177, loss = 0.10662859\n",
      "Iteration 178, loss = 0.10592062\n",
      "Iteration 179, loss = 0.10593383\n",
      "Iteration 180, loss = 0.10415804\n",
      "Iteration 181, loss = 0.10393588\n",
      "Iteration 182, loss = 0.10274938\n",
      "Iteration 183, loss = 0.10185160\n",
      "Iteration 184, loss = 0.10124653\n",
      "Iteration 185, loss = 0.10014260\n",
      "Iteration 186, loss = 0.09968255\n",
      "Iteration 187, loss = 0.09846718\n",
      "Iteration 188, loss = 0.09763950\n",
      "Iteration 189, loss = 0.09715381\n",
      "Iteration 190, loss = 0.09627327\n",
      "Iteration 191, loss = 0.09610658\n",
      "Iteration 192, loss = 0.09499086\n",
      "Iteration 193, loss = 0.09439013\n",
      "Iteration 194, loss = 0.09327937\n",
      "Iteration 195, loss = 0.09226861\n",
      "Iteration 196, loss = 0.09178547\n",
      "Iteration 197, loss = 0.09121994\n",
      "Iteration 198, loss = 0.09055939\n",
      "Iteration 199, loss = 0.09031905\n",
      "Iteration 200, loss = 0.08923660\n",
      "Iteration 201, loss = 0.08849794\n",
      "Iteration 202, loss = 0.08777129\n",
      "Iteration 203, loss = 0.08680385\n",
      "Iteration 204, loss = 0.08631999\n",
      "Iteration 205, loss = 0.08561054\n",
      "Iteration 206, loss = 0.08503185\n",
      "Iteration 207, loss = 0.08448829\n",
      "Iteration 208, loss = 0.08370268\n",
      "Iteration 209, loss = 0.08304225\n",
      "Iteration 210, loss = 0.08312304\n",
      "Iteration 211, loss = 0.08204040\n",
      "Iteration 212, loss = 0.08145041\n",
      "Iteration 213, loss = 0.08087655\n",
      "Iteration 214, loss = 0.08060345\n",
      "Iteration 215, loss = 0.07946133\n",
      "Iteration 216, loss = 0.07914137\n",
      "Iteration 217, loss = 0.07854593\n",
      "Iteration 218, loss = 0.07775907\n",
      "Iteration 219, loss = 0.07722750\n",
      "Iteration 220, loss = 0.07701487\n",
      "Iteration 221, loss = 0.07610716\n",
      "Iteration 222, loss = 0.07555924\n",
      "Iteration 223, loss = 0.07486472\n",
      "Iteration 224, loss = 0.07463850\n",
      "Iteration 225, loss = 0.07366962\n",
      "Iteration 226, loss = 0.07395028\n",
      "Iteration 227, loss = 0.07284694\n",
      "Iteration 228, loss = 0.07288336\n",
      "Iteration 229, loss = 0.07191022\n",
      "Iteration 230, loss = 0.07110837\n",
      "Iteration 231, loss = 0.07089465\n",
      "Iteration 232, loss = 0.07043527\n",
      "Iteration 233, loss = 0.07030488\n",
      "Iteration 234, loss = 0.06929463\n",
      "Iteration 235, loss = 0.06888733\n",
      "Iteration 236, loss = 0.06881735\n",
      "Iteration 237, loss = 0.06805549\n",
      "Iteration 238, loss = 0.06752275\n",
      "Iteration 239, loss = 0.06684467\n",
      "Iteration 240, loss = 0.06624363\n",
      "Iteration 241, loss = 0.06636739\n",
      "Iteration 242, loss = 0.06537238\n",
      "Iteration 243, loss = 0.06479479\n",
      "Iteration 244, loss = 0.06489857\n",
      "Iteration 245, loss = 0.06417664\n",
      "Iteration 246, loss = 0.06369211\n",
      "Iteration 247, loss = 0.06351511\n",
      "Iteration 248, loss = 0.06311592\n",
      "Iteration 249, loss = 0.06257515\n",
      "Iteration 250, loss = 0.06206888\n",
      "Iteration 251, loss = 0.06133745\n",
      "Iteration 252, loss = 0.06091933\n",
      "Iteration 253, loss = 0.06062929\n",
      "Iteration 254, loss = 0.06058381\n",
      "Iteration 255, loss = 0.05978187\n",
      "Iteration 256, loss = 0.05932396\n",
      "Iteration 257, loss = 0.05875408\n",
      "Iteration 258, loss = 0.05891886\n",
      "Iteration 259, loss = 0.05824158\n",
      "Iteration 260, loss = 0.05766582\n",
      "Iteration 261, loss = 0.05767413\n",
      "Iteration 262, loss = 0.05717196\n",
      "Iteration 263, loss = 0.05691272\n",
      "Iteration 264, loss = 0.05592782\n",
      "Iteration 265, loss = 0.05596777\n",
      "Iteration 266, loss = 0.05545987\n",
      "Iteration 267, loss = 0.05534807\n",
      "Iteration 268, loss = 0.05469213\n",
      "Iteration 269, loss = 0.05405870\n",
      "Iteration 270, loss = 0.05371893\n",
      "Iteration 271, loss = 0.05379777\n",
      "Iteration 272, loss = 0.05329881\n",
      "Iteration 273, loss = 0.05296300\n",
      "Iteration 274, loss = 0.05220518\n",
      "Iteration 275, loss = 0.05204276\n",
      "Iteration 276, loss = 0.05198837\n",
      "Iteration 277, loss = 0.05161683\n",
      "Iteration 278, loss = 0.05085892\n",
      "Iteration 279, loss = 0.05072413\n",
      "Iteration 280, loss = 0.05000966\n",
      "Iteration 281, loss = 0.05052043\n",
      "Iteration 282, loss = 0.04987307\n",
      "Iteration 283, loss = 0.04881502\n",
      "Iteration 284, loss = 0.04835933\n",
      "Iteration 285, loss = 0.04874316\n",
      "Iteration 286, loss = 0.04852574\n",
      "Iteration 287, loss = 0.04843882\n",
      "Iteration 288, loss = 0.04758885\n",
      "Iteration 289, loss = 0.04696584\n",
      "Iteration 290, loss = 0.04705432\n",
      "Iteration 291, loss = 0.04687135\n",
      "Iteration 292, loss = 0.04656337\n",
      "Iteration 293, loss = 0.04591667\n",
      "Iteration 294, loss = 0.04562508\n",
      "Iteration 295, loss = 0.04544274\n",
      "Iteration 296, loss = 0.04506172\n",
      "Iteration 297, loss = 0.04486218\n",
      "Iteration 298, loss = 0.04496459\n",
      "Iteration 299, loss = 0.04435305\n",
      "Iteration 300, loss = 0.04382736\n",
      "Iteration 301, loss = 0.04358074\n",
      "Iteration 302, loss = 0.04324337\n",
      "Iteration 303, loss = 0.04330332\n",
      "Iteration 304, loss = 0.04281921\n",
      "Iteration 305, loss = 0.04286907\n",
      "Iteration 306, loss = 0.04224674\n",
      "Iteration 307, loss = 0.04199419\n",
      "Iteration 308, loss = 0.04161709\n",
      "Iteration 309, loss = 0.04164826\n",
      "Iteration 310, loss = 0.04129437\n",
      "Iteration 311, loss = 0.04120344\n",
      "Iteration 312, loss = 0.04106589\n",
      "Iteration 313, loss = 0.03989329\n",
      "Iteration 314, loss = 0.04033141\n",
      "Iteration 315, loss = 0.04028848\n",
      "Iteration 316, loss = 0.03962478\n",
      "Iteration 317, loss = 0.03935532\n",
      "Iteration 318, loss = 0.03939214\n",
      "Iteration 319, loss = 0.03918971\n",
      "Iteration 320, loss = 0.03855935\n",
      "Iteration 321, loss = 0.03887681\n",
      "Iteration 322, loss = 0.03810415\n",
      "Iteration 323, loss = 0.03790869\n",
      "Iteration 324, loss = 0.03781655\n",
      "Iteration 325, loss = 0.03747905\n",
      "Iteration 326, loss = 0.03737715\n",
      "Iteration 327, loss = 0.03712495\n",
      "Iteration 328, loss = 0.03671414\n",
      "Iteration 329, loss = 0.03644095\n",
      "Iteration 330, loss = 0.03649267\n",
      "Iteration 331, loss = 0.03635845\n",
      "Iteration 332, loss = 0.03596407\n",
      "Iteration 333, loss = 0.03607528\n",
      "Iteration 334, loss = 0.03547044\n",
      "Iteration 335, loss = 0.03537639\n",
      "Iteration 336, loss = 0.03479588\n",
      "Iteration 337, loss = 0.03477558\n",
      "Iteration 338, loss = 0.03451258\n",
      "Iteration 339, loss = 0.03471547\n",
      "Iteration 340, loss = 0.03408153\n",
      "Iteration 341, loss = 0.03398874\n",
      "Iteration 342, loss = 0.03450553\n",
      "Iteration 343, loss = 0.03390842\n",
      "Iteration 344, loss = 0.03374381\n",
      "Iteration 345, loss = 0.03334670\n",
      "Iteration 346, loss = 0.03287946\n",
      "Iteration 347, loss = 0.03327202\n",
      "Iteration 348, loss = 0.03229834\n",
      "Iteration 349, loss = 0.03284340\n",
      "Iteration 350, loss = 0.03230346\n",
      "Iteration 351, loss = 0.03222173\n",
      "Iteration 352, loss = 0.03227336\n",
      "Iteration 353, loss = 0.03188696\n",
      "Iteration 354, loss = 0.03201851\n",
      "Iteration 355, loss = 0.03142050\n",
      "Iteration 356, loss = 0.03138724\n",
      "Iteration 357, loss = 0.03060732\n",
      "Iteration 358, loss = 0.03142206\n",
      "Iteration 359, loss = 0.03092128\n",
      "Iteration 360, loss = 0.03089754\n",
      "Iteration 361, loss = 0.03109661\n",
      "Iteration 362, loss = 0.03086543\n",
      "Iteration 363, loss = 0.03051801\n",
      "Iteration 364, loss = 0.03049824\n",
      "Iteration 365, loss = 0.02959550\n",
      "Iteration 366, loss = 0.03001964\n",
      "Iteration 367, loss = 0.02994651\n",
      "Iteration 368, loss = 0.02947961\n",
      "Iteration 369, loss = 0.02934617\n",
      "Iteration 370, loss = 0.02936375\n",
      "Iteration 371, loss = 0.02954369\n",
      "Iteration 372, loss = 0.02916437\n",
      "Iteration 373, loss = 0.02953179\n",
      "Iteration 374, loss = 0.02928251\n",
      "Iteration 375, loss = 0.02873078\n",
      "Iteration 376, loss = 0.02860009\n",
      "Iteration 377, loss = 0.02834690\n",
      "Iteration 378, loss = 0.02823657\n",
      "Iteration 379, loss = 0.02846914\n",
      "Iteration 380, loss = 0.02867101\n",
      "Iteration 381, loss = 0.02750062\n",
      "Iteration 382, loss = 0.02756689\n",
      "Iteration 383, loss = 0.02793130\n",
      "Iteration 384, loss = 0.02775069\n",
      "Iteration 385, loss = 0.02751213\n",
      "Iteration 386, loss = 0.02722744\n",
      "Iteration 387, loss = 0.02702582\n",
      "Iteration 388, loss = 0.02753132\n",
      "Iteration 389, loss = 0.02755666\n",
      "Iteration 390, loss = 0.02632384\n",
      "Iteration 391, loss = 0.02696282\n",
      "Iteration 392, loss = 0.02746503\n",
      "Iteration 393, loss = 0.02633034\n",
      "Iteration 394, loss = 0.02641452\n",
      "Iteration 395, loss = 0.02592163\n",
      "Iteration 396, loss = 0.02669170\n",
      "Iteration 397, loss = 0.02753026\n",
      "Iteration 398, loss = 0.02587705\n",
      "Iteration 399, loss = 0.02631491\n",
      "Iteration 400, loss = 0.02575248\n",
      "Iteration 401, loss = 0.02559735\n",
      "Iteration 402, loss = 0.02556326\n",
      "Iteration 403, loss = 0.02562570\n",
      "Iteration 404, loss = 0.02538148\n",
      "Iteration 405, loss = 0.02671800\n",
      "Iteration 406, loss = 0.02559932\n",
      "Iteration 407, loss = 0.02480227\n",
      "Iteration 408, loss = 0.02500061\n",
      "Iteration 409, loss = 0.02574897\n",
      "Iteration 410, loss = 0.02499970\n",
      "Iteration 411, loss = 0.02473011\n",
      "Iteration 412, loss = 0.02445902\n",
      "Iteration 413, loss = 0.02592434\n",
      "Iteration 414, loss = 0.02541897\n",
      "Iteration 415, loss = 0.02399133\n",
      "Iteration 416, loss = 0.02408240\n",
      "Iteration 417, loss = 0.02460618\n",
      "Iteration 418, loss = 0.02496172\n",
      "Iteration 419, loss = 0.02424044\n",
      "Iteration 420, loss = 0.02402103\n",
      "Iteration 421, loss = 0.02452787\n",
      "Iteration 422, loss = 0.02470591\n",
      "Iteration 423, loss = 0.02483878\n",
      "Iteration 424, loss = 0.02393691\n",
      "Iteration 425, loss = 0.02381193\n",
      "Iteration 426, loss = 0.02370003\n",
      "Iteration 427, loss = 0.02343280\n",
      "Iteration 428, loss = 0.02435672\n",
      "Iteration 429, loss = 0.02366267\n",
      "Iteration 430, loss = 0.02265373\n",
      "Iteration 431, loss = 0.02396556\n",
      "Iteration 432, loss = 0.02471360\n",
      "Iteration 433, loss = 0.02287417\n",
      "Iteration 434, loss = 0.02317674\n",
      "Iteration 435, loss = 0.02331248\n",
      "Iteration 436, loss = 0.02423621\n",
      "Iteration 437, loss = 0.02266505\n",
      "Iteration 438, loss = 0.02289865\n",
      "Iteration 439, loss = 0.02308773\n",
      "Iteration 440, loss = 0.02270975\n",
      "Iteration 441, loss = 0.02379144\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66635800\n",
      "Iteration 2, loss = 0.65045319\n",
      "Iteration 3, loss = 0.64939106\n",
      "Iteration 4, loss = 0.64882322\n",
      "Iteration 5, loss = 0.64889952\n",
      "Iteration 6, loss = 0.64864652\n",
      "Iteration 7, loss = 0.64857581\n",
      "Iteration 8, loss = 0.64804686\n",
      "Iteration 9, loss = 0.64779999\n",
      "Iteration 10, loss = 0.64769648\n",
      "Iteration 11, loss = 0.64735706\n",
      "Iteration 12, loss = 0.64748583\n",
      "Iteration 13, loss = 0.64703502\n",
      "Iteration 14, loss = 0.64678716\n",
      "Iteration 15, loss = 0.64669094\n",
      "Iteration 16, loss = 0.64615196\n",
      "Iteration 17, loss = 0.64587459\n",
      "Iteration 18, loss = 0.64537822\n",
      "Iteration 19, loss = 0.64488860\n",
      "Iteration 20, loss = 0.64437187\n",
      "Iteration 21, loss = 0.64370970\n",
      "Iteration 22, loss = 0.64305009\n",
      "Iteration 23, loss = 0.64227175\n",
      "Iteration 24, loss = 0.64173862\n",
      "Iteration 25, loss = 0.64110060\n",
      "Iteration 26, loss = 0.64041354\n",
      "Iteration 27, loss = 0.63959056\n",
      "Iteration 28, loss = 0.63864338\n",
      "Iteration 29, loss = 0.63840452\n",
      "Iteration 30, loss = 0.63760024\n",
      "Iteration 31, loss = 0.63668618\n",
      "Iteration 32, loss = 0.63621799\n",
      "Iteration 33, loss = 0.63526341\n",
      "Iteration 34, loss = 0.63442570\n",
      "Iteration 35, loss = 0.63367590\n",
      "Iteration 36, loss = 0.63303865\n",
      "Iteration 37, loss = 0.63208186\n",
      "Iteration 38, loss = 0.63100751\n",
      "Iteration 39, loss = 0.62981574\n",
      "Iteration 40, loss = 0.62977912\n",
      "Iteration 41, loss = 0.62853793\n",
      "Iteration 42, loss = 0.62776481\n",
      "Iteration 43, loss = 0.62590084\n",
      "Iteration 44, loss = 0.62581960\n",
      "Iteration 45, loss = 0.62421020\n",
      "Iteration 46, loss = 0.62323651\n",
      "Iteration 47, loss = 0.62243422\n",
      "Iteration 48, loss = 0.62107546\n",
      "Iteration 49, loss = 0.62001508\n",
      "Iteration 50, loss = 0.61895948\n",
      "Iteration 51, loss = 0.61780592\n",
      "Iteration 52, loss = 0.61693465\n",
      "Iteration 53, loss = 0.61625521\n",
      "Iteration 54, loss = 0.61509969\n",
      "Iteration 55, loss = 0.61384625\n",
      "Iteration 56, loss = 0.61305148\n",
      "Iteration 57, loss = 0.61173416\n",
      "Iteration 58, loss = 0.61061994\n",
      "Iteration 59, loss = 0.60933976\n",
      "Iteration 60, loss = 0.60785925\n",
      "Iteration 61, loss = 0.60682196\n",
      "Iteration 62, loss = 0.60535290\n",
      "Iteration 63, loss = 0.60432862\n",
      "Iteration 64, loss = 0.60298006\n",
      "Iteration 65, loss = 0.60174710\n",
      "Iteration 66, loss = 0.59997069\n",
      "Iteration 67, loss = 0.59900579\n",
      "Iteration 68, loss = 0.59796591\n",
      "Iteration 69, loss = 0.59593102\n",
      "Iteration 70, loss = 0.59484851\n",
      "Iteration 71, loss = 0.59315922\n",
      "Iteration 72, loss = 0.59221702\n",
      "Iteration 73, loss = 0.59056275\n",
      "Iteration 74, loss = 0.58989197\n",
      "Iteration 75, loss = 0.58733795\n",
      "Iteration 76, loss = 0.58633467\n",
      "Iteration 77, loss = 0.58470845\n",
      "Iteration 78, loss = 0.58334199\n",
      "Iteration 79, loss = 0.58176878\n",
      "Iteration 80, loss = 0.57996276\n",
      "Iteration 81, loss = 0.57911742\n",
      "Iteration 82, loss = 0.57772125\n",
      "Iteration 83, loss = 0.57668887\n",
      "Iteration 84, loss = 0.57459705\n",
      "Iteration 85, loss = 0.57335315\n",
      "Iteration 86, loss = 0.57205489\n",
      "Iteration 87, loss = 0.57107378\n",
      "Iteration 88, loss = 0.56900835\n",
      "Iteration 89, loss = 0.56725622\n",
      "Iteration 90, loss = 0.56661802\n",
      "Iteration 91, loss = 0.56530005\n",
      "Iteration 92, loss = 0.56295965\n",
      "Iteration 93, loss = 0.56191637\n",
      "Iteration 94, loss = 0.56086158\n",
      "Iteration 95, loss = 0.55835039\n",
      "Iteration 96, loss = 0.55713079\n",
      "Iteration 97, loss = 0.55601891\n",
      "Iteration 98, loss = 0.55541292\n",
      "Iteration 99, loss = 0.55341222\n",
      "Iteration 100, loss = 0.55149387\n",
      "Iteration 101, loss = 0.55036227\n",
      "Iteration 102, loss = 0.54930509\n",
      "Iteration 103, loss = 0.54774345\n",
      "Iteration 104, loss = 0.54674727\n",
      "Iteration 105, loss = 0.54532810\n",
      "Iteration 106, loss = 0.54476773\n",
      "Iteration 107, loss = 0.54231414\n",
      "Iteration 108, loss = 0.54150069\n",
      "Iteration 109, loss = 0.53930625\n",
      "Iteration 110, loss = 0.53803637\n",
      "Iteration 111, loss = 0.53741672\n",
      "Iteration 112, loss = 0.53608664\n",
      "Iteration 113, loss = 0.53456715\n",
      "Iteration 114, loss = 0.53385698\n",
      "Iteration 115, loss = 0.53282809\n",
      "Iteration 116, loss = 0.53129296\n",
      "Iteration 117, loss = 0.53024784\n",
      "Iteration 118, loss = 0.52874323\n",
      "Iteration 119, loss = 0.52815356\n",
      "Iteration 120, loss = 0.52716269\n",
      "Iteration 121, loss = 0.52548869\n",
      "Iteration 122, loss = 0.52450030\n",
      "Iteration 123, loss = 0.52331231\n",
      "Iteration 124, loss = 0.52283722\n",
      "Iteration 125, loss = 0.52065063\n",
      "Iteration 126, loss = 0.52052887\n",
      "Iteration 127, loss = 0.51931173\n",
      "Iteration 128, loss = 0.51781121\n",
      "Iteration 129, loss = 0.51685699\n",
      "Iteration 130, loss = 0.51623914\n",
      "Iteration 131, loss = 0.51456884\n",
      "Iteration 132, loss = 0.51328670\n",
      "Iteration 133, loss = 0.51425703\n",
      "Iteration 134, loss = 0.51142631\n",
      "Iteration 135, loss = 0.51083396\n",
      "Iteration 136, loss = 0.50924607\n",
      "Iteration 137, loss = 0.50936381\n",
      "Iteration 138, loss = 0.50950502\n",
      "Iteration 139, loss = 0.50690601\n",
      "Iteration 140, loss = 0.50649083\n",
      "Iteration 141, loss = 0.50491849\n",
      "Iteration 142, loss = 0.50396616\n",
      "Iteration 143, loss = 0.50365471\n",
      "Iteration 144, loss = 0.50220306\n",
      "Iteration 145, loss = 0.50182295\n",
      "Iteration 146, loss = 0.50045892\n",
      "Iteration 147, loss = 0.50033778\n",
      "Iteration 148, loss = 0.49948306\n",
      "Iteration 149, loss = 0.49860876\n",
      "Iteration 150, loss = 0.49655261\n",
      "Iteration 151, loss = 0.49651750\n",
      "Iteration 152, loss = 0.49546819\n",
      "Iteration 153, loss = 0.49437028\n",
      "Iteration 154, loss = 0.49444974\n",
      "Iteration 155, loss = 0.49342167\n",
      "Iteration 156, loss = 0.49238304\n",
      "Iteration 157, loss = 0.49103358\n",
      "Iteration 158, loss = 0.49075065\n",
      "Iteration 159, loss = 0.49015343\n",
      "Iteration 160, loss = 0.48899090\n",
      "Iteration 161, loss = 0.48971271\n",
      "Iteration 162, loss = 0.48738897\n",
      "Iteration 163, loss = 0.48621947\n",
      "Iteration 164, loss = 0.48617547\n",
      "Iteration 165, loss = 0.48493363\n",
      "Iteration 166, loss = 0.48420957\n",
      "Iteration 167, loss = 0.48404017\n",
      "Iteration 168, loss = 0.48354224\n",
      "Iteration 169, loss = 0.48292980\n",
      "Iteration 170, loss = 0.48215134\n",
      "Iteration 171, loss = 0.48070528\n",
      "Iteration 172, loss = 0.48085142\n",
      "Iteration 173, loss = 0.47971330\n",
      "Iteration 174, loss = 0.47906728\n",
      "Iteration 175, loss = 0.47838159\n",
      "Iteration 176, loss = 0.47864935\n",
      "Iteration 177, loss = 0.47711010\n",
      "Iteration 178, loss = 0.47638652\n",
      "Iteration 179, loss = 0.47532074\n",
      "Iteration 180, loss = 0.47564803\n",
      "Iteration 181, loss = 0.47406650\n",
      "Iteration 182, loss = 0.47360434\n",
      "Iteration 183, loss = 0.47270717\n",
      "Iteration 184, loss = 0.47229912\n",
      "Iteration 185, loss = 0.47136673\n",
      "Iteration 186, loss = 0.47180287\n",
      "Iteration 187, loss = 0.47025321\n",
      "Iteration 188, loss = 0.46977482\n",
      "Iteration 189, loss = 0.46907153\n",
      "Iteration 190, loss = 0.46908938\n",
      "Iteration 191, loss = 0.46877343\n",
      "Iteration 192, loss = 0.46784516\n",
      "Iteration 193, loss = 0.46641271\n",
      "Iteration 194, loss = 0.46601974\n",
      "Iteration 195, loss = 0.46568129\n",
      "Iteration 196, loss = 0.46496042\n",
      "Iteration 197, loss = 0.46539385\n",
      "Iteration 198, loss = 0.46314541\n",
      "Iteration 199, loss = 0.46359803\n",
      "Iteration 200, loss = 0.46295260\n",
      "Iteration 201, loss = 0.46227291\n",
      "Iteration 202, loss = 0.46194513\n",
      "Iteration 203, loss = 0.46063177\n",
      "Iteration 204, loss = 0.46058175\n",
      "Iteration 205, loss = 0.45974812\n",
      "Iteration 206, loss = 0.45903057\n",
      "Iteration 207, loss = 0.45967058\n",
      "Iteration 208, loss = 0.45801755\n",
      "Iteration 209, loss = 0.45862369\n",
      "Iteration 210, loss = 0.45699717\n",
      "Iteration 211, loss = 0.45743815\n",
      "Iteration 212, loss = 0.45588403\n",
      "Iteration 213, loss = 0.45550736\n",
      "Iteration 214, loss = 0.45481575\n",
      "Iteration 215, loss = 0.45442246\n",
      "Iteration 216, loss = 0.45343537\n",
      "Iteration 217, loss = 0.45511945\n",
      "Iteration 218, loss = 0.45296911\n",
      "Iteration 219, loss = 0.45306541\n",
      "Iteration 220, loss = 0.45134813\n",
      "Iteration 221, loss = 0.45102619\n",
      "Iteration 222, loss = 0.45190476\n",
      "Iteration 223, loss = 0.45088168\n",
      "Iteration 224, loss = 0.44981209\n",
      "Iteration 225, loss = 0.44915082\n",
      "Iteration 226, loss = 0.44888440\n",
      "Iteration 227, loss = 0.44834560\n",
      "Iteration 228, loss = 0.44815404\n",
      "Iteration 229, loss = 0.44696139\n",
      "Iteration 230, loss = 0.44673516\n",
      "Iteration 231, loss = 0.44655840\n",
      "Iteration 232, loss = 0.44499030\n",
      "Iteration 233, loss = 0.44658806\n",
      "Iteration 234, loss = 0.44429793\n",
      "Iteration 235, loss = 0.44532331\n",
      "Iteration 236, loss = 0.44416211\n",
      "Iteration 237, loss = 0.44350025\n",
      "Iteration 238, loss = 0.44241144\n",
      "Iteration 239, loss = 0.44198327\n",
      "Iteration 240, loss = 0.44244898\n",
      "Iteration 241, loss = 0.44150297\n",
      "Iteration 242, loss = 0.44086681\n",
      "Iteration 243, loss = 0.44012725\n",
      "Iteration 244, loss = 0.44059986\n",
      "Iteration 245, loss = 0.44050622\n",
      "Iteration 246, loss = 0.43899671\n",
      "Iteration 247, loss = 0.43840013\n",
      "Iteration 248, loss = 0.43817207\n",
      "Iteration 249, loss = 0.43794644\n",
      "Iteration 250, loss = 0.43657475\n",
      "Iteration 251, loss = 0.43619146\n",
      "Iteration 252, loss = 0.43587356\n",
      "Iteration 253, loss = 0.43599459\n",
      "Iteration 254, loss = 0.43570430\n",
      "Iteration 255, loss = 0.43574953\n",
      "Iteration 256, loss = 0.43492336\n",
      "Iteration 257, loss = 0.43419384\n",
      "Iteration 258, loss = 0.43330429\n",
      "Iteration 259, loss = 0.43273661\n",
      "Iteration 260, loss = 0.43291189\n",
      "Iteration 261, loss = 0.43254343\n",
      "Iteration 262, loss = 0.43141769\n",
      "Iteration 263, loss = 0.43081624\n",
      "Iteration 264, loss = 0.43088704\n",
      "Iteration 265, loss = 0.43010166\n",
      "Iteration 266, loss = 0.42956166\n",
      "Iteration 267, loss = 0.42890030\n",
      "Iteration 268, loss = 0.42911195\n",
      "Iteration 269, loss = 0.42866780\n",
      "Iteration 270, loss = 0.42818348\n",
      "Iteration 271, loss = 0.42769121\n",
      "Iteration 272, loss = 0.42615806\n",
      "Iteration 273, loss = 0.42674489\n",
      "Iteration 274, loss = 0.42673144\n",
      "Iteration 275, loss = 0.42588086\n",
      "Iteration 276, loss = 0.42530191\n",
      "Iteration 277, loss = 0.42570174\n",
      "Iteration 278, loss = 0.42356241\n",
      "Iteration 279, loss = 0.42421758\n",
      "Iteration 280, loss = 0.42350047\n",
      "Iteration 281, loss = 0.42286134\n",
      "Iteration 282, loss = 0.42187059\n",
      "Iteration 283, loss = 0.42166391\n",
      "Iteration 284, loss = 0.42165543\n",
      "Iteration 285, loss = 0.42209983\n",
      "Iteration 286, loss = 0.42035449\n",
      "Iteration 287, loss = 0.42084997\n",
      "Iteration 288, loss = 0.42026601\n",
      "Iteration 289, loss = 0.41943900\n",
      "Iteration 290, loss = 0.41877329\n",
      "Iteration 291, loss = 0.41889035\n",
      "Iteration 292, loss = 0.41853823\n",
      "Iteration 293, loss = 0.41701904\n",
      "Iteration 294, loss = 0.41671884\n",
      "Iteration 295, loss = 0.41679202\n",
      "Iteration 296, loss = 0.41655068\n",
      "Iteration 297, loss = 0.41542508\n",
      "Iteration 298, loss = 0.41534404\n",
      "Iteration 299, loss = 0.41502310\n",
      "Iteration 300, loss = 0.41414638\n",
      "Iteration 301, loss = 0.41395400\n",
      "Iteration 302, loss = 0.41306096\n",
      "Iteration 303, loss = 0.41318799\n",
      "Iteration 304, loss = 0.41211434\n",
      "Iteration 305, loss = 0.41141664\n",
      "Iteration 306, loss = 0.41120534\n",
      "Iteration 307, loss = 0.41090726\n",
      "Iteration 308, loss = 0.41028058\n",
      "Iteration 309, loss = 0.41007159\n",
      "Iteration 310, loss = 0.40878545\n",
      "Iteration 311, loss = 0.40843606\n",
      "Iteration 312, loss = 0.40859261\n",
      "Iteration 313, loss = 0.40815847\n",
      "Iteration 314, loss = 0.40720074\n",
      "Iteration 315, loss = 0.40640469\n",
      "Iteration 316, loss = 0.40655061\n",
      "Iteration 317, loss = 0.40619803\n",
      "Iteration 318, loss = 0.40573674\n",
      "Iteration 319, loss = 0.40520002\n",
      "Iteration 320, loss = 0.40479699\n",
      "Iteration 321, loss = 0.40418701\n",
      "Iteration 322, loss = 0.40417996\n",
      "Iteration 323, loss = 0.40343428\n",
      "Iteration 324, loss = 0.40273011\n",
      "Iteration 325, loss = 0.40334387\n",
      "Iteration 326, loss = 0.40121788\n",
      "Iteration 327, loss = 0.40093391\n",
      "Iteration 328, loss = 0.40061303\n",
      "Iteration 329, loss = 0.40002685\n",
      "Iteration 330, loss = 0.40001864\n",
      "Iteration 331, loss = 0.39962866\n",
      "Iteration 332, loss = 0.39911736\n",
      "Iteration 333, loss = 0.39849641\n",
      "Iteration 334, loss = 0.39845938\n",
      "Iteration 335, loss = 0.39731297\n",
      "Iteration 336, loss = 0.39608097\n",
      "Iteration 337, loss = 0.39631824\n",
      "Iteration 338, loss = 0.39633163\n",
      "Iteration 339, loss = 0.39566774\n",
      "Iteration 340, loss = 0.39502606\n",
      "Iteration 341, loss = 0.39381833\n",
      "Iteration 342, loss = 0.39259507\n",
      "Iteration 343, loss = 0.39350104\n",
      "Iteration 344, loss = 0.39300012\n",
      "Iteration 345, loss = 0.39264748\n",
      "Iteration 346, loss = 0.39193799\n",
      "Iteration 347, loss = 0.39116216\n",
      "Iteration 348, loss = 0.39118543\n",
      "Iteration 349, loss = 0.39088019\n",
      "Iteration 350, loss = 0.39031770\n",
      "Iteration 351, loss = 0.38985521\n",
      "Iteration 352, loss = 0.38929765\n",
      "Iteration 353, loss = 0.38918510\n",
      "Iteration 354, loss = 0.38827059\n",
      "Iteration 355, loss = 0.38717732\n",
      "Iteration 356, loss = 0.38694463\n",
      "Iteration 357, loss = 0.38654374\n",
      "Iteration 358, loss = 0.38660545\n",
      "Iteration 359, loss = 0.38517607\n",
      "Iteration 360, loss = 0.38559327\n",
      "Iteration 361, loss = 0.38468580\n",
      "Iteration 362, loss = 0.38431437\n",
      "Iteration 363, loss = 0.38366572\n",
      "Iteration 364, loss = 0.38315272\n",
      "Iteration 365, loss = 0.38246676\n",
      "Iteration 366, loss = 0.38185049\n",
      "Iteration 367, loss = 0.38131916\n",
      "Iteration 368, loss = 0.38090669\n",
      "Iteration 369, loss = 0.38055419\n",
      "Iteration 370, loss = 0.38045782\n",
      "Iteration 371, loss = 0.37919129\n",
      "Iteration 372, loss = 0.37911404\n",
      "Iteration 373, loss = 0.37790660\n",
      "Iteration 374, loss = 0.37819142\n",
      "Iteration 375, loss = 0.37794696\n",
      "Iteration 376, loss = 0.37676716\n",
      "Iteration 377, loss = 0.37707491\n",
      "Iteration 378, loss = 0.37558608\n",
      "Iteration 379, loss = 0.37480330\n",
      "Iteration 380, loss = 0.37413176\n",
      "Iteration 381, loss = 0.37424245\n",
      "Iteration 382, loss = 0.37397697\n",
      "Iteration 383, loss = 0.37304531\n",
      "Iteration 384, loss = 0.37252484\n",
      "Iteration 385, loss = 0.37189233\n",
      "Iteration 386, loss = 0.37160514\n",
      "Iteration 387, loss = 0.37115699\n",
      "Iteration 388, loss = 0.36984208\n",
      "Iteration 389, loss = 0.36938923\n",
      "Iteration 390, loss = 0.36894011\n",
      "Iteration 391, loss = 0.36838127\n",
      "Iteration 392, loss = 0.36866139\n",
      "Iteration 393, loss = 0.36748037\n",
      "Iteration 394, loss = 0.36797590\n",
      "Iteration 395, loss = 0.36660400\n",
      "Iteration 396, loss = 0.36644701\n",
      "Iteration 397, loss = 0.36509559\n",
      "Iteration 398, loss = 0.36497339\n",
      "Iteration 399, loss = 0.36438143\n",
      "Iteration 400, loss = 0.36375887\n",
      "Iteration 401, loss = 0.36403666\n",
      "Iteration 402, loss = 0.36289118\n",
      "Iteration 403, loss = 0.36233736\n",
      "Iteration 404, loss = 0.36169577\n",
      "Iteration 405, loss = 0.36066811\n",
      "Iteration 406, loss = 0.36108969\n",
      "Iteration 407, loss = 0.36040135\n",
      "Iteration 408, loss = 0.35926185\n",
      "Iteration 409, loss = 0.35850643\n",
      "Iteration 410, loss = 0.35868386\n",
      "Iteration 411, loss = 0.35769246\n",
      "Iteration 412, loss = 0.35745973\n",
      "Iteration 413, loss = 0.35651502\n",
      "Iteration 414, loss = 0.35622397\n",
      "Iteration 415, loss = 0.35534923\n",
      "Iteration 416, loss = 0.35451611\n",
      "Iteration 417, loss = 0.35438996\n",
      "Iteration 418, loss = 0.35429842\n",
      "Iteration 419, loss = 0.35406205\n",
      "Iteration 420, loss = 0.35262708\n",
      "Iteration 421, loss = 0.35140859\n",
      "Iteration 422, loss = 0.35143955\n",
      "Iteration 423, loss = 0.35118185\n",
      "Iteration 424, loss = 0.35038906\n",
      "Iteration 425, loss = 0.34959882\n",
      "Iteration 426, loss = 0.34926176\n",
      "Iteration 427, loss = 0.34914572\n",
      "Iteration 428, loss = 0.34772562\n",
      "Iteration 429, loss = 0.34818397\n",
      "Iteration 430, loss = 0.34659341\n",
      "Iteration 431, loss = 0.34713855\n",
      "Iteration 432, loss = 0.34616997\n",
      "Iteration 433, loss = 0.34556214\n",
      "Iteration 434, loss = 0.34464324\n",
      "Iteration 435, loss = 0.34398984\n",
      "Iteration 436, loss = 0.34383533\n",
      "Iteration 437, loss = 0.34330348\n",
      "Iteration 438, loss = 0.34277438\n",
      "Iteration 439, loss = 0.34183258\n",
      "Iteration 440, loss = 0.34109786\n",
      "Iteration 441, loss = 0.34084035\n",
      "Iteration 442, loss = 0.34058225\n",
      "Iteration 443, loss = 0.33980765\n",
      "Iteration 444, loss = 0.33920818\n",
      "Iteration 445, loss = 0.33868712\n",
      "Iteration 446, loss = 0.33876830\n",
      "Iteration 447, loss = 0.33800058\n",
      "Iteration 448, loss = 0.33637371\n",
      "Iteration 449, loss = 0.33622848\n",
      "Iteration 450, loss = 0.33594823\n",
      "Iteration 451, loss = 0.33503249\n",
      "Iteration 452, loss = 0.33475333\n",
      "Iteration 453, loss = 0.33348414\n",
      "Iteration 454, loss = 0.33349412\n",
      "Iteration 455, loss = 0.33256539\n",
      "Iteration 456, loss = 0.33269956\n",
      "Iteration 457, loss = 0.33207212\n",
      "Iteration 458, loss = 0.33023940\n",
      "Iteration 459, loss = 0.33053509\n",
      "Iteration 460, loss = 0.32980586\n",
      "Iteration 461, loss = 0.33001096\n",
      "Iteration 462, loss = 0.32958094\n",
      "Iteration 463, loss = 0.32860275\n",
      "Iteration 464, loss = 0.32833404\n",
      "Iteration 465, loss = 0.32820197\n",
      "Iteration 466, loss = 0.32658497\n",
      "Iteration 467, loss = 0.32639585\n",
      "Iteration 468, loss = 0.32580267\n",
      "Iteration 469, loss = 0.32506526\n",
      "Iteration 470, loss = 0.32410859\n",
      "Iteration 471, loss = 0.32397579\n",
      "Iteration 472, loss = 0.32388359\n",
      "Iteration 473, loss = 0.32319385\n",
      "Iteration 474, loss = 0.32204801\n",
      "Iteration 475, loss = 0.32168307\n",
      "Iteration 476, loss = 0.32089336\n",
      "Iteration 477, loss = 0.32092564\n",
      "Iteration 478, loss = 0.31989777\n",
      "Iteration 479, loss = 0.31933774\n",
      "Iteration 480, loss = 0.31956691\n",
      "Iteration 481, loss = 0.31833581\n",
      "Iteration 482, loss = 0.31799000\n",
      "Iteration 483, loss = 0.31739806\n",
      "Iteration 484, loss = 0.31707807\n",
      "Iteration 485, loss = 0.31610567\n",
      "Iteration 486, loss = 0.31586182\n",
      "Iteration 487, loss = 0.31538299\n",
      "Iteration 488, loss = 0.31424514\n",
      "Iteration 489, loss = 0.31435925\n",
      "Iteration 490, loss = 0.31428819\n",
      "Iteration 491, loss = 0.31291709\n",
      "Iteration 492, loss = 0.31261611\n",
      "Iteration 493, loss = 0.31229626\n",
      "Iteration 494, loss = 0.31127276\n",
      "Iteration 495, loss = 0.31130332\n",
      "Iteration 496, loss = 0.31032131\n",
      "Iteration 497, loss = 0.31010618\n",
      "Iteration 498, loss = 0.30938932\n",
      "Iteration 499, loss = 0.30880748\n",
      "Iteration 500, loss = 0.30798378\n",
      "Iteration 501, loss = 0.30802871\n",
      "Iteration 502, loss = 0.30699863\n",
      "Iteration 503, loss = 0.30658395\n",
      "Iteration 504, loss = 0.30610511\n",
      "Iteration 505, loss = 0.30545345\n",
      "Iteration 506, loss = 0.30508595\n",
      "Iteration 507, loss = 0.30421098\n",
      "Iteration 508, loss = 0.30375998\n",
      "Iteration 509, loss = 0.30293621\n",
      "Iteration 510, loss = 0.30310452\n",
      "Iteration 511, loss = 0.30252084\n",
      "Iteration 512, loss = 0.30208056\n",
      "Iteration 513, loss = 0.30128503\n",
      "Iteration 514, loss = 0.30066491\n",
      "Iteration 515, loss = 0.30092182\n",
      "Iteration 516, loss = 0.29965861\n",
      "Iteration 517, loss = 0.29885009\n",
      "Iteration 518, loss = 0.29882077\n",
      "Iteration 519, loss = 0.29829362\n",
      "Iteration 520, loss = 0.29789211\n",
      "Iteration 521, loss = 0.29699896\n",
      "Iteration 522, loss = 0.29609479\n",
      "Iteration 523, loss = 0.29578358\n",
      "Iteration 524, loss = 0.29591311\n",
      "Iteration 525, loss = 0.29424793\n",
      "Iteration 526, loss = 0.29575874\n",
      "Iteration 527, loss = 0.29402745\n",
      "Iteration 528, loss = 0.29336597\n",
      "Iteration 529, loss = 0.29258240\n",
      "Iteration 530, loss = 0.29288159\n",
      "Iteration 531, loss = 0.29203964\n",
      "Iteration 532, loss = 0.29119642\n",
      "Iteration 533, loss = 0.29126480\n",
      "Iteration 534, loss = 0.29050861\n",
      "Iteration 535, loss = 0.28949419\n",
      "Iteration 536, loss = 0.28906284\n",
      "Iteration 537, loss = 0.28904641\n",
      "Iteration 538, loss = 0.28767155\n",
      "Iteration 539, loss = 0.28727254\n",
      "Iteration 540, loss = 0.28774883\n",
      "Iteration 541, loss = 0.28652385\n",
      "Iteration 542, loss = 0.28631702\n",
      "Iteration 543, loss = 0.28532464\n",
      "Iteration 544, loss = 0.28540544\n",
      "Iteration 545, loss = 0.28532563\n",
      "Iteration 546, loss = 0.28397933\n",
      "Iteration 547, loss = 0.28338182\n",
      "Iteration 548, loss = 0.28322773\n",
      "Iteration 549, loss = 0.28358600\n",
      "Iteration 550, loss = 0.28195658\n",
      "Iteration 551, loss = 0.28244441\n",
      "Iteration 552, loss = 0.28083581\n",
      "Iteration 553, loss = 0.28077273\n",
      "Iteration 554, loss = 0.28021924\n",
      "Iteration 555, loss = 0.27962181\n",
      "Iteration 556, loss = 0.27977471\n",
      "Iteration 557, loss = 0.27880088\n",
      "Iteration 558, loss = 0.27821180\n",
      "Iteration 559, loss = 0.27787984\n",
      "Iteration 560, loss = 0.27798518\n",
      "Iteration 561, loss = 0.27654628\n",
      "Iteration 562, loss = 0.27622074\n",
      "Iteration 563, loss = 0.27557530\n",
      "Iteration 564, loss = 0.27537502\n",
      "Iteration 565, loss = 0.27469983\n",
      "Iteration 566, loss = 0.27457178\n",
      "Iteration 567, loss = 0.27446778\n",
      "Iteration 568, loss = 0.27316757\n",
      "Iteration 569, loss = 0.27285125\n",
      "Iteration 570, loss = 0.27215424\n",
      "Iteration 571, loss = 0.27234128\n",
      "Iteration 572, loss = 0.27213476\n",
      "Iteration 573, loss = 0.27130955\n",
      "Iteration 574, loss = 0.27039533\n",
      "Iteration 575, loss = 0.26962510\n",
      "Iteration 576, loss = 0.26936702\n",
      "Iteration 577, loss = 0.26851144\n",
      "Iteration 578, loss = 0.26863864\n",
      "Iteration 579, loss = 0.26765644\n",
      "Iteration 580, loss = 0.26772910\n",
      "Iteration 581, loss = 0.26671667\n",
      "Iteration 582, loss = 0.26715261\n",
      "Iteration 583, loss = 0.26588535\n",
      "Iteration 584, loss = 0.26651778\n",
      "Iteration 585, loss = 0.26501823\n",
      "Iteration 586, loss = 0.26463098\n",
      "Iteration 587, loss = 0.26431399\n",
      "Iteration 588, loss = 0.26391108\n",
      "Iteration 589, loss = 0.26271933\n",
      "Iteration 590, loss = 0.26296904\n",
      "Iteration 591, loss = 0.26253230\n",
      "Iteration 592, loss = 0.26182840\n",
      "Iteration 593, loss = 0.26143066\n",
      "Iteration 594, loss = 0.26050878\n",
      "Iteration 595, loss = 0.26041499\n",
      "Iteration 596, loss = 0.25996491\n",
      "Iteration 597, loss = 0.25991924\n",
      "Iteration 598, loss = 0.25891740\n",
      "Iteration 599, loss = 0.25842156\n",
      "Iteration 600, loss = 0.25848690\n",
      "Iteration 601, loss = 0.25822516\n",
      "Iteration 602, loss = 0.25721461\n",
      "Iteration 603, loss = 0.25697041\n",
      "Iteration 604, loss = 0.25677722\n",
      "Iteration 605, loss = 0.25590463\n",
      "Iteration 606, loss = 0.25607898\n",
      "Iteration 607, loss = 0.25522716\n",
      "Iteration 608, loss = 0.25480955\n",
      "Iteration 609, loss = 0.25406641\n",
      "Iteration 610, loss = 0.25441426\n",
      "Iteration 611, loss = 0.25351409\n",
      "Iteration 612, loss = 0.25368992\n",
      "Iteration 613, loss = 0.25201534\n",
      "Iteration 614, loss = 0.25251832\n",
      "Iteration 615, loss = 0.25166029\n",
      "Iteration 616, loss = 0.25087411\n",
      "Iteration 617, loss = 0.25085117\n",
      "Iteration 618, loss = 0.25004832\n",
      "Iteration 619, loss = 0.25038559\n",
      "Iteration 620, loss = 0.24921404\n",
      "Iteration 621, loss = 0.24854920\n",
      "Iteration 622, loss = 0.24818945\n",
      "Iteration 623, loss = 0.24820107\n",
      "Iteration 624, loss = 0.24822300\n",
      "Iteration 625, loss = 0.24697289\n",
      "Iteration 626, loss = 0.24681608\n",
      "Iteration 627, loss = 0.24668469\n",
      "Iteration 628, loss = 0.24581379\n",
      "Iteration 629, loss = 0.24602957\n",
      "Iteration 630, loss = 0.24466591\n",
      "Iteration 631, loss = 0.24420164\n",
      "Iteration 632, loss = 0.24435245\n",
      "Iteration 633, loss = 0.24374713\n",
      "Iteration 634, loss = 0.24323469\n",
      "Iteration 635, loss = 0.24278575\n",
      "Iteration 636, loss = 0.24290561\n",
      "Iteration 637, loss = 0.24205007\n",
      "Iteration 638, loss = 0.24161293\n",
      "Iteration 639, loss = 0.24108166\n",
      "Iteration 640, loss = 0.24083044\n",
      "Iteration 641, loss = 0.24038001\n",
      "Iteration 642, loss = 0.23994380\n",
      "Iteration 643, loss = 0.24035487\n",
      "Iteration 644, loss = 0.23911592\n",
      "Iteration 645, loss = 0.23877460\n",
      "Iteration 646, loss = 0.23846487\n",
      "Iteration 647, loss = 0.23778200\n",
      "Iteration 648, loss = 0.23776406\n",
      "Iteration 649, loss = 0.23711263\n",
      "Iteration 650, loss = 0.23700445\n",
      "Iteration 651, loss = 0.23611831\n",
      "Iteration 652, loss = 0.23564966\n",
      "Iteration 653, loss = 0.23528878\n",
      "Iteration 654, loss = 0.23503295\n",
      "Iteration 655, loss = 0.23468652\n",
      "Iteration 656, loss = 0.23416478\n",
      "Iteration 657, loss = 0.23361124\n",
      "Iteration 658, loss = 0.23376955\n",
      "Iteration 659, loss = 0.23328734\n",
      "Iteration 660, loss = 0.23264689\n",
      "Iteration 661, loss = 0.23226673\n",
      "Iteration 662, loss = 0.23168725\n",
      "Iteration 663, loss = 0.23150123\n",
      "Iteration 664, loss = 0.23124489\n",
      "Iteration 665, loss = 0.23086445\n",
      "Iteration 666, loss = 0.23010395\n",
      "Iteration 667, loss = 0.22964950\n",
      "Iteration 668, loss = 0.22949481\n",
      "Iteration 669, loss = 0.22924603\n",
      "Iteration 670, loss = 0.22841845\n",
      "Iteration 671, loss = 0.22830699\n",
      "Iteration 672, loss = 0.22741031\n",
      "Iteration 673, loss = 0.22740783\n",
      "Iteration 674, loss = 0.22701318\n",
      "Iteration 675, loss = 0.22653583\n",
      "Iteration 676, loss = 0.22661512\n",
      "Iteration 677, loss = 0.22586803\n",
      "Iteration 678, loss = 0.22518141\n",
      "Iteration 679, loss = 0.22536202\n",
      "Iteration 680, loss = 0.22456188\n",
      "Iteration 681, loss = 0.22429118\n",
      "Iteration 682, loss = 0.22403010\n",
      "Iteration 683, loss = 0.22308872\n",
      "Iteration 684, loss = 0.22324512\n",
      "Iteration 685, loss = 0.22269380\n",
      "Iteration 686, loss = 0.22227014\n",
      "Iteration 687, loss = 0.22194041\n",
      "Iteration 688, loss = 0.22181156\n",
      "Iteration 689, loss = 0.22157552\n",
      "Iteration 690, loss = 0.22106559\n",
      "Iteration 691, loss = 0.22117827\n",
      "Iteration 692, loss = 0.21983855\n",
      "Iteration 693, loss = 0.21975668\n",
      "Iteration 694, loss = 0.21924091\n",
      "Iteration 695, loss = 0.21927569\n",
      "Iteration 696, loss = 0.21856810\n",
      "Iteration 697, loss = 0.21820041\n",
      "Iteration 698, loss = 0.21811639\n",
      "Iteration 699, loss = 0.21755837\n",
      "Iteration 700, loss = 0.21702952\n",
      "Iteration 701, loss = 0.21662142\n",
      "Iteration 702, loss = 0.21634148\n",
      "Iteration 703, loss = 0.21592172\n",
      "Iteration 704, loss = 0.21556573\n",
      "Iteration 705, loss = 0.21497065\n",
      "Iteration 706, loss = 0.21556488\n",
      "Iteration 707, loss = 0.21482669\n",
      "Iteration 708, loss = 0.21401714\n",
      "Iteration 709, loss = 0.21415324\n",
      "Iteration 710, loss = 0.21376549\n",
      "Iteration 711, loss = 0.21344515\n",
      "Iteration 712, loss = 0.21317302\n",
      "Iteration 713, loss = 0.21232539\n",
      "Iteration 714, loss = 0.21246716\n",
      "Iteration 715, loss = 0.21173901\n",
      "Iteration 716, loss = 0.21141987\n",
      "Iteration 717, loss = 0.21189650\n",
      "Iteration 718, loss = 0.21106990\n",
      "Iteration 719, loss = 0.21031251\n",
      "Iteration 720, loss = 0.21060158\n",
      "Iteration 721, loss = 0.20986327\n",
      "Iteration 722, loss = 0.20925708\n",
      "Iteration 723, loss = 0.20979327\n",
      "Iteration 724, loss = 0.20842753\n",
      "Iteration 725, loss = 0.20858197\n",
      "Iteration 726, loss = 0.20785291\n",
      "Iteration 727, loss = 0.20803452\n",
      "Iteration 728, loss = 0.20725556\n",
      "Iteration 729, loss = 0.20677183\n",
      "Iteration 730, loss = 0.20740444\n",
      "Iteration 731, loss = 0.20629106\n",
      "Iteration 732, loss = 0.20656404\n",
      "Iteration 733, loss = 0.20603026\n",
      "Iteration 734, loss = 0.20582988\n",
      "Iteration 735, loss = 0.20518040\n",
      "Iteration 736, loss = 0.20524346\n",
      "Iteration 737, loss = 0.20470897\n",
      "Iteration 738, loss = 0.20413889\n",
      "Iteration 739, loss = 0.20401660\n",
      "Iteration 740, loss = 0.20335060\n",
      "Iteration 741, loss = 0.20317482\n",
      "Iteration 742, loss = 0.20298885\n",
      "Iteration 743, loss = 0.20274088\n",
      "Iteration 744, loss = 0.20284264\n",
      "Iteration 745, loss = 0.20233639\n",
      "Iteration 746, loss = 0.20172356\n",
      "Iteration 747, loss = 0.20148659\n",
      "Iteration 748, loss = 0.20164952\n",
      "Iteration 749, loss = 0.20113649\n",
      "Iteration 750, loss = 0.20042042\n",
      "Iteration 751, loss = 0.19982217\n",
      "Iteration 752, loss = 0.20048502\n",
      "Iteration 753, loss = 0.19991370\n",
      "Iteration 754, loss = 0.19962247\n",
      "Iteration 755, loss = 0.19890370\n",
      "Iteration 756, loss = 0.19880144\n",
      "Iteration 757, loss = 0.19871720\n",
      "Iteration 758, loss = 0.19849692\n",
      "Iteration 759, loss = 0.19762001\n",
      "Iteration 760, loss = 0.19728352\n",
      "Iteration 761, loss = 0.19739946\n",
      "Iteration 762, loss = 0.19752289\n",
      "Iteration 763, loss = 0.19689865\n",
      "Iteration 764, loss = 0.19618398\n",
      "Iteration 765, loss = 0.19594310\n",
      "Iteration 766, loss = 0.19575828\n",
      "Iteration 767, loss = 0.19580987\n",
      "Iteration 768, loss = 0.19536694\n",
      "Iteration 769, loss = 0.19496929\n",
      "Iteration 770, loss = 0.19487804\n",
      "Iteration 771, loss = 0.19475259\n",
      "Iteration 772, loss = 0.19404944\n",
      "Iteration 773, loss = 0.19375955\n",
      "Iteration 774, loss = 0.19319333\n",
      "Iteration 775, loss = 0.19388293\n",
      "Iteration 776, loss = 0.19316073\n",
      "Iteration 777, loss = 0.19300601\n",
      "Iteration 778, loss = 0.19234540\n",
      "Iteration 779, loss = 0.19198076\n",
      "Iteration 780, loss = 0.19203614\n",
      "Iteration 781, loss = 0.19169337\n",
      "Iteration 782, loss = 0.19129880\n",
      "Iteration 783, loss = 0.19174427\n",
      "Iteration 784, loss = 0.19094263\n",
      "Iteration 785, loss = 0.19035281\n",
      "Iteration 786, loss = 0.19098546\n",
      "Iteration 787, loss = 0.19018357\n",
      "Iteration 788, loss = 0.18967505\n",
      "Iteration 789, loss = 0.18961304\n",
      "Iteration 790, loss = 0.18946289\n",
      "Iteration 791, loss = 0.18910434\n",
      "Iteration 792, loss = 0.18888019\n",
      "Iteration 793, loss = 0.18875427\n",
      "Iteration 794, loss = 0.18821716\n",
      "Iteration 795, loss = 0.18816963\n",
      "Iteration 796, loss = 0.18778960\n",
      "Iteration 797, loss = 0.18705206\n",
      "Iteration 798, loss = 0.18746333\n",
      "Iteration 799, loss = 0.18712008\n",
      "Iteration 800, loss = 0.18674089\n",
      "Iteration 801, loss = 0.18641829\n",
      "Iteration 802, loss = 0.18651766\n",
      "Iteration 803, loss = 0.18576071\n",
      "Iteration 804, loss = 0.18542698\n",
      "Iteration 805, loss = 0.18526434\n",
      "Iteration 806, loss = 0.18518882\n",
      "Iteration 807, loss = 0.18564398\n",
      "Iteration 808, loss = 0.18443753\n",
      "Iteration 809, loss = 0.18455578\n",
      "Iteration 810, loss = 0.18414538\n",
      "Iteration 811, loss = 0.18384289\n",
      "Iteration 812, loss = 0.18388393\n",
      "Iteration 813, loss = 0.18329534\n",
      "Iteration 814, loss = 0.18348545\n",
      "Iteration 815, loss = 0.18279754\n",
      "Iteration 816, loss = 0.18278110\n",
      "Iteration 817, loss = 0.18254178\n",
      "Iteration 818, loss = 0.18189400\n",
      "Iteration 819, loss = 0.18184146\n",
      "Iteration 820, loss = 0.18172128\n",
      "Iteration 821, loss = 0.18159110\n",
      "Iteration 822, loss = 0.18176101\n",
      "Iteration 823, loss = 0.18112211\n",
      "Iteration 824, loss = 0.18058511\n",
      "Iteration 825, loss = 0.18060282\n",
      "Iteration 826, loss = 0.18040391\n",
      "Iteration 827, loss = 0.18065771\n",
      "Iteration 828, loss = 0.17980616\n",
      "Iteration 829, loss = 0.17953972\n",
      "Iteration 830, loss = 0.17975131\n",
      "Iteration 831, loss = 0.17893479\n",
      "Iteration 832, loss = 0.17906804\n",
      "Iteration 833, loss = 0.17863398\n",
      "Iteration 834, loss = 0.17847523\n",
      "Iteration 835, loss = 0.17807383\n",
      "Iteration 836, loss = 0.17790901\n",
      "Iteration 837, loss = 0.17771371\n",
      "Iteration 838, loss = 0.17781913\n",
      "Iteration 839, loss = 0.17752530\n",
      "Iteration 840, loss = 0.17708996\n",
      "Iteration 841, loss = 0.17690674\n",
      "Iteration 842, loss = 0.17694485\n",
      "Iteration 843, loss = 0.17659212\n",
      "Iteration 844, loss = 0.17658957\n",
      "Iteration 845, loss = 0.17601600\n",
      "Iteration 846, loss = 0.17538766\n",
      "Iteration 847, loss = 0.17585164\n",
      "Iteration 848, loss = 0.17521636\n",
      "Iteration 849, loss = 0.17515114\n",
      "Iteration 850, loss = 0.17518749\n",
      "Iteration 851, loss = 0.17477321\n",
      "Iteration 852, loss = 0.17442533\n",
      "Iteration 853, loss = 0.17391963\n",
      "Iteration 854, loss = 0.17391604\n",
      "Iteration 855, loss = 0.17382587\n",
      "Iteration 856, loss = 0.17330077\n",
      "Iteration 857, loss = 0.17288544\n",
      "Iteration 858, loss = 0.17353697\n",
      "Iteration 859, loss = 0.17287509\n",
      "Iteration 860, loss = 0.17264065\n",
      "Iteration 861, loss = 0.17269418\n",
      "Iteration 862, loss = 0.17215941\n",
      "Iteration 863, loss = 0.17194385\n",
      "Iteration 864, loss = 0.17189720\n",
      "Iteration 865, loss = 0.17144751\n",
      "Iteration 866, loss = 0.17175666\n",
      "Iteration 867, loss = 0.17138966\n",
      "Iteration 868, loss = 0.17146392\n",
      "Iteration 869, loss = 0.17055810\n",
      "Iteration 870, loss = 0.17035588\n",
      "Iteration 871, loss = 0.17041234\n",
      "Iteration 872, loss = 0.16983109\n",
      "Iteration 873, loss = 0.17019066\n",
      "Iteration 874, loss = 0.16936334\n",
      "Iteration 875, loss = 0.16931545\n",
      "Iteration 876, loss = 0.16912836\n",
      "Iteration 877, loss = 0.16886801\n",
      "Iteration 878, loss = 0.16895193\n",
      "Iteration 879, loss = 0.16878142\n",
      "Iteration 880, loss = 0.16852869\n",
      "Iteration 881, loss = 0.16827364\n",
      "Iteration 882, loss = 0.16791127\n",
      "Iteration 883, loss = 0.16799672\n",
      "Iteration 884, loss = 0.16775018\n",
      "Iteration 885, loss = 0.16692697\n",
      "Iteration 886, loss = 0.16704103\n",
      "Iteration 887, loss = 0.16732869\n",
      "Iteration 888, loss = 0.16686863\n",
      "Iteration 889, loss = 0.16619658\n",
      "Iteration 890, loss = 0.16641603\n",
      "Iteration 891, loss = 0.16609782\n",
      "Iteration 892, loss = 0.16575304\n",
      "Iteration 893, loss = 0.16611670\n",
      "Iteration 894, loss = 0.16566985\n",
      "Iteration 895, loss = 0.16525822\n",
      "Iteration 896, loss = 0.16512278\n",
      "Iteration 897, loss = 0.16482965\n",
      "Iteration 898, loss = 0.16461322\n",
      "Iteration 899, loss = 0.16427272\n",
      "Iteration 900, loss = 0.16424951\n",
      "Iteration 901, loss = 0.16403283\n",
      "Iteration 902, loss = 0.16443239\n",
      "Iteration 903, loss = 0.16348525\n",
      "Iteration 904, loss = 0.16343117\n",
      "Iteration 905, loss = 0.16313974\n",
      "Iteration 906, loss = 0.16320562\n",
      "Iteration 907, loss = 0.16313602\n",
      "Iteration 908, loss = 0.16262975\n",
      "Iteration 909, loss = 0.16224326\n",
      "Iteration 910, loss = 0.16251376\n",
      "Iteration 911, loss = 0.16215427\n",
      "Iteration 912, loss = 0.16217238\n",
      "Iteration 913, loss = 0.16194344\n",
      "Iteration 914, loss = 0.16167699\n",
      "Iteration 915, loss = 0.16165574\n",
      "Iteration 916, loss = 0.16136236\n",
      "Iteration 917, loss = 0.16148574\n",
      "Iteration 918, loss = 0.16062051\n",
      "Iteration 919, loss = 0.16104871\n",
      "Iteration 920, loss = 0.16059128\n",
      "Iteration 921, loss = 0.16033058\n",
      "Iteration 922, loss = 0.15984480\n",
      "Iteration 923, loss = 0.15995198\n",
      "Iteration 924, loss = 0.15981705\n",
      "Iteration 925, loss = 0.15955027\n",
      "Iteration 926, loss = 0.15947706\n",
      "Iteration 927, loss = 0.15918381\n",
      "Iteration 928, loss = 0.15903468\n",
      "Iteration 929, loss = 0.15887502\n",
      "Iteration 930, loss = 0.15859341\n",
      "Iteration 931, loss = 0.15838950\n",
      "Iteration 932, loss = 0.15811020\n",
      "Iteration 933, loss = 0.15842354\n",
      "Iteration 934, loss = 0.15792901\n",
      "Iteration 935, loss = 0.15742396\n",
      "Iteration 936, loss = 0.15764655\n",
      "Iteration 937, loss = 0.15772263\n",
      "Iteration 938, loss = 0.15726019\n",
      "Iteration 939, loss = 0.15691060\n",
      "Iteration 940, loss = 0.15697576\n",
      "Iteration 941, loss = 0.15676000\n",
      "Iteration 942, loss = 0.15653654\n",
      "Iteration 943, loss = 0.15620325\n",
      "Iteration 944, loss = 0.15607994\n",
      "Iteration 945, loss = 0.15588883\n",
      "Iteration 946, loss = 0.15572612\n",
      "Iteration 947, loss = 0.15567232\n",
      "Iteration 948, loss = 0.15595228\n",
      "Iteration 949, loss = 0.15534646\n",
      "Iteration 950, loss = 0.15492250\n",
      "Iteration 951, loss = 0.15493791\n",
      "Iteration 952, loss = 0.15469805\n",
      "Iteration 953, loss = 0.15471044\n",
      "Iteration 954, loss = 0.15431764\n",
      "Iteration 955, loss = 0.15406065\n",
      "Iteration 956, loss = 0.15405453\n",
      "Iteration 957, loss = 0.15371112\n",
      "Iteration 958, loss = 0.15398421\n",
      "Iteration 959, loss = 0.15388252\n",
      "Iteration 960, loss = 0.15345804\n",
      "Iteration 961, loss = 0.15327037\n",
      "Iteration 962, loss = 0.15337813\n",
      "Iteration 963, loss = 0.15295422\n",
      "Iteration 964, loss = 0.15256558\n",
      "Iteration 965, loss = 0.15264373\n",
      "Iteration 966, loss = 0.15293771\n",
      "Iteration 967, loss = 0.15252019\n",
      "Iteration 968, loss = 0.15178714\n",
      "Iteration 969, loss = 0.15160622\n",
      "Iteration 970, loss = 0.15169799\n",
      "Iteration 971, loss = 0.15180561\n",
      "Iteration 972, loss = 0.15146063\n",
      "Iteration 973, loss = 0.15157657\n",
      "Iteration 974, loss = 0.15126973\n",
      "Iteration 975, loss = 0.15068904\n",
      "Iteration 976, loss = 0.15060591\n",
      "Iteration 977, loss = 0.15094290\n",
      "Iteration 978, loss = 0.15046757\n",
      "Iteration 979, loss = 0.15014441\n",
      "Iteration 980, loss = 0.15031785\n",
      "Iteration 981, loss = 0.15022841\n",
      "Iteration 982, loss = 0.14993149\n",
      "Iteration 983, loss = 0.14967253\n",
      "Iteration 984, loss = 0.14967975\n",
      "Iteration 985, loss = 0.14927197\n",
      "Iteration 986, loss = 0.14963554\n",
      "Iteration 987, loss = 0.14902374\n",
      "Iteration 988, loss = 0.14889870\n",
      "Iteration 989, loss = 0.14858970\n",
      "Iteration 990, loss = 0.14889079\n",
      "Iteration 991, loss = 0.14848434\n",
      "Iteration 992, loss = 0.14823838\n",
      "Iteration 993, loss = 0.14832059\n",
      "Iteration 994, loss = 0.14829327\n",
      "Iteration 995, loss = 0.14779722\n",
      "Iteration 996, loss = 0.14773846\n",
      "Iteration 997, loss = 0.14763689\n",
      "Iteration 998, loss = 0.14755632\n",
      "Iteration 999, loss = 0.14764474\n",
      "Iteration 1000, loss = 0.14715415\n",
      "Iteration 1001, loss = 0.14677771\n",
      "Iteration 1002, loss = 0.14688594\n",
      "Iteration 1003, loss = 0.14687154\n",
      "Iteration 1004, loss = 0.14630775\n",
      "Iteration 1005, loss = 0.14640368\n",
      "Iteration 1006, loss = 0.14612345\n",
      "Iteration 1007, loss = 0.14589041\n",
      "Iteration 1008, loss = 0.14636957\n",
      "Iteration 1009, loss = 0.14567656\n",
      "Iteration 1010, loss = 0.14556569\n",
      "Iteration 1011, loss = 0.14551882\n",
      "Iteration 1012, loss = 0.14524517\n",
      "Iteration 1013, loss = 0.14539388\n",
      "Iteration 1014, loss = 0.14533203\n",
      "Iteration 1015, loss = 0.14500975\n",
      "Iteration 1016, loss = 0.14488103\n",
      "Iteration 1017, loss = 0.14469862\n",
      "Iteration 1018, loss = 0.14461579\n",
      "Iteration 1019, loss = 0.14444247\n",
      "Iteration 1020, loss = 0.14471518\n",
      "Iteration 1021, loss = 0.14411838\n",
      "Iteration 1022, loss = 0.14391771\n",
      "Iteration 1023, loss = 0.14408390\n",
      "Iteration 1024, loss = 0.14366477\n",
      "Iteration 1025, loss = 0.14408256\n",
      "Iteration 1026, loss = 0.14380674\n",
      "Iteration 1027, loss = 0.14349672\n",
      "Iteration 1028, loss = 0.14337566\n",
      "Iteration 1029, loss = 0.14310650\n",
      "Iteration 1030, loss = 0.14303889\n",
      "Iteration 1031, loss = 0.14281472\n",
      "Iteration 1032, loss = 0.14297903\n",
      "Iteration 1033, loss = 0.14262303\n",
      "Iteration 1034, loss = 0.14241200\n",
      "Iteration 1035, loss = 0.14252579\n",
      "Iteration 1036, loss = 0.14164641\n",
      "Iteration 1037, loss = 0.14225911\n",
      "Iteration 1038, loss = 0.14177195\n",
      "Iteration 1039, loss = 0.14165601\n",
      "Iteration 1040, loss = 0.14145990\n",
      "Iteration 1041, loss = 0.14133359\n",
      "Iteration 1042, loss = 0.14132534\n",
      "Iteration 1043, loss = 0.14140911\n",
      "Iteration 1044, loss = 0.14129180\n",
      "Iteration 1045, loss = 0.14097285\n",
      "Iteration 1046, loss = 0.14128927\n",
      "Iteration 1047, loss = 0.14073878\n",
      "Iteration 1048, loss = 0.14086226\n",
      "Iteration 1049, loss = 0.14040567\n",
      "Iteration 1050, loss = 0.14053489\n",
      "Iteration 1051, loss = 0.14020686\n",
      "Iteration 1052, loss = 0.14005824\n",
      "Iteration 1053, loss = 0.14011184\n",
      "Iteration 1054, loss = 0.13995479\n",
      "Iteration 1055, loss = 0.13969432\n",
      "Iteration 1056, loss = 0.13956446\n",
      "Iteration 1057, loss = 0.13950232\n",
      "Iteration 1058, loss = 0.13891682\n",
      "Iteration 1059, loss = 0.13941645\n",
      "Iteration 1060, loss = 0.13925265\n",
      "Iteration 1061, loss = 0.13904923\n",
      "Iteration 1062, loss = 0.13926108\n",
      "Iteration 1063, loss = 0.13862493\n",
      "Iteration 1064, loss = 0.13907977\n",
      "Iteration 1065, loss = 0.13816090\n",
      "Iteration 1066, loss = 0.13853316\n",
      "Iteration 1067, loss = 0.13796981\n",
      "Iteration 1068, loss = 0.13835743\n",
      "Iteration 1069, loss = 0.13799541\n",
      "Iteration 1070, loss = 0.13822171\n",
      "Iteration 1071, loss = 0.13765382\n",
      "Iteration 1072, loss = 0.13784470\n",
      "Iteration 1073, loss = 0.13750021\n",
      "Iteration 1074, loss = 0.13781130\n",
      "Iteration 1075, loss = 0.13751711\n",
      "Iteration 1076, loss = 0.13745057\n",
      "Iteration 1077, loss = 0.13711509\n",
      "Iteration 1078, loss = 0.13746991\n",
      "Iteration 1079, loss = 0.13694848\n",
      "Iteration 1080, loss = 0.13691555\n",
      "Iteration 1081, loss = 0.13661865\n",
      "Iteration 1082, loss = 0.13680578\n",
      "Iteration 1083, loss = 0.13622093\n",
      "Iteration 1084, loss = 0.13654711\n",
      "Iteration 1085, loss = 0.13603770\n",
      "Iteration 1086, loss = 0.13633489\n",
      "Iteration 1087, loss = 0.13593070\n",
      "Iteration 1088, loss = 0.13557234\n",
      "Iteration 1089, loss = 0.13590708\n",
      "Iteration 1090, loss = 0.13594943\n",
      "Iteration 1091, loss = 0.13541555\n",
      "Iteration 1092, loss = 0.13573165\n",
      "Iteration 1093, loss = 0.13546492\n",
      "Iteration 1094, loss = 0.13507705\n",
      "Iteration 1095, loss = 0.13549604\n",
      "Iteration 1096, loss = 0.13521052\n",
      "Iteration 1097, loss = 0.13496772\n",
      "Iteration 1098, loss = 0.13477946\n",
      "Iteration 1099, loss = 0.13482581\n",
      "Iteration 1100, loss = 0.13464390\n",
      "Iteration 1101, loss = 0.13460258\n",
      "Iteration 1102, loss = 0.13493087\n",
      "Iteration 1103, loss = 0.13459280\n",
      "Iteration 1104, loss = 0.13401160\n",
      "Iteration 1105, loss = 0.13391798\n",
      "Iteration 1106, loss = 0.13394229\n",
      "Iteration 1107, loss = 0.13390563\n",
      "Iteration 1108, loss = 0.13356565\n",
      "Iteration 1109, loss = 0.13366863\n",
      "Iteration 1110, loss = 0.13428101\n",
      "Iteration 1111, loss = 0.13358208\n",
      "Iteration 1112, loss = 0.13322649\n",
      "Iteration 1113, loss = 0.13324743\n",
      "Iteration 1114, loss = 0.13330745\n",
      "Iteration 1115, loss = 0.13310100\n",
      "Iteration 1116, loss = 0.13291683\n",
      "Iteration 1117, loss = 0.13263532\n",
      "Iteration 1118, loss = 0.13296904\n",
      "Iteration 1119, loss = 0.13268121\n",
      "Iteration 1120, loss = 0.13261861\n",
      "Iteration 1121, loss = 0.13255116\n",
      "Iteration 1122, loss = 0.13240810\n",
      "Iteration 1123, loss = 0.13235038\n",
      "Iteration 1124, loss = 0.13213247\n",
      "Iteration 1125, loss = 0.13238844\n",
      "Iteration 1126, loss = 0.13212640\n",
      "Iteration 1127, loss = 0.13209853\n",
      "Iteration 1128, loss = 0.13167344\n",
      "Iteration 1129, loss = 0.13161887\n",
      "Iteration 1130, loss = 0.13175740\n",
      "Iteration 1131, loss = 0.13131349\n",
      "Iteration 1132, loss = 0.13156660\n",
      "Iteration 1133, loss = 0.13158769\n",
      "Iteration 1134, loss = 0.13125659\n",
      "Iteration 1135, loss = 0.13083453\n",
      "Iteration 1136, loss = 0.13109969\n",
      "Iteration 1137, loss = 0.13109368\n",
      "Iteration 1138, loss = 0.13073756\n",
      "Iteration 1139, loss = 0.13127684\n",
      "Iteration 1140, loss = 0.13060632\n",
      "Iteration 1141, loss = 0.13057228\n",
      "Iteration 1142, loss = 0.13065347\n",
      "Iteration 1143, loss = 0.13017787\n",
      "Iteration 1144, loss = 0.13055277\n",
      "Iteration 1145, loss = 0.12986609\n",
      "Iteration 1146, loss = 0.12962579\n",
      "Iteration 1147, loss = 0.13027836\n",
      "Iteration 1148, loss = 0.12967191\n",
      "Iteration 1149, loss = 0.12965699\n",
      "Iteration 1150, loss = 0.13007336\n",
      "Iteration 1151, loss = 0.12970888\n",
      "Iteration 1152, loss = 0.12969597\n",
      "Iteration 1153, loss = 0.12927243\n",
      "Iteration 1154, loss = 0.12940740\n",
      "Iteration 1155, loss = 0.12928375\n",
      "Iteration 1156, loss = 0.12908737\n",
      "Iteration 1157, loss = 0.12895529\n",
      "Iteration 1158, loss = 0.12937611\n",
      "Iteration 1159, loss = 0.12876937\n",
      "Iteration 1160, loss = 0.12869447\n",
      "Iteration 1161, loss = 0.12895135\n",
      "Iteration 1162, loss = 0.12853631\n",
      "Iteration 1163, loss = 0.12857214\n",
      "Iteration 1164, loss = 0.12864988\n",
      "Iteration 1165, loss = 0.12830232\n",
      "Iteration 1166, loss = 0.12811015\n",
      "Iteration 1167, loss = 0.12825045\n",
      "Iteration 1168, loss = 0.12783414\n",
      "Iteration 1169, loss = 0.12786334\n",
      "Iteration 1170, loss = 0.12776668\n",
      "Iteration 1171, loss = 0.12790347\n",
      "Iteration 1172, loss = 0.12755096\n",
      "Iteration 1173, loss = 0.12738619\n",
      "Iteration 1174, loss = 0.12778661\n",
      "Iteration 1175, loss = 0.12768764\n",
      "Iteration 1176, loss = 0.12724093\n",
      "Iteration 1177, loss = 0.12710243\n",
      "Iteration 1178, loss = 0.12715044\n",
      "Iteration 1179, loss = 0.12715515\n",
      "Iteration 1180, loss = 0.12667365\n",
      "Iteration 1181, loss = 0.12683597\n",
      "Iteration 1182, loss = 0.12692796\n",
      "Iteration 1183, loss = 0.12666896\n",
      "Iteration 1184, loss = 0.12670908\n",
      "Iteration 1185, loss = 0.12665320\n",
      "Iteration 1186, loss = 0.12626014\n",
      "Iteration 1187, loss = 0.12627715\n",
      "Iteration 1188, loss = 0.12661454\n",
      "Iteration 1189, loss = 0.12620125\n",
      "Iteration 1190, loss = 0.12587214\n",
      "Iteration 1191, loss = 0.12608745\n",
      "Iteration 1192, loss = 0.12599591\n",
      "Iteration 1193, loss = 0.12607548\n",
      "Iteration 1194, loss = 0.12605763\n",
      "Iteration 1195, loss = 0.12564200\n",
      "Iteration 1196, loss = 0.12550287\n",
      "Iteration 1197, loss = 0.12532450\n",
      "Iteration 1198, loss = 0.12520193\n",
      "Iteration 1199, loss = 0.12525217\n",
      "Iteration 1200, loss = 0.12518200\n",
      "Iteration 1201, loss = 0.12493855\n",
      "Iteration 1202, loss = 0.12489790\n",
      "Iteration 1203, loss = 0.12462049\n",
      "Iteration 1204, loss = 0.12500274\n",
      "Iteration 1205, loss = 0.12543889\n",
      "Iteration 1206, loss = 0.12450272\n",
      "Iteration 1207, loss = 0.12456465\n",
      "Iteration 1208, loss = 0.12473990\n",
      "Iteration 1209, loss = 0.12455186\n",
      "Iteration 1210, loss = 0.12484207\n",
      "Iteration 1211, loss = 0.12415198\n",
      "Iteration 1212, loss = 0.12396872\n",
      "Iteration 1213, loss = 0.12405994\n",
      "Iteration 1214, loss = 0.12414705\n",
      "Iteration 1215, loss = 0.12384231\n",
      "Iteration 1216, loss = 0.12375336\n",
      "Iteration 1217, loss = 0.12381918\n",
      "Iteration 1218, loss = 0.12375460\n",
      "Iteration 1219, loss = 0.12369648\n",
      "Iteration 1220, loss = 0.12348003\n",
      "Iteration 1221, loss = 0.12360174\n",
      "Iteration 1222, loss = 0.12361230\n",
      "Iteration 1223, loss = 0.12363271\n",
      "Iteration 1224, loss = 0.12302448\n",
      "Iteration 1225, loss = 0.12311995\n",
      "Iteration 1226, loss = 0.12309511\n",
      "Iteration 1227, loss = 0.12268079\n",
      "Iteration 1228, loss = 0.12310979\n",
      "Iteration 1229, loss = 0.12248261\n",
      "Iteration 1230, loss = 0.12278806\n",
      "Iteration 1231, loss = 0.12297468\n",
      "Iteration 1232, loss = 0.12223246\n",
      "Iteration 1233, loss = 0.12264026\n",
      "Iteration 1234, loss = 0.12240174\n",
      "Iteration 1235, loss = 0.12241541\n",
      "Iteration 1236, loss = 0.12234225\n",
      "Iteration 1237, loss = 0.12199504\n",
      "Iteration 1238, loss = 0.12188055\n",
      "Iteration 1239, loss = 0.12194970\n",
      "Iteration 1240, loss = 0.12212214\n",
      "Iteration 1241, loss = 0.12182419\n",
      "Iteration 1242, loss = 0.12159409\n",
      "Iteration 1243, loss = 0.12171619\n",
      "Iteration 1244, loss = 0.12157168\n",
      "Iteration 1245, loss = 0.12179394\n",
      "Iteration 1246, loss = 0.12111292\n",
      "Iteration 1247, loss = 0.12096631\n",
      "Iteration 1248, loss = 0.12105863\n",
      "Iteration 1249, loss = 0.12095879\n",
      "Iteration 1250, loss = 0.12115537\n",
      "Iteration 1251, loss = 0.12138053\n",
      "Iteration 1252, loss = 0.12120963\n",
      "Iteration 1253, loss = 0.12104614\n",
      "Iteration 1254, loss = 0.12086540\n",
      "Iteration 1255, loss = 0.12063241\n",
      "Iteration 1256, loss = 0.12036639\n",
      "Iteration 1257, loss = 0.12067397\n",
      "Iteration 1258, loss = 0.12057872\n",
      "Iteration 1259, loss = 0.12071712\n",
      "Iteration 1260, loss = 0.12053392\n",
      "Iteration 1261, loss = 0.12067069\n",
      "Iteration 1262, loss = 0.12015366\n",
      "Iteration 1263, loss = 0.11993260\n",
      "Iteration 1264, loss = 0.12028415\n",
      "Iteration 1265, loss = 0.12010787\n",
      "Iteration 1266, loss = 0.11993382\n",
      "Iteration 1267, loss = 0.11965149\n",
      "Iteration 1268, loss = 0.12001240\n",
      "Iteration 1269, loss = 0.11947701\n",
      "Iteration 1270, loss = 0.11975725\n",
      "Iteration 1271, loss = 0.11918518\n",
      "Iteration 1272, loss = 0.11934120\n",
      "Iteration 1273, loss = 0.11936971\n",
      "Iteration 1274, loss = 0.11911341\n",
      "Iteration 1275, loss = 0.11942399\n",
      "Iteration 1276, loss = 0.11925465\n",
      "Iteration 1277, loss = 0.11939492\n",
      "Iteration 1278, loss = 0.11897191\n",
      "Iteration 1279, loss = 0.11895144\n",
      "Iteration 1280, loss = 0.11878100\n",
      "Iteration 1281, loss = 0.11905325\n",
      "Iteration 1282, loss = 0.11885668\n",
      "Iteration 1283, loss = 0.11870760\n",
      "Iteration 1284, loss = 0.11856779\n",
      "Iteration 1285, loss = 0.11852662\n",
      "Iteration 1286, loss = 0.11867212\n",
      "Iteration 1287, loss = 0.11881681\n",
      "Iteration 1288, loss = 0.11815786\n",
      "Iteration 1289, loss = 0.11851593\n",
      "Iteration 1290, loss = 0.11816884\n",
      "Iteration 1291, loss = 0.11859456\n",
      "Iteration 1292, loss = 0.11815926\n",
      "Iteration 1293, loss = 0.11802265\n",
      "Iteration 1294, loss = 0.11787849\n",
      "Iteration 1295, loss = 0.11789943\n",
      "Iteration 1296, loss = 0.11768139\n",
      "Iteration 1297, loss = 0.11779096\n",
      "Iteration 1298, loss = 0.11788979\n",
      "Iteration 1299, loss = 0.11758304\n",
      "Iteration 1300, loss = 0.11742891\n",
      "Iteration 1301, loss = 0.11726898\n",
      "Iteration 1302, loss = 0.11753682\n",
      "Iteration 1303, loss = 0.11695195\n",
      "Iteration 1304, loss = 0.11742036\n",
      "Iteration 1305, loss = 0.11731870\n",
      "Iteration 1306, loss = 0.11705566\n",
      "Iteration 1307, loss = 0.11706068\n",
      "Iteration 1308, loss = 0.11708101\n",
      "Iteration 1309, loss = 0.11683614\n",
      "Iteration 1310, loss = 0.11717070\n",
      "Iteration 1311, loss = 0.11677621\n",
      "Iteration 1312, loss = 0.11662908\n",
      "Iteration 1313, loss = 0.11664850\n",
      "Iteration 1314, loss = 0.11697587\n",
      "Iteration 1315, loss = 0.11627352\n",
      "Iteration 1316, loss = 0.11657675\n",
      "Iteration 1317, loss = 0.11635202\n",
      "Iteration 1318, loss = 0.11673176\n",
      "Iteration 1319, loss = 0.11644359\n",
      "Iteration 1320, loss = 0.11590800\n",
      "Iteration 1321, loss = 0.11605900\n",
      "Iteration 1322, loss = 0.11596524\n",
      "Iteration 1323, loss = 0.11586141\n",
      "Iteration 1324, loss = 0.11613858\n",
      "Iteration 1325, loss = 0.11582656\n",
      "Iteration 1326, loss = 0.11613326\n",
      "Iteration 1327, loss = 0.11568614\n",
      "Iteration 1328, loss = 0.11591728\n",
      "Iteration 1329, loss = 0.11572017\n",
      "Iteration 1330, loss = 0.11544131\n",
      "Iteration 1331, loss = 0.11544755\n",
      "Iteration 1332, loss = 0.11580327\n",
      "Iteration 1333, loss = 0.11519825\n",
      "Iteration 1334, loss = 0.11509614\n",
      "Iteration 1335, loss = 0.11548130\n",
      "Iteration 1336, loss = 0.11558488\n",
      "Iteration 1337, loss = 0.11475072\n",
      "Iteration 1338, loss = 0.11578348\n",
      "Iteration 1339, loss = 0.11498127\n",
      "Iteration 1340, loss = 0.11516259\n",
      "Iteration 1341, loss = 0.11469157\n",
      "Iteration 1342, loss = 0.11474036\n",
      "Iteration 1343, loss = 0.11504443\n",
      "Iteration 1344, loss = 0.11470016\n",
      "Iteration 1345, loss = 0.11455353\n",
      "Iteration 1346, loss = 0.11472463\n",
      "Iteration 1347, loss = 0.11449335\n",
      "Iteration 1348, loss = 0.11465715\n",
      "Iteration 1349, loss = 0.11422266\n",
      "Iteration 1350, loss = 0.11434456\n",
      "Iteration 1351, loss = 0.11461214\n",
      "Iteration 1352, loss = 0.11438858\n",
      "Iteration 1353, loss = 0.11405278\n",
      "Iteration 1354, loss = 0.11415060\n",
      "Iteration 1355, loss = 0.11425911\n",
      "Iteration 1356, loss = 0.11379300\n",
      "Iteration 1357, loss = 0.11407815\n",
      "Iteration 1358, loss = 0.11396252\n",
      "Iteration 1359, loss = 0.11403113\n",
      "Iteration 1360, loss = 0.11379903\n",
      "Iteration 1361, loss = 0.11407398\n",
      "Iteration 1362, loss = 0.11378186\n",
      "Iteration 1363, loss = 0.11357955\n",
      "Iteration 1364, loss = 0.11361767\n",
      "Iteration 1365, loss = 0.11325975\n",
      "Iteration 1366, loss = 0.11347395\n",
      "Iteration 1367, loss = 0.11330880\n",
      "Iteration 1368, loss = 0.11367764\n",
      "Iteration 1369, loss = 0.11322928\n",
      "Iteration 1370, loss = 0.11323882\n",
      "Iteration 1371, loss = 0.11314498\n",
      "Iteration 1372, loss = 0.11347114\n",
      "Iteration 1373, loss = 0.11293456\n",
      "Iteration 1374, loss = 0.11305235\n",
      "Iteration 1375, loss = 0.11323639\n",
      "Iteration 1376, loss = 0.11282278\n",
      "Iteration 1377, loss = 0.11302143\n",
      "Iteration 1378, loss = 0.11294297\n",
      "Iteration 1379, loss = 0.11302684\n",
      "Iteration 1380, loss = 0.11254646\n",
      "Iteration 1381, loss = 0.11290168\n",
      "Iteration 1382, loss = 0.11251410\n",
      "Iteration 1383, loss = 0.11272852\n",
      "Iteration 1384, loss = 0.11292016\n",
      "Iteration 1385, loss = 0.11279449\n",
      "Iteration 1386, loss = 0.11255766\n",
      "Iteration 1387, loss = 0.11238376\n",
      "Iteration 1388, loss = 0.11234528\n",
      "Iteration 1389, loss = 0.11241835\n",
      "Iteration 1390, loss = 0.11276470\n",
      "Iteration 1391, loss = 0.11200956\n",
      "Iteration 1392, loss = 0.11217386\n",
      "Iteration 1393, loss = 0.11208595\n",
      "Iteration 1394, loss = 0.11179833\n",
      "Iteration 1395, loss = 0.11179488\n",
      "Iteration 1396, loss = 0.11195590\n",
      "Iteration 1397, loss = 0.11217356\n",
      "Iteration 1398, loss = 0.11192537\n",
      "Iteration 1399, loss = 0.11189911\n",
      "Iteration 1400, loss = 0.11166184\n",
      "Iteration 1401, loss = 0.11182689\n",
      "Iteration 1402, loss = 0.11234477\n",
      "Iteration 1403, loss = 0.11176550\n",
      "Iteration 1404, loss = 0.11133481\n",
      "Iteration 1405, loss = 0.11141587\n",
      "Iteration 1406, loss = 0.11130442\n",
      "Iteration 1407, loss = 0.11113494\n",
      "Iteration 1408, loss = 0.11172811\n",
      "Iteration 1409, loss = 0.11155173\n",
      "Iteration 1410, loss = 0.11107502\n",
      "Iteration 1411, loss = 0.11149169\n",
      "Iteration 1412, loss = 0.11093240\n",
      "Iteration 1413, loss = 0.11096627\n",
      "Iteration 1414, loss = 0.11125221\n",
      "Iteration 1415, loss = 0.11144051\n",
      "Iteration 1416, loss = 0.11112400\n",
      "Iteration 1417, loss = 0.11085270\n",
      "Iteration 1418, loss = 0.11057191\n",
      "Iteration 1419, loss = 0.11094755\n",
      "Iteration 1420, loss = 0.11088939\n",
      "Iteration 1421, loss = 0.11087443\n",
      "Iteration 1422, loss = 0.11056786\n",
      "Iteration 1423, loss = 0.11064245\n",
      "Iteration 1424, loss = 0.11081306\n",
      "Iteration 1425, loss = 0.11056805\n",
      "Iteration 1426, loss = 0.11088667\n",
      "Iteration 1427, loss = 0.11064493\n",
      "Iteration 1428, loss = 0.11049771\n",
      "Iteration 1429, loss = 0.11027602\n",
      "Iteration 1430, loss = 0.11024392\n",
      "Iteration 1431, loss = 0.11032178\n",
      "Iteration 1432, loss = 0.11066035\n",
      "Iteration 1433, loss = 0.11016521\n",
      "Iteration 1434, loss = 0.11022766\n",
      "Iteration 1435, loss = 0.11031275\n",
      "Iteration 1436, loss = 0.11015822\n",
      "Iteration 1437, loss = 0.11008100\n",
      "Iteration 1438, loss = 0.11032114\n",
      "Iteration 1439, loss = 0.10997195\n",
      "Iteration 1440, loss = 0.11003813\n",
      "Iteration 1441, loss = 0.10972649\n",
      "Iteration 1442, loss = 0.10988557\n",
      "Iteration 1443, loss = 0.10995213\n",
      "Iteration 1444, loss = 0.10986693\n",
      "Iteration 1445, loss = 0.10955458\n",
      "Iteration 1446, loss = 0.10998187\n",
      "Iteration 1447, loss = 0.10951874\n",
      "Iteration 1448, loss = 0.10977520\n",
      "Iteration 1449, loss = 0.10981630\n",
      "Iteration 1450, loss = 0.10951282\n",
      "Iteration 1451, loss = 0.10937805\n",
      "Iteration 1452, loss = 0.10956753\n",
      "Iteration 1453, loss = 0.10923190\n",
      "Iteration 1454, loss = 0.10928802\n",
      "Iteration 1455, loss = 0.10949484\n",
      "Iteration 1456, loss = 0.10914630\n",
      "Iteration 1457, loss = 0.10925397\n",
      "Iteration 1458, loss = 0.10903310\n",
      "Iteration 1459, loss = 0.10916006\n",
      "Iteration 1460, loss = 0.10905180\n",
      "Iteration 1461, loss = 0.10902348\n",
      "Iteration 1462, loss = 0.10915659\n",
      "Iteration 1463, loss = 0.10895529\n",
      "Iteration 1464, loss = 0.10917172\n",
      "Iteration 1465, loss = 0.10867288\n",
      "Iteration 1466, loss = 0.10865473\n",
      "Iteration 1467, loss = 0.10872157\n",
      "Iteration 1468, loss = 0.10889139\n",
      "Iteration 1469, loss = 0.10851317\n",
      "Iteration 1470, loss = 0.10940430\n",
      "Iteration 1471, loss = 0.10859488\n",
      "Iteration 1472, loss = 0.10842820\n",
      "Iteration 1473, loss = 0.10879318\n",
      "Iteration 1474, loss = 0.10854138\n",
      "Iteration 1475, loss = 0.10870792\n",
      "Iteration 1476, loss = 0.10891397\n",
      "Iteration 1477, loss = 0.10817022\n",
      "Iteration 1478, loss = 0.10843292\n",
      "Iteration 1479, loss = 0.10861650\n",
      "Iteration 1480, loss = 0.10828362\n",
      "Iteration 1481, loss = 0.10820525\n",
      "Iteration 1482, loss = 0.10824741\n",
      "Iteration 1483, loss = 0.10798115\n",
      "Iteration 1484, loss = 0.10828506\n",
      "Iteration 1485, loss = 0.10799197\n",
      "Iteration 1486, loss = 0.10822355\n",
      "Iteration 1487, loss = 0.10826414\n",
      "Iteration 1488, loss = 0.10849670\n",
      "Iteration 1489, loss = 0.10776247\n",
      "Iteration 1490, loss = 0.10781766\n",
      "Iteration 1491, loss = 0.10819608\n",
      "Iteration 1492, loss = 0.10815921\n",
      "Iteration 1493, loss = 0.10772291\n",
      "Iteration 1494, loss = 0.10770558\n",
      "Iteration 1495, loss = 0.10764895\n",
      "Iteration 1496, loss = 0.10796511\n",
      "Iteration 1497, loss = 0.10769770\n",
      "Iteration 1498, loss = 0.10777966\n",
      "Iteration 1499, loss = 0.10772732\n",
      "Iteration 1500, loss = 0.10727567\n",
      "Iteration 1501, loss = 0.10760268\n",
      "Iteration 1502, loss = 0.10758186\n",
      "Iteration 1503, loss = 0.10743270\n",
      "Iteration 1504, loss = 0.10740080\n",
      "Iteration 1505, loss = 0.10748647\n",
      "Iteration 1506, loss = 0.10759794\n",
      "Iteration 1507, loss = 0.10740832\n",
      "Iteration 1508, loss = 0.10710985\n",
      "Iteration 1509, loss = 0.10710856\n",
      "Iteration 1510, loss = 0.10730842\n",
      "Iteration 1511, loss = 0.10706903\n",
      "Iteration 1512, loss = 0.10732312\n",
      "Iteration 1513, loss = 0.10693652\n",
      "Iteration 1514, loss = 0.10708611\n",
      "Iteration 1515, loss = 0.10731565\n",
      "Iteration 1516, loss = 0.10683297\n",
      "Iteration 1517, loss = 0.10681666\n",
      "Iteration 1518, loss = 0.10702653\n",
      "Iteration 1519, loss = 0.10702109\n",
      "Iteration 1520, loss = 0.10706515\n",
      "Iteration 1521, loss = 0.10661728\n",
      "Iteration 1522, loss = 0.10639274\n",
      "Iteration 1523, loss = 0.10703295\n",
      "Iteration 1524, loss = 0.10748946\n",
      "Iteration 1525, loss = 0.10641750\n",
      "Iteration 1526, loss = 0.10619070\n",
      "Iteration 1527, loss = 0.10665313\n",
      "Iteration 1528, loss = 0.10640259\n",
      "Iteration 1529, loss = 0.10652810\n",
      "Iteration 1530, loss = 0.10640208\n",
      "Iteration 1531, loss = 0.10607005\n",
      "Iteration 1532, loss = 0.10640695\n",
      "Iteration 1533, loss = 0.10660667\n",
      "Iteration 1534, loss = 0.10630542\n",
      "Iteration 1535, loss = 0.10638760\n",
      "Iteration 1536, loss = 0.10610012\n",
      "Iteration 1537, loss = 0.10627129\n",
      "Iteration 1538, loss = 0.10661494\n",
      "Iteration 1539, loss = 0.10608025\n",
      "Iteration 1540, loss = 0.10624144\n",
      "Iteration 1541, loss = 0.10600398\n",
      "Iteration 1542, loss = 0.10596421\n",
      "Iteration 1543, loss = 0.10595337\n",
      "Iteration 1544, loss = 0.10562410\n",
      "Iteration 1545, loss = 0.10573540\n",
      "Iteration 1546, loss = 0.10611172\n",
      "Iteration 1547, loss = 0.10623070\n",
      "Iteration 1548, loss = 0.10643489\n",
      "Iteration 1549, loss = 0.10566539\n",
      "Iteration 1550, loss = 0.10573459\n",
      "Iteration 1551, loss = 0.10554318\n",
      "Iteration 1552, loss = 0.10578751\n",
      "Iteration 1553, loss = 0.10560458\n",
      "Iteration 1554, loss = 0.10565975\n",
      "Iteration 1555, loss = 0.10549612\n",
      "Iteration 1556, loss = 0.10558030\n",
      "Iteration 1557, loss = 0.10558972\n",
      "Iteration 1558, loss = 0.10555162\n",
      "Iteration 1559, loss = 0.10541569\n",
      "Iteration 1560, loss = 0.10547165\n",
      "Iteration 1561, loss = 0.10546721\n",
      "Iteration 1562, loss = 0.10540271\n",
      "Iteration 1563, loss = 0.10576464\n",
      "Iteration 1564, loss = 0.10531848\n",
      "Iteration 1565, loss = 0.10514731\n",
      "Iteration 1566, loss = 0.10522600\n",
      "Iteration 1567, loss = 0.10560875\n",
      "Iteration 1568, loss = 0.10520280\n",
      "Iteration 1569, loss = 0.10523121\n",
      "Iteration 1570, loss = 0.10520821\n",
      "Iteration 1571, loss = 0.10490217\n",
      "Iteration 1572, loss = 0.10473434\n",
      "Iteration 1573, loss = 0.10498719\n",
      "Iteration 1574, loss = 0.10510761\n",
      "Iteration 1575, loss = 0.10511402\n",
      "Iteration 1576, loss = 0.10491678\n",
      "Iteration 1577, loss = 0.10508277\n",
      "Iteration 1578, loss = 0.10499955\n",
      "Iteration 1579, loss = 0.10446002\n",
      "Iteration 1580, loss = 0.10454260\n",
      "Iteration 1581, loss = 0.10480662\n",
      "Iteration 1582, loss = 0.10473349\n",
      "Iteration 1583, loss = 0.10461017\n",
      "Iteration 1584, loss = 0.10474933\n",
      "Iteration 1585, loss = 0.10446011\n",
      "Iteration 1586, loss = 0.10450314\n",
      "Iteration 1587, loss = 0.10453775\n",
      "Iteration 1588, loss = 0.10462473\n",
      "Iteration 1589, loss = 0.10508014\n",
      "Iteration 1590, loss = 0.10446396\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65163449\n",
      "Iteration 2, loss = 0.64568639\n",
      "Iteration 3, loss = 0.64178817\n",
      "Iteration 4, loss = 0.63798521\n",
      "Iteration 5, loss = 0.63390166\n",
      "Iteration 6, loss = 0.62908503\n",
      "Iteration 7, loss = 0.62423493\n",
      "Iteration 8, loss = 0.61893730\n",
      "Iteration 9, loss = 0.61409519\n",
      "Iteration 10, loss = 0.60826204\n",
      "Iteration 11, loss = 0.60233906\n",
      "Iteration 12, loss = 0.59627551\n",
      "Iteration 13, loss = 0.58983280\n",
      "Iteration 14, loss = 0.58338872\n",
      "Iteration 15, loss = 0.57658458\n",
      "Iteration 16, loss = 0.56886913\n",
      "Iteration 17, loss = 0.56171838\n",
      "Iteration 18, loss = 0.55428909\n",
      "Iteration 19, loss = 0.54677487\n",
      "Iteration 20, loss = 0.53867361\n",
      "Iteration 21, loss = 0.53174212\n",
      "Iteration 22, loss = 0.52369316\n",
      "Iteration 23, loss = 0.51688001\n",
      "Iteration 24, loss = 0.50978107\n",
      "Iteration 25, loss = 0.50182332\n",
      "Iteration 26, loss = 0.49502492\n",
      "Iteration 27, loss = 0.48777953\n",
      "Iteration 28, loss = 0.48099633\n",
      "Iteration 29, loss = 0.47466775\n",
      "Iteration 30, loss = 0.46876004\n",
      "Iteration 31, loss = 0.46081862\n",
      "Iteration 32, loss = 0.45548092\n",
      "Iteration 33, loss = 0.44807547\n",
      "Iteration 34, loss = 0.44189758\n",
      "Iteration 35, loss = 0.43659378\n",
      "Iteration 36, loss = 0.42979853\n",
      "Iteration 37, loss = 0.42420000\n",
      "Iteration 38, loss = 0.41846438\n",
      "Iteration 39, loss = 0.41253405\n",
      "Iteration 40, loss = 0.40711513\n",
      "Iteration 41, loss = 0.40180529\n",
      "Iteration 42, loss = 0.39592847\n",
      "Iteration 43, loss = 0.39107435\n",
      "Iteration 44, loss = 0.38566627\n",
      "Iteration 45, loss = 0.38022974\n",
      "Iteration 46, loss = 0.37639444\n",
      "Iteration 47, loss = 0.37113153\n",
      "Iteration 48, loss = 0.36583402\n",
      "Iteration 49, loss = 0.36133267\n",
      "Iteration 50, loss = 0.35733912\n",
      "Iteration 51, loss = 0.35222311\n",
      "Iteration 52, loss = 0.34793077\n",
      "Iteration 53, loss = 0.34403523\n",
      "Iteration 54, loss = 0.33983713\n",
      "Iteration 55, loss = 0.33602391\n",
      "Iteration 56, loss = 0.33173059\n",
      "Iteration 57, loss = 0.32820300\n",
      "Iteration 58, loss = 0.32392519\n",
      "Iteration 59, loss = 0.32038875\n",
      "Iteration 60, loss = 0.31660557\n",
      "Iteration 61, loss = 0.31348286\n",
      "Iteration 62, loss = 0.30986689\n",
      "Iteration 63, loss = 0.30664520\n",
      "Iteration 64, loss = 0.30207317\n",
      "Iteration 65, loss = 0.29957786\n",
      "Iteration 66, loss = 0.29631840\n",
      "Iteration 67, loss = 0.29297095\n",
      "Iteration 68, loss = 0.28954602\n",
      "Iteration 69, loss = 0.28670088\n",
      "Iteration 70, loss = 0.28310306\n",
      "Iteration 71, loss = 0.28056450\n",
      "Iteration 72, loss = 0.27705814\n",
      "Iteration 73, loss = 0.27385009\n",
      "Iteration 74, loss = 0.27113889\n",
      "Iteration 75, loss = 0.26753793\n",
      "Iteration 76, loss = 0.26534971\n",
      "Iteration 77, loss = 0.26232534\n",
      "Iteration 78, loss = 0.26003941\n",
      "Iteration 79, loss = 0.25671366\n",
      "Iteration 80, loss = 0.25396182\n",
      "Iteration 81, loss = 0.25158001\n",
      "Iteration 82, loss = 0.24900225\n",
      "Iteration 83, loss = 0.24669636\n",
      "Iteration 84, loss = 0.24393285\n",
      "Iteration 85, loss = 0.24154532\n",
      "Iteration 86, loss = 0.23883574\n",
      "Iteration 87, loss = 0.23671925\n",
      "Iteration 88, loss = 0.23386728\n",
      "Iteration 89, loss = 0.23193324\n",
      "Iteration 90, loss = 0.22961487\n",
      "Iteration 91, loss = 0.22773092\n",
      "Iteration 92, loss = 0.22499472\n",
      "Iteration 93, loss = 0.22309119\n",
      "Iteration 94, loss = 0.22104072\n",
      "Iteration 95, loss = 0.21848281\n",
      "Iteration 96, loss = 0.21648380\n",
      "Iteration 97, loss = 0.21426667\n",
      "Iteration 98, loss = 0.21260986\n",
      "Iteration 99, loss = 0.21010004\n",
      "Iteration 100, loss = 0.20816706\n",
      "Iteration 101, loss = 0.20656795\n",
      "Iteration 102, loss = 0.20434770\n",
      "Iteration 103, loss = 0.20246628\n",
      "Iteration 104, loss = 0.20087801\n",
      "Iteration 105, loss = 0.19930349\n",
      "Iteration 106, loss = 0.19735586\n",
      "Iteration 107, loss = 0.19537748\n",
      "Iteration 108, loss = 0.19386067\n",
      "Iteration 109, loss = 0.19163043\n",
      "Iteration 110, loss = 0.19012716\n",
      "Iteration 111, loss = 0.18848472\n",
      "Iteration 112, loss = 0.18648982\n",
      "Iteration 113, loss = 0.18469590\n",
      "Iteration 114, loss = 0.18336955\n",
      "Iteration 115, loss = 0.18162370\n",
      "Iteration 116, loss = 0.18011195\n",
      "Iteration 117, loss = 0.17866432\n",
      "Iteration 118, loss = 0.17687153\n",
      "Iteration 119, loss = 0.17536892\n",
      "Iteration 120, loss = 0.17361887\n",
      "Iteration 121, loss = 0.17281871\n",
      "Iteration 122, loss = 0.17055282\n",
      "Iteration 123, loss = 0.16924891\n",
      "Iteration 124, loss = 0.16788457\n",
      "Iteration 125, loss = 0.16596200\n",
      "Iteration 126, loss = 0.16497115\n",
      "Iteration 127, loss = 0.16311448\n",
      "Iteration 128, loss = 0.16176653\n",
      "Iteration 129, loss = 0.16054520\n",
      "Iteration 130, loss = 0.15908238\n",
      "Iteration 131, loss = 0.15777602\n",
      "Iteration 132, loss = 0.15656561\n",
      "Iteration 133, loss = 0.15517781\n",
      "Iteration 134, loss = 0.15345895\n",
      "Iteration 135, loss = 0.15196859\n",
      "Iteration 136, loss = 0.15065832\n",
      "Iteration 137, loss = 0.15012596\n",
      "Iteration 138, loss = 0.14823011\n",
      "Iteration 139, loss = 0.14664873\n",
      "Iteration 140, loss = 0.14600372\n",
      "Iteration 141, loss = 0.14493564\n",
      "Iteration 142, loss = 0.14335774\n",
      "Iteration 143, loss = 0.14192837\n",
      "Iteration 144, loss = 0.14101488\n",
      "Iteration 145, loss = 0.13973353\n",
      "Iteration 146, loss = 0.13827172\n",
      "Iteration 147, loss = 0.13756248\n",
      "Iteration 148, loss = 0.13617107\n",
      "Iteration 149, loss = 0.13470482\n",
      "Iteration 150, loss = 0.13398374\n",
      "Iteration 151, loss = 0.13220397\n",
      "Iteration 152, loss = 0.13129770\n",
      "Iteration 153, loss = 0.13078766\n",
      "Iteration 154, loss = 0.12961339\n",
      "Iteration 155, loss = 0.12832421\n",
      "Iteration 156, loss = 0.12743980\n",
      "Iteration 157, loss = 0.12610771\n",
      "Iteration 158, loss = 0.12526129\n",
      "Iteration 159, loss = 0.12434675\n",
      "Iteration 160, loss = 0.12303309\n",
      "Iteration 161, loss = 0.12197889\n",
      "Iteration 162, loss = 0.12065088\n",
      "Iteration 163, loss = 0.11979773\n",
      "Iteration 164, loss = 0.11897523\n",
      "Iteration 165, loss = 0.11820733\n",
      "Iteration 166, loss = 0.11729170\n",
      "Iteration 167, loss = 0.11653787\n",
      "Iteration 168, loss = 0.11556212\n",
      "Iteration 169, loss = 0.11384284\n",
      "Iteration 170, loss = 0.11333269\n",
      "Iteration 171, loss = 0.11269601\n",
      "Iteration 172, loss = 0.11141252\n",
      "Iteration 173, loss = 0.11050067\n",
      "Iteration 174, loss = 0.10999164\n",
      "Iteration 175, loss = 0.10907547\n",
      "Iteration 176, loss = 0.10785037\n",
      "Iteration 177, loss = 0.10662859\n",
      "Iteration 178, loss = 0.10592062\n",
      "Iteration 179, loss = 0.10593383\n",
      "Iteration 180, loss = 0.10415804\n",
      "Iteration 181, loss = 0.10393588\n",
      "Iteration 182, loss = 0.10274938\n",
      "Iteration 183, loss = 0.10185160\n",
      "Iteration 184, loss = 0.10124653\n",
      "Iteration 185, loss = 0.10014260\n",
      "Iteration 186, loss = 0.09968255\n",
      "Iteration 187, loss = 0.09846718\n",
      "Iteration 188, loss = 0.09763950\n",
      "Iteration 189, loss = 0.09715381\n",
      "Iteration 190, loss = 0.09627327\n",
      "Iteration 191, loss = 0.09610658\n",
      "Iteration 192, loss = 0.09499086\n",
      "Iteration 193, loss = 0.09439013\n",
      "Iteration 194, loss = 0.09327937\n",
      "Iteration 195, loss = 0.09226861\n",
      "Iteration 196, loss = 0.09178547\n",
      "Iteration 197, loss = 0.09121994\n",
      "Iteration 198, loss = 0.09055939\n",
      "Iteration 199, loss = 0.09031905\n",
      "Iteration 200, loss = 0.08923660\n",
      "Iteration 201, loss = 0.08849794\n",
      "Iteration 202, loss = 0.08777129\n",
      "Iteration 203, loss = 0.08680385\n",
      "Iteration 204, loss = 0.08631999\n",
      "Iteration 205, loss = 0.08561054\n",
      "Iteration 206, loss = 0.08503185\n",
      "Iteration 207, loss = 0.08448829\n",
      "Iteration 208, loss = 0.08370268\n",
      "Iteration 209, loss = 0.08304225\n",
      "Iteration 210, loss = 0.08312304\n",
      "Iteration 211, loss = 0.08204040\n",
      "Iteration 212, loss = 0.08145041\n",
      "Iteration 213, loss = 0.08087655\n",
      "Iteration 214, loss = 0.08060345\n",
      "Iteration 215, loss = 0.07946133\n",
      "Iteration 216, loss = 0.07914137\n",
      "Iteration 217, loss = 0.07854593\n",
      "Iteration 218, loss = 0.07775907\n",
      "Iteration 219, loss = 0.07722750\n",
      "Iteration 220, loss = 0.07701487\n",
      "Iteration 221, loss = 0.07610716\n",
      "Iteration 222, loss = 0.07555924\n",
      "Iteration 223, loss = 0.07486472\n",
      "Iteration 224, loss = 0.07463850\n",
      "Iteration 225, loss = 0.07366962\n",
      "Iteration 226, loss = 0.07395028\n",
      "Iteration 227, loss = 0.07284694\n",
      "Iteration 228, loss = 0.07288336\n",
      "Iteration 229, loss = 0.07191022\n",
      "Iteration 230, loss = 0.07110837\n",
      "Iteration 231, loss = 0.07089465\n",
      "Iteration 232, loss = 0.07043527\n",
      "Iteration 233, loss = 0.07030488\n",
      "Iteration 234, loss = 0.06929463\n",
      "Iteration 235, loss = 0.06888733\n",
      "Iteration 236, loss = 0.06881735\n",
      "Iteration 237, loss = 0.06805549\n",
      "Iteration 238, loss = 0.06752275\n",
      "Iteration 239, loss = 0.06684467\n",
      "Iteration 240, loss = 0.06624363\n",
      "Iteration 241, loss = 0.06636739\n",
      "Iteration 242, loss = 0.06537238\n",
      "Iteration 243, loss = 0.06479479\n",
      "Iteration 244, loss = 0.06489857\n",
      "Iteration 245, loss = 0.06417664\n",
      "Iteration 246, loss = 0.06369211\n",
      "Iteration 247, loss = 0.06351511\n",
      "Iteration 248, loss = 0.06311592\n",
      "Iteration 249, loss = 0.06257515\n",
      "Iteration 250, loss = 0.06206888\n",
      "Iteration 251, loss = 0.06133745\n",
      "Iteration 252, loss = 0.06091933\n",
      "Iteration 253, loss = 0.06062929\n",
      "Iteration 254, loss = 0.06058381\n",
      "Iteration 255, loss = 0.05978187\n",
      "Iteration 256, loss = 0.05932396\n",
      "Iteration 257, loss = 0.05875408\n",
      "Iteration 258, loss = 0.05891886\n",
      "Iteration 259, loss = 0.05824158\n",
      "Iteration 260, loss = 0.05766582\n",
      "Iteration 261, loss = 0.05767413\n",
      "Iteration 262, loss = 0.05717196\n",
      "Iteration 263, loss = 0.05691272\n",
      "Iteration 264, loss = 0.05592782\n",
      "Iteration 265, loss = 0.05596777\n",
      "Iteration 266, loss = 0.05545987\n",
      "Iteration 267, loss = 0.05534807\n",
      "Iteration 268, loss = 0.05469213\n",
      "Iteration 269, loss = 0.05405870\n",
      "Iteration 270, loss = 0.05371893\n",
      "Iteration 271, loss = 0.05379777\n",
      "Iteration 272, loss = 0.05329881\n",
      "Iteration 273, loss = 0.05296300\n",
      "Iteration 274, loss = 0.05220518\n",
      "Iteration 275, loss = 0.05204276\n",
      "Iteration 276, loss = 0.05198837\n",
      "Iteration 277, loss = 0.05161683\n",
      "Iteration 278, loss = 0.05085892\n",
      "Iteration 279, loss = 0.05072413\n",
      "Iteration 280, loss = 0.05000966\n",
      "Iteration 281, loss = 0.05052043\n",
      "Iteration 282, loss = 0.04987307\n",
      "Iteration 283, loss = 0.04881502\n",
      "Iteration 284, loss = 0.04835933\n",
      "Iteration 285, loss = 0.04874316\n",
      "Iteration 286, loss = 0.04852574\n",
      "Iteration 287, loss = 0.04843882\n",
      "Iteration 288, loss = 0.04758885\n",
      "Iteration 289, loss = 0.04696584\n",
      "Iteration 290, loss = 0.04705432\n",
      "Iteration 291, loss = 0.04687135\n",
      "Iteration 292, loss = 0.04656337\n",
      "Iteration 293, loss = 0.04591667\n",
      "Iteration 294, loss = 0.04562508\n",
      "Iteration 295, loss = 0.04544274\n",
      "Iteration 296, loss = 0.04506172\n",
      "Iteration 297, loss = 0.04486218\n",
      "Iteration 298, loss = 0.04496459\n",
      "Iteration 299, loss = 0.04435305\n",
      "Iteration 300, loss = 0.04382736\n",
      "Iteration 301, loss = 0.04358074\n",
      "Iteration 302, loss = 0.04324337\n",
      "Iteration 303, loss = 0.04330332\n",
      "Iteration 304, loss = 0.04281921\n",
      "Iteration 305, loss = 0.04286907\n",
      "Iteration 306, loss = 0.04224674\n",
      "Iteration 307, loss = 0.04199419\n",
      "Iteration 308, loss = 0.04161709\n",
      "Iteration 309, loss = 0.04164826\n",
      "Iteration 310, loss = 0.04129437\n",
      "Iteration 311, loss = 0.04120344\n",
      "Iteration 312, loss = 0.04106589\n",
      "Iteration 313, loss = 0.03989329\n",
      "Iteration 314, loss = 0.04033141\n",
      "Iteration 315, loss = 0.04028848\n",
      "Iteration 316, loss = 0.03962478\n",
      "Iteration 317, loss = 0.03935532\n",
      "Iteration 318, loss = 0.03939214\n",
      "Iteration 319, loss = 0.03918971\n",
      "Iteration 320, loss = 0.03855935\n",
      "Iteration 321, loss = 0.03887681\n",
      "Iteration 322, loss = 0.03810415\n",
      "Iteration 323, loss = 0.03790869\n",
      "Iteration 324, loss = 0.03781655\n",
      "Iteration 325, loss = 0.03747905\n",
      "Iteration 326, loss = 0.03737715\n",
      "Iteration 327, loss = 0.03712495\n",
      "Iteration 328, loss = 0.03671414\n",
      "Iteration 329, loss = 0.03644095\n",
      "Iteration 330, loss = 0.03649267\n",
      "Iteration 331, loss = 0.03635845\n",
      "Iteration 332, loss = 0.03596407\n",
      "Iteration 333, loss = 0.03607528\n",
      "Iteration 334, loss = 0.03547044\n",
      "Iteration 335, loss = 0.03537639\n",
      "Iteration 336, loss = 0.03479588\n",
      "Iteration 337, loss = 0.03477558\n",
      "Iteration 338, loss = 0.03451258\n",
      "Iteration 339, loss = 0.03471547\n",
      "Iteration 340, loss = 0.03408153\n",
      "Iteration 341, loss = 0.03398874\n",
      "Iteration 342, loss = 0.03450553\n",
      "Iteration 343, loss = 0.03390842\n",
      "Iteration 344, loss = 0.03374381\n",
      "Iteration 345, loss = 0.03334670\n",
      "Iteration 346, loss = 0.03287946\n",
      "Iteration 347, loss = 0.03327202\n",
      "Iteration 348, loss = 0.03229834\n",
      "Iteration 349, loss = 0.03284340\n",
      "Iteration 350, loss = 0.03230346\n",
      "Iteration 351, loss = 0.03222173\n",
      "Iteration 352, loss = 0.03227336\n",
      "Iteration 353, loss = 0.03188696\n",
      "Iteration 354, loss = 0.03201851\n",
      "Iteration 355, loss = 0.03142050\n",
      "Iteration 356, loss = 0.03138724\n",
      "Iteration 357, loss = 0.03060732\n",
      "Iteration 358, loss = 0.03142206\n",
      "Iteration 359, loss = 0.03092128\n",
      "Iteration 360, loss = 0.03089754\n",
      "Iteration 361, loss = 0.03109661\n",
      "Iteration 362, loss = 0.03086543\n",
      "Iteration 363, loss = 0.03051801\n",
      "Iteration 364, loss = 0.03049824\n",
      "Iteration 365, loss = 0.02959550\n",
      "Iteration 366, loss = 0.03001964\n",
      "Iteration 367, loss = 0.02994651\n",
      "Iteration 368, loss = 0.02947961\n",
      "Iteration 369, loss = 0.02934617\n",
      "Iteration 370, loss = 0.02936375\n",
      "Iteration 371, loss = 0.02954369\n",
      "Iteration 372, loss = 0.02916437\n",
      "Iteration 373, loss = 0.02953179\n",
      "Iteration 374, loss = 0.02928251\n",
      "Iteration 375, loss = 0.02873078\n",
      "Iteration 376, loss = 0.02860009\n",
      "Iteration 377, loss = 0.02834690\n",
      "Iteration 378, loss = 0.02823657\n",
      "Iteration 379, loss = 0.02846914\n",
      "Iteration 380, loss = 0.02867101\n",
      "Iteration 381, loss = 0.02750062\n",
      "Iteration 382, loss = 0.02756689\n",
      "Iteration 383, loss = 0.02793130\n",
      "Iteration 384, loss = 0.02775069\n",
      "Iteration 385, loss = 0.02751213\n",
      "Iteration 386, loss = 0.02722744\n",
      "Iteration 387, loss = 0.02702582\n",
      "Iteration 388, loss = 0.02753132\n",
      "Iteration 389, loss = 0.02755666\n",
      "Iteration 390, loss = 0.02632384\n",
      "Iteration 391, loss = 0.02696282\n",
      "Iteration 392, loss = 0.02746503\n",
      "Iteration 393, loss = 0.02633034\n",
      "Iteration 394, loss = 0.02641452\n",
      "Iteration 395, loss = 0.02592163\n",
      "Iteration 396, loss = 0.02669170\n",
      "Iteration 397, loss = 0.02753026\n",
      "Iteration 398, loss = 0.02587705\n",
      "Iteration 399, loss = 0.02631491\n",
      "Iteration 400, loss = 0.02575248\n",
      "Iteration 401, loss = 0.02559735\n",
      "Iteration 402, loss = 0.02556326\n",
      "Iteration 403, loss = 0.02562570\n",
      "Iteration 404, loss = 0.02538148\n",
      "Iteration 405, loss = 0.02671800\n",
      "Iteration 406, loss = 0.02559932\n",
      "Iteration 407, loss = 0.02480227\n",
      "Iteration 408, loss = 0.02500061\n",
      "Iteration 409, loss = 0.02574897\n",
      "Iteration 410, loss = 0.02499970\n",
      "Iteration 411, loss = 0.02473011\n",
      "Iteration 412, loss = 0.02445902\n",
      "Iteration 413, loss = 0.02592434\n",
      "Iteration 414, loss = 0.02541897\n",
      "Iteration 415, loss = 0.02399133\n",
      "Iteration 416, loss = 0.02408240\n",
      "Iteration 417, loss = 0.02460618\n",
      "Iteration 418, loss = 0.02496172\n",
      "Iteration 419, loss = 0.02424044\n",
      "Iteration 420, loss = 0.02402103\n",
      "Iteration 421, loss = 0.02452787\n",
      "Iteration 422, loss = 0.02470591\n",
      "Iteration 423, loss = 0.02483878\n",
      "Iteration 424, loss = 0.02393691\n",
      "Iteration 425, loss = 0.02381193\n",
      "Iteration 426, loss = 0.02370003\n",
      "Iteration 427, loss = 0.02343280\n",
      "Iteration 428, loss = 0.02435672\n",
      "Iteration 429, loss = 0.02366267\n",
      "Iteration 430, loss = 0.02265373\n",
      "Iteration 431, loss = 0.02396556\n",
      "Iteration 432, loss = 0.02471360\n",
      "Iteration 433, loss = 0.02287417\n",
      "Iteration 434, loss = 0.02317674\n",
      "Iteration 435, loss = 0.02331248\n",
      "Iteration 436, loss = 0.02423621\n",
      "Iteration 437, loss = 0.02266505\n",
      "Iteration 438, loss = 0.02289865\n",
      "Iteration 439, loss = 0.02308773\n",
      "Iteration 440, loss = 0.02270975\n",
      "Iteration 441, loss = 0.02379144\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64924670\n",
      "Iteration 2, loss = 0.63164650\n",
      "Iteration 3, loss = 0.61758614\n",
      "Iteration 4, loss = 0.60133673\n",
      "Iteration 5, loss = 0.58542244\n",
      "Iteration 6, loss = 0.56949804\n",
      "Iteration 7, loss = 0.55363910\n",
      "Iteration 8, loss = 0.53994718\n",
      "Iteration 9, loss = 0.52674321\n",
      "Iteration 10, loss = 0.51491727\n",
      "Iteration 11, loss = 0.50264029\n",
      "Iteration 12, loss = 0.49165407\n",
      "Iteration 13, loss = 0.48334813\n",
      "Iteration 14, loss = 0.47509469\n",
      "Iteration 15, loss = 0.46752119\n",
      "Iteration 16, loss = 0.45844537\n",
      "Iteration 17, loss = 0.45169970\n",
      "Iteration 18, loss = 0.44557570\n",
      "Iteration 19, loss = 0.43871075\n",
      "Iteration 20, loss = 0.43157565\n",
      "Iteration 21, loss = 0.42651653\n",
      "Iteration 22, loss = 0.41990525\n",
      "Iteration 23, loss = 0.41515559\n",
      "Iteration 24, loss = 0.41057314\n",
      "Iteration 25, loss = 0.40477870\n",
      "Iteration 26, loss = 0.40145894\n",
      "Iteration 27, loss = 0.39649495\n",
      "Iteration 28, loss = 0.39169466\n",
      "Iteration 29, loss = 0.38834944\n",
      "Iteration 30, loss = 0.38414214\n",
      "Iteration 31, loss = 0.38112039\n",
      "Iteration 32, loss = 0.37803663\n",
      "Iteration 33, loss = 0.37265918\n",
      "Iteration 34, loss = 0.36868077\n",
      "Iteration 35, loss = 0.36678187\n",
      "Iteration 36, loss = 0.36247727\n",
      "Iteration 37, loss = 0.35963185\n",
      "Iteration 38, loss = 0.35749005\n",
      "Iteration 39, loss = 0.35349691\n",
      "Iteration 40, loss = 0.35023795\n",
      "Iteration 41, loss = 0.34723293\n",
      "Iteration 42, loss = 0.34563900\n",
      "Iteration 43, loss = 0.34230774\n",
      "Iteration 44, loss = 0.34087331\n",
      "Iteration 45, loss = 0.33762537\n",
      "Iteration 46, loss = 0.33600082\n",
      "Iteration 47, loss = 0.33253352\n",
      "Iteration 48, loss = 0.33048901\n",
      "Iteration 49, loss = 0.32781300\n",
      "Iteration 50, loss = 0.32475168\n",
      "Iteration 51, loss = 0.32508239\n",
      "Iteration 52, loss = 0.32130474\n",
      "Iteration 53, loss = 0.31952867\n",
      "Iteration 54, loss = 0.31704948\n",
      "Iteration 55, loss = 0.31446462\n",
      "Iteration 56, loss = 0.31231206\n",
      "Iteration 57, loss = 0.31127886\n",
      "Iteration 58, loss = 0.30969578\n",
      "Iteration 59, loss = 0.30835098\n",
      "Iteration 60, loss = 0.30430646\n",
      "Iteration 61, loss = 0.30292214\n",
      "Iteration 62, loss = 0.30336232\n",
      "Iteration 63, loss = 0.29900414\n",
      "Iteration 64, loss = 0.29686204\n",
      "Iteration 65, loss = 0.29631784\n",
      "Iteration 66, loss = 0.29390489\n",
      "Iteration 67, loss = 0.29244667\n",
      "Iteration 68, loss = 0.29078968\n",
      "Iteration 69, loss = 0.28858115\n",
      "Iteration 70, loss = 0.28773910\n",
      "Iteration 71, loss = 0.28684307\n",
      "Iteration 72, loss = 0.28389869\n",
      "Iteration 73, loss = 0.28335575\n",
      "Iteration 74, loss = 0.28158770\n",
      "Iteration 75, loss = 0.27954354\n",
      "Iteration 76, loss = 0.28001094\n",
      "Iteration 77, loss = 0.27651406\n",
      "Iteration 78, loss = 0.27610540\n",
      "Iteration 79, loss = 0.27406332\n",
      "Iteration 80, loss = 0.27195061\n",
      "Iteration 81, loss = 0.27112009\n",
      "Iteration 82, loss = 0.27044428\n",
      "Iteration 83, loss = 0.26900770\n",
      "Iteration 84, loss = 0.26784899\n",
      "Iteration 85, loss = 0.26575847\n",
      "Iteration 86, loss = 0.26523445\n",
      "Iteration 87, loss = 0.26504797\n",
      "Iteration 88, loss = 0.26222858\n",
      "Iteration 89, loss = 0.26100747\n",
      "Iteration 90, loss = 0.26125937\n",
      "Iteration 91, loss = 0.25867575\n",
      "Iteration 92, loss = 0.25725674\n",
      "Iteration 93, loss = 0.25723709\n",
      "Iteration 94, loss = 0.25629843\n",
      "Iteration 95, loss = 0.25356365\n",
      "Iteration 96, loss = 0.25328071\n",
      "Iteration 97, loss = 0.25142433\n",
      "Iteration 98, loss = 0.25193780\n",
      "Iteration 99, loss = 0.24880423\n",
      "Iteration 100, loss = 0.24806181\n",
      "Iteration 101, loss = 0.24835461\n",
      "Iteration 102, loss = 0.24665872\n",
      "Iteration 103, loss = 0.24579316\n",
      "Iteration 104, loss = 0.24353913\n",
      "Iteration 105, loss = 0.24462788\n",
      "Iteration 106, loss = 0.24308088\n",
      "Iteration 107, loss = 0.24063660\n",
      "Iteration 108, loss = 0.24153821\n",
      "Iteration 109, loss = 0.23995799\n",
      "Iteration 110, loss = 0.24011020\n",
      "Iteration 111, loss = 0.23712408\n",
      "Iteration 112, loss = 0.23798632\n",
      "Iteration 113, loss = 0.23562012\n",
      "Iteration 114, loss = 0.23498534\n",
      "Iteration 115, loss = 0.23334670\n",
      "Iteration 116, loss = 0.23469749\n",
      "Iteration 117, loss = 0.23235380\n",
      "Iteration 118, loss = 0.23099888\n",
      "Iteration 119, loss = 0.23187978\n",
      "Iteration 120, loss = 0.22974317\n",
      "Iteration 121, loss = 0.22858043\n",
      "Iteration 122, loss = 0.22822744\n",
      "Iteration 123, loss = 0.22642612\n",
      "Iteration 124, loss = 0.22827496\n",
      "Iteration 125, loss = 0.22745281\n",
      "Iteration 126, loss = 0.22548148\n",
      "Iteration 127, loss = 0.22445326\n",
      "Iteration 128, loss = 0.22485180\n",
      "Iteration 129, loss = 0.22266527\n",
      "Iteration 130, loss = 0.22128160\n",
      "Iteration 131, loss = 0.22016777\n",
      "Iteration 132, loss = 0.22019876\n",
      "Iteration 133, loss = 0.21909657\n",
      "Iteration 134, loss = 0.21960061\n",
      "Iteration 135, loss = 0.21870605\n",
      "Iteration 136, loss = 0.21851060\n",
      "Iteration 137, loss = 0.21617666\n",
      "Iteration 138, loss = 0.21677434\n",
      "Iteration 139, loss = 0.21645727\n",
      "Iteration 140, loss = 0.21409722\n",
      "Iteration 141, loss = 0.21751027\n",
      "Iteration 142, loss = 0.21338278\n",
      "Iteration 143, loss = 0.21232504\n",
      "Iteration 144, loss = 0.21096008\n",
      "Iteration 145, loss = 0.21223658\n",
      "Iteration 146, loss = 0.20940555\n",
      "Iteration 147, loss = 0.21322386\n",
      "Iteration 148, loss = 0.21054053\n",
      "Iteration 149, loss = 0.21260676\n",
      "Iteration 150, loss = 0.20906562\n",
      "Iteration 151, loss = 0.20829192\n",
      "Iteration 152, loss = 0.20653020\n",
      "Iteration 153, loss = 0.20626860\n",
      "Iteration 154, loss = 0.20522087\n",
      "Iteration 155, loss = 0.20609642\n",
      "Iteration 156, loss = 0.20505275\n",
      "Iteration 157, loss = 0.20411832\n",
      "Iteration 158, loss = 0.20426385\n",
      "Iteration 159, loss = 0.20669367\n",
      "Iteration 160, loss = 0.20252457\n",
      "Iteration 161, loss = 0.20411945\n",
      "Iteration 162, loss = 0.20245727\n",
      "Iteration 163, loss = 0.20288105\n",
      "Iteration 164, loss = 0.20071377\n",
      "Iteration 165, loss = 0.20318895\n",
      "Iteration 166, loss = 0.19954125\n",
      "Iteration 167, loss = 0.19900714\n",
      "Iteration 168, loss = 0.19876439\n",
      "Iteration 169, loss = 0.19878353\n",
      "Iteration 170, loss = 0.19811784\n",
      "Iteration 171, loss = 0.19765795\n",
      "Iteration 172, loss = 0.19752094\n",
      "Iteration 173, loss = 0.19639463\n",
      "Iteration 174, loss = 0.19665919\n",
      "Iteration 175, loss = 0.19522065\n",
      "Iteration 176, loss = 0.19590930\n",
      "Iteration 177, loss = 0.19497658\n",
      "Iteration 178, loss = 0.19483594\n",
      "Iteration 179, loss = 0.19403879\n",
      "Iteration 180, loss = 0.19310305\n",
      "Iteration 181, loss = 0.19537895\n",
      "Iteration 182, loss = 0.19472229\n",
      "Iteration 183, loss = 0.19284715\n",
      "Iteration 184, loss = 0.19070511\n",
      "Iteration 185, loss = 0.19180474\n",
      "Iteration 186, loss = 0.18976135\n",
      "Iteration 187, loss = 0.19074600\n",
      "Iteration 188, loss = 0.18997519\n",
      "Iteration 189, loss = 0.19126066\n",
      "Iteration 190, loss = 0.18899122\n",
      "Iteration 191, loss = 0.18783357\n",
      "Iteration 192, loss = 0.18911339\n",
      "Iteration 193, loss = 0.18991270\n",
      "Iteration 194, loss = 0.18663145\n",
      "Iteration 195, loss = 0.18688344\n",
      "Iteration 196, loss = 0.18619864\n",
      "Iteration 197, loss = 0.18551975\n",
      "Iteration 198, loss = 0.18594113\n",
      "Iteration 199, loss = 0.18606583\n",
      "Iteration 200, loss = 0.18558332\n",
      "Iteration 201, loss = 0.18444929\n",
      "Iteration 202, loss = 0.18328539\n",
      "Iteration 203, loss = 0.18446118\n",
      "Iteration 204, loss = 0.18201756\n",
      "Iteration 205, loss = 0.18363091\n",
      "Iteration 206, loss = 0.18264306\n",
      "Iteration 207, loss = 0.18283268\n",
      "Iteration 208, loss = 0.18140912\n",
      "Iteration 209, loss = 0.18313301\n",
      "Iteration 210, loss = 0.18311042\n",
      "Iteration 211, loss = 0.17960235\n",
      "Iteration 212, loss = 0.18370429\n",
      "Iteration 213, loss = 0.17836810\n",
      "Iteration 214, loss = 0.18207618\n",
      "Iteration 215, loss = 0.17960405\n",
      "Iteration 216, loss = 0.17912306\n",
      "Iteration 217, loss = 0.17776742\n",
      "Iteration 218, loss = 0.17750551\n",
      "Iteration 219, loss = 0.17775284\n",
      "Iteration 220, loss = 0.17681992\n",
      "Iteration 221, loss = 0.17793087\n",
      "Iteration 222, loss = 0.17795451\n",
      "Iteration 223, loss = 0.17667290\n",
      "Iteration 224, loss = 0.17708610\n",
      "Iteration 225, loss = 0.17529539\n",
      "Iteration 226, loss = 0.17559165\n",
      "Iteration 227, loss = 0.17503789\n",
      "Iteration 228, loss = 0.17566996\n",
      "Iteration 229, loss = 0.17597760\n",
      "Iteration 230, loss = 0.17549503\n",
      "Iteration 231, loss = 0.17262719\n",
      "Iteration 232, loss = 0.17305594\n",
      "Iteration 233, loss = 0.17237970\n",
      "Iteration 234, loss = 0.17340031\n",
      "Iteration 235, loss = 0.17324940\n",
      "Iteration 236, loss = 0.17147701\n",
      "Iteration 237, loss = 0.17435964\n",
      "Iteration 238, loss = 0.17163436\n",
      "Iteration 239, loss = 0.17156550\n",
      "Iteration 240, loss = 0.17148533\n",
      "Iteration 241, loss = 0.17228990\n",
      "Iteration 242, loss = 0.16961196\n",
      "Iteration 243, loss = 0.17156820\n",
      "Iteration 244, loss = 0.16886705\n",
      "Iteration 245, loss = 0.16946052\n",
      "Iteration 246, loss = 0.16848278\n",
      "Iteration 247, loss = 0.16892836\n",
      "Iteration 248, loss = 0.17047154\n",
      "Iteration 249, loss = 0.16993444\n",
      "Iteration 250, loss = 0.16972773\n",
      "Iteration 251, loss = 0.16834317\n",
      "Iteration 252, loss = 0.16868794\n",
      "Iteration 253, loss = 0.16811374\n",
      "Iteration 254, loss = 0.16644520\n",
      "Iteration 255, loss = 0.16651214\n",
      "Iteration 256, loss = 0.16635045\n",
      "Iteration 257, loss = 0.16591850\n",
      "Iteration 258, loss = 0.16970244\n",
      "Iteration 259, loss = 0.16508584\n",
      "Iteration 260, loss = 0.16639266\n",
      "Iteration 261, loss = 0.16391895\n",
      "Iteration 262, loss = 0.16602654\n",
      "Iteration 263, loss = 0.16583529\n",
      "Iteration 264, loss = 0.16411495\n",
      "Iteration 265, loss = 0.16575030\n",
      "Iteration 266, loss = 0.16454568\n",
      "Iteration 267, loss = 0.16348531\n",
      "Iteration 268, loss = 0.16160211\n",
      "Iteration 269, loss = 0.16335129\n",
      "Iteration 270, loss = 0.16362471\n",
      "Iteration 271, loss = 0.16168600\n",
      "Iteration 272, loss = 0.16369160\n",
      "Iteration 273, loss = 0.16087156\n",
      "Iteration 274, loss = 0.16167088\n",
      "Iteration 275, loss = 0.16346589\n",
      "Iteration 276, loss = 0.16120711\n",
      "Iteration 277, loss = 0.15979981\n",
      "Iteration 278, loss = 0.16180063\n",
      "Iteration 279, loss = 0.16094005\n",
      "Iteration 280, loss = 0.15982608\n",
      "Iteration 281, loss = 0.15941870\n",
      "Iteration 282, loss = 0.15836150\n",
      "Iteration 283, loss = 0.16055627\n",
      "Iteration 284, loss = 0.16090509\n",
      "Iteration 285, loss = 0.15872508\n",
      "Iteration 286, loss = 0.15730918\n",
      "Iteration 287, loss = 0.15861238\n",
      "Iteration 288, loss = 0.15725497\n",
      "Iteration 289, loss = 0.15804014\n",
      "Iteration 290, loss = 0.15754823\n",
      "Iteration 291, loss = 0.15643451\n",
      "Iteration 292, loss = 0.15611826\n",
      "Iteration 293, loss = 0.15816727\n",
      "Iteration 294, loss = 0.15708932\n",
      "Iteration 295, loss = 0.15811054\n",
      "Iteration 296, loss = 0.15805192\n",
      "Iteration 297, loss = 0.15370634\n",
      "Iteration 298, loss = 0.15420249\n",
      "Iteration 299, loss = 0.15502957\n",
      "Iteration 300, loss = 0.15696920\n",
      "Iteration 301, loss = 0.15538040\n",
      "Iteration 302, loss = 0.15470225\n",
      "Iteration 303, loss = 0.15462935\n",
      "Iteration 304, loss = 0.15583414\n",
      "Iteration 305, loss = 0.15307094\n",
      "Iteration 306, loss = 0.15360858\n",
      "Iteration 307, loss = 0.15428351\n",
      "Iteration 308, loss = 0.15278064\n",
      "Iteration 309, loss = 0.15200533\n",
      "Iteration 310, loss = 0.15366166\n",
      "Iteration 311, loss = 0.15160831\n",
      "Iteration 312, loss = 0.15322498\n",
      "Iteration 313, loss = 0.15175660\n",
      "Iteration 314, loss = 0.15462672\n",
      "Iteration 315, loss = 0.15060800\n",
      "Iteration 316, loss = 0.15253980\n",
      "Iteration 317, loss = 0.15098762\n",
      "Iteration 318, loss = 0.15307551\n",
      "Iteration 319, loss = 0.14984585\n",
      "Iteration 320, loss = 0.14908639\n",
      "Iteration 321, loss = 0.15351316\n",
      "Iteration 322, loss = 0.15246324\n",
      "Iteration 323, loss = 0.15098262\n",
      "Iteration 324, loss = 0.14990804\n",
      "Iteration 325, loss = 0.14952247\n",
      "Iteration 326, loss = 0.14808057\n",
      "Iteration 327, loss = 0.14840864\n",
      "Iteration 328, loss = 0.14897805\n",
      "Iteration 329, loss = 0.14932382\n",
      "Iteration 330, loss = 0.14902957\n",
      "Iteration 331, loss = 0.14947979\n",
      "Iteration 332, loss = 0.14647682\n",
      "Iteration 333, loss = 0.14940114\n",
      "Iteration 334, loss = 0.14827715\n",
      "Iteration 335, loss = 0.14664307\n",
      "Iteration 336, loss = 0.14989550\n",
      "Iteration 337, loss = 0.14618216\n",
      "Iteration 338, loss = 0.14453201\n",
      "Iteration 339, loss = 0.15000445\n",
      "Iteration 340, loss = 0.14455256\n",
      "Iteration 341, loss = 0.14571348\n",
      "Iteration 342, loss = 0.14527029\n",
      "Iteration 343, loss = 0.14873333\n",
      "Iteration 344, loss = 0.14616943\n",
      "Iteration 345, loss = 0.14432976\n",
      "Iteration 346, loss = 0.14531716\n",
      "Iteration 347, loss = 0.14824833\n",
      "Iteration 348, loss = 0.14482689\n",
      "Iteration 349, loss = 0.14267613\n",
      "Iteration 350, loss = 0.14593788\n",
      "Iteration 351, loss = 0.14406837\n",
      "Iteration 352, loss = 0.14431726\n",
      "Iteration 353, loss = 0.14400848\n",
      "Iteration 354, loss = 0.14501232\n",
      "Iteration 355, loss = 0.14394852\n",
      "Iteration 356, loss = 0.14249595\n",
      "Iteration 357, loss = 0.14519304\n",
      "Iteration 358, loss = 0.14327603\n",
      "Iteration 359, loss = 0.14202497\n",
      "Iteration 360, loss = 0.14510929\n",
      "Iteration 361, loss = 0.14136024\n",
      "Iteration 362, loss = 0.14269107\n",
      "Iteration 363, loss = 0.14246297\n",
      "Iteration 364, loss = 0.14248574\n",
      "Iteration 365, loss = 0.14264140\n",
      "Iteration 366, loss = 0.14126640\n",
      "Iteration 367, loss = 0.14147583\n",
      "Iteration 368, loss = 0.14293185\n",
      "Iteration 369, loss = 0.14015263\n",
      "Iteration 370, loss = 0.14031008\n",
      "Iteration 371, loss = 0.14402831\n",
      "Iteration 372, loss = 0.14277645\n",
      "Iteration 373, loss = 0.14146262\n",
      "Iteration 374, loss = 0.14011757\n",
      "Iteration 375, loss = 0.13938497\n",
      "Iteration 376, loss = 0.13887994\n",
      "Iteration 377, loss = 0.13888794\n",
      "Iteration 378, loss = 0.14198159\n",
      "Iteration 379, loss = 0.13739454\n",
      "Iteration 380, loss = 0.14305478\n",
      "Iteration 381, loss = 0.13851072\n",
      "Iteration 382, loss = 0.13840122\n",
      "Iteration 383, loss = 0.13840614\n",
      "Iteration 384, loss = 0.14007726\n",
      "Iteration 385, loss = 0.13891648\n",
      "Iteration 386, loss = 0.14176468\n",
      "Iteration 387, loss = 0.13821643\n",
      "Iteration 388, loss = 0.13941883\n",
      "Iteration 389, loss = 0.13940450\n",
      "Iteration 390, loss = 0.13761922\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "Activation: logistic\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 1   logistic         0.616127 0.602284     110.218058         0.107103\n",
      "   MLP preprocessador 2   logistic         0.811586 0.562505     882.888276         0.052316\n",
      "   MLP preprocessador 3   logistic         0.839550 0.537142    1659.753832         0.059463\n",
      "   MLP preprocessador 4   logistic         0.908429 0.527676    1370.209552         0.059119\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 1 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 1   logistic         0.616127 0.602284     110.218058         0.107103\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 2 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 2   logistic         0.811586 0.562505     882.888276         0.052316\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 3 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 3   logistic          0.83955 0.537142    1659.753832         0.059463\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 4 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 4   logistic         0.908429 0.527676    1370.209552         0.059119\n",
      "Média Accuracy: 0.7939 | Média Recall: 0.5574\n",
      "\n",
      "Activation: relu\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 1       relu         0.693885 0.570991     224.982450         0.031225\n",
      "   MLP preprocessador 2       relu         0.738771 0.547190     210.220662         0.032913\n",
      "   MLP preprocessador 3       relu         0.861394 0.533472     455.668976         0.044919\n",
      "   MLP preprocessador 4       relu         0.900048 0.535699     370.812482         0.038782\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 1 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 1       relu         0.693885 0.570991      224.98245         0.031225\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 2 ---\n",
      "modelo   preprocessador activation  mean_test_score  Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 2       relu         0.738771 0.54719     210.220662         0.032913\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 3 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 3       relu         0.861394 0.533472     455.668976         0.044919\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 4 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 4       relu         0.900048 0.535699     370.812482         0.038782\n",
      "Média Accuracy: 0.7985 | Média Recall: 0.5468\n",
      "\n",
      "Activation: tanh\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 1       tanh         0.744020 0.569777     454.051893         0.046959\n",
      "   MLP preprocessador 2       tanh         0.792218 0.549011     544.124066         0.072056\n",
      "   MLP preprocessador 3       tanh         0.905216 0.525096    1015.046192         0.098826\n",
      "   MLP preprocessador 4       tanh         0.915886 0.523958     499.412812         0.102536\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 1 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 1       tanh          0.74402 0.569777     454.051893         0.046959\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 2 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 2       tanh         0.792218 0.549011     544.124066         0.072056\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 3 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 3       tanh         0.905216 0.525096    1015.046192         0.098826\n",
      "\n",
      "--- Resultados para Preprocessador: preprocessador 4 ---\n",
      "modelo   preprocessador activation  mean_test_score   Recall  mean_fit_time  mean_score_time\n",
      "   MLP preprocessador 4       tanh         0.915886 0.523958     499.412812         0.102536\n",
      "Média Accuracy: 0.8393 | Média Recall: 0.5420\n",
      "\n",
      "======= Media por pre-processador =======\n",
      "Preprocessador: preprocessador 1 | Média Accuracy: 0.6847 | Média Recall: 0.5810\n",
      "Preprocessador: preprocessador 2 | Média Accuracy: 0.7809 | Média Recall: 0.5529\n",
      "Preprocessador: preprocessador 3 | Média Accuracy: 0.8687 | Média Recall: 0.5319\n",
      "Preprocessador: preprocessador 4 | Média Accuracy: 0.9081 | Média Recall: 0.5291\n",
      "\n",
      " =======  Media total ======= \n",
      " Accuracy: 0.8106 | Recall: 0.5487\n",
      "\n",
      "======= Melhor Resultado Recall =======\n",
      "Preprocessador: preprocessador 1\n",
      "Modelo: MLP\n",
      "activation: logistic\n",
      "Accuracy (mean_test_score): 0.6161\n",
      "Recall-score: 0.6023\n",
      "Tempo médio de treino: 110.2181 s\n",
      "Tempo médio de score: 0.1071 s\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------MLP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "amostra = x.copy()\n",
    "amostra['target'] = y\n",
    "amostra = amostra.sample(50000)\n",
    "\n",
    "x_sample = amostra.drop('target', axis=1)\n",
    "y_sample = amostra['target']\n",
    "\n",
    "modelo_parametrosMLP = {\n",
    "    'MLP': {\n",
    "        'modelo': MLPClassifier(),\n",
    "        'parametros': {\n",
    "            'activation': ['logistic', 'tanh', 'relu'],\n",
    "            'max_iter': [2000],\n",
    "            'hidden_layer_sizes': [(64,64)],\n",
    "            'random_state': [0],\n",
    "            'tol': [0.00001],\n",
    "            'learning_rate_init': [0.001],\n",
    "            'verbose': [True],\n",
    "            'alpha': [0.0005],\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "detalhes_MLP = []\n",
    "\n",
    "for nome_prep, preprocessador in preprocessadores.items():\n",
    "    x_sample_prep = x_sample.copy()\n",
    "    x_sample_prep = preprocessador.fit_transform(x_sample_prep)\n",
    "    x_treinamento_sample, x_teste_sample, y_treinamento_sample, y_teste_sample = train_test_split(x_sample_prep, y_sample, test_size=0.15, random_state=0)\n",
    "\n",
    "    if hasattr(x_treinamento_sample, \"toarray\"):\n",
    "        x_treinamento_sample = x_treinamento_sample.toarray()\n",
    "\n",
    "    x_treinamento_sample = pd.DataFrame(\n",
    "        x_treinamento_sample,\n",
    "        index=y_treinamento_sample.index\n",
    "    )\n",
    "\n",
    "    train_df = pd.concat([x_treinamento_sample, y_treinamento_sample], axis=1)\n",
    "\n",
    "    classe_0 = train_df[train_df['target'] == 0]\n",
    "    classe_1 = train_df[train_df['target'] == 1]\n",
    "\n",
    "    fator = round(len(classe_0) / len(classe_1))\n",
    "\n",
    "    classe_1_bal = pd.concat([classe_1] * fator, ignore_index=True)\n",
    "\n",
    "    balanced_train_df = pd.concat([classe_0, classe_1_bal], ignore_index=True)\n",
    "    x_bal_train = balanced_train_df.drop('target', axis=1)\n",
    "    y_bal_train = balanced_train_df['target']\n",
    "\n",
    "    for nome_modelo, objeto in modelo_parametrosMLP.items():\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('modelo', objeto['modelo']),\n",
    "        ])\n",
    "\n",
    "        grid = GridSearchCV(objeto['modelo'], objeto['parametros'], cv=5, scoring='recall_macro', n_jobs=-1)\n",
    "        grid.fit(x_bal_train, y_bal_train)\n",
    "\n",
    "        df_MLP= pd.DataFrame(grid.cv_results_)\n",
    "        y_pred = grid.best_estimator_.predict(x_teste_sample)\n",
    "\n",
    "        df_MLP['modelo'] = nome_modelo\n",
    "        df_MLP['preprocessador'] = nome_prep\n",
    "        df_MLP['activation'] = df_MLP['params'].apply(lambda x: x['activation'])\n",
    "        \n",
    "        recall_scores = []\n",
    "        for params in df_MLP['params']:\n",
    "            modelo = MLPClassifier(**params)\n",
    "            modelo.fit(x_bal_train, y_bal_train)\n",
    "            y_pred = modelo.predict(x_teste_sample)\n",
    "            recall = recall_score(y_teste_sample, y_pred, average='macro')\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "        df_MLP['Recall'] = recall_scores\n",
    "\n",
    "        detalhes_MLP.append(df_MLP[[\n",
    "            'modelo', 'preprocessador', 'activation',\n",
    "            'mean_test_score', 'Recall', 'mean_fit_time', 'mean_score_time',\n",
    "        ]])\n",
    "\n",
    "        \n",
    "\n",
    "df_detalhadoMLP= pd.concat(detalhes_MLP, ignore_index=True)\n",
    "df_detalhadoMLP = df_detalhadoMLP.sort_values(by=['preprocessador', 'activation'], ascending=[True, True])\n",
    "\n",
    "for activation, df_sub in df_detalhadoMLP.groupby('activation'):\n",
    "    print(f\"\\nActivation: {activation}\")\n",
    "    print(df_sub.to_string(index=False))\n",
    "\n",
    "    for preproc, df_sub_prep in df_sub.groupby('preprocessador'):\n",
    "        print(f\"\\n--- Resultados para Preprocessador: {preproc} ---\")\n",
    "        print(df_sub_prep.to_string(index=False))\n",
    "\n",
    "    media_acc = df_sub['mean_test_score'].mean()\n",
    "    media_recall = df_sub['Recall'].mean()\n",
    "    print(f\"Média Accuracy: {media_acc:.4f} | Média Recall: {media_recall:.4f}\")\n",
    "\n",
    "print(\"\\n======= Media por pre-processador =======\")\n",
    "medias_por_preprocessador = df_detalhadoMLP.groupby('preprocessador')[['mean_test_score', 'Recall']].mean().reset_index()\n",
    "\n",
    "for _, linha in medias_por_preprocessador.iterrows():\n",
    "    print(f\"Preprocessador: {linha['preprocessador']} | Média Accuracy: {linha['mean_test_score']:.4f} | Média Recall: {linha['Recall']:.4f}\")\n",
    "\n",
    "\n",
    "media_recall_total = df_detalhadoMLP['Recall'].mean()\n",
    "media_acc_total = df_detalhadoMLP['mean_test_score'].mean()\n",
    "print(f\"\\n =======  Media total ======= \\n Accuracy: {media_acc_total:.4f} | Recall: {media_recall_total:.4f}\")\n",
    "\n",
    "melhor_linhaMLP = df_detalhadoMLP.loc[df_detalhadoMLP['Recall'].idxmax()]\n",
    "\n",
    "print(\"\\n======= Melhor Resultado Recall =======\")\n",
    "print(f\"Preprocessador: {melhor_linhaMLP['preprocessador']}\")\n",
    "print(f\"Modelo: {melhor_linhaMLP['modelo']}\")\n",
    "print(f\"activation: {melhor_linhaMLP['activation']}\")\n",
    "print(f\"Accuracy (mean_test_score): {melhor_linhaMLP['mean_test_score']:.4f}\")\n",
    "print(f\"Recall-score: {melhor_linhaMLP['Recall']:.4f}\")\n",
    "print(f\"Tempo médio de treino: {melhor_linhaMLP['mean_fit_time']:.4f} s\")\n",
    "print(f\"Tempo médio de score: {melhor_linhaMLP['mean_score_time']:.4f} s\")\n",
    "\n",
    "\n",
    "df_detalhadoMLP.to_csv('df_resultadosMLP.csv', index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc995c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
